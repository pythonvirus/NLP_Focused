{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=True, wordpieces_prefix=##)\n",
      "tokenizer1 Tokenizer(vocabulary_size=0, model=SentencePieceBPE, unk_token=<unk>, replacement=▁, add_prefix_space=True, dropout=None)\n",
      "tokenizer2 Tokenizer(vocabulary_size=0, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "tokenizer3 Tokenizer(vocabulary_size=0, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "output Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "output1 Encoding(num_tokens=0, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "output2 Encoding(num_tokens=0, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "output3 Encoding(num_tokens=0, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "ids: [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tokens: ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '?', '[SEP]']\n",
      "offsets: [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), (14, 17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "overflowing: []\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import (BertWordPieceTokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer)\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
    "tokenizer1 = SentencePieceBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
    "tokenizer2 = ByteLevelBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
    "tokenizer3 = CharBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
    "print('tokenizer',tokenizer)\n",
    "print('tokenizer1',tokenizer1)\n",
    "print('tokenizer2',tokenizer2)\n",
    "print('tokenizer3',tokenizer3)\n",
    "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
    "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
    "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
    "# lowercase=True, wordpieces_prefix=##)\n",
    "\n",
    "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
    "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
    "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
    "output1 = tokenizer1.encode(\"Hello, y'all! How are you?\")\n",
    "output2 = tokenizer2.encode(\"Hello, y'all! How are you?\")\n",
    "output3 = tokenizer3.encode(\"Hello, y'all! How are you?\")\n",
    "\n",
    "print('output',output) \n",
    "print('output1',output1)\n",
    "print('output2',output2)\n",
    "print('output3',output3)\n",
    "\n",
    "print(o)\n",
    "\n",
    "\n",
    "# Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask, \t\t\t\t\t\t\t special_tokens_mask, overflowing])\n",
    "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
    "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', \t\t\t\t\t\t\t\t\t\t\t'you', '?', '[SEP]']\n",
    "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
    "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
    "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "print(f\"overflowing: {output.overflowing}\") # []\n",
    "\n",
    "# Provided tokenizers\n",
    "# CharBPETokenizer: The original BPE\n",
    "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
    "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
    "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= CharBPETokenizer ==========\n"
     ]
    }
   ],
   "source": [
    "DATAFILE = 'pg16457.txt'\n",
    "MODELDIR = ''\n",
    "\n",
    "input_text = 'This is a test'\n",
    "\n",
    "# Training the tokenizers\n",
    "\n",
    "print(\"========= CharBPETokenizer ==========\")\n",
    "# CharBPETokenizer\n",
    "tokenizer = CharBPETokenizer()\n",
    "tokenizer.train([DATAFILE], vocab_size=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "========= ByteLevelBPETokenizer ==========\n",
      "['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "========= SentencePieceBPETokenizer ==========\n",
      "['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "========= BertWordPieceTokenizer ==========\n",
      "['this', 'is', 'a', 't', '##est']\n"
     ]
    }
   ],
   "source": [
    "#tokenizer.save(MODELDIR, 'char_bpe')\n",
    "\n",
    "output = tokenizer.encode(input_text)\n",
    "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
    "\n",
    "print(\"========= ByteLevelBPETokenizer ==========\")\n",
    "# ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train([DATAFILE], vocab_size=500)\n",
    "\n",
    "#tokenizer.save(MODELDIR, 'byte_bpe')\n",
    "output = tokenizer.encode(input_text)\n",
    "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
    "\n",
    "print(\"========= SentencePieceBPETokenizer ==========\")\n",
    "# SentencePieceBPETokenizer\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train([DATAFILE], vocab_size=500)\n",
    "\n",
    "#tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
    "output = tokenizer.encode(input_text)\n",
    "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
    "\n",
    "print(\"========= BertWordPieceTokenizer ==========\")\n",
    "# BertWordPieceTokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer.train([DATAFILE], vocab_size=500)\n",
    "\n",
    "#tokenizer.save(MODELDIR, 'bert_bpe')\n",
    "output = tokenizer.encode(input_text)\n",
    "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"../data/bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "print(tokenizer)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print(output)   # Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                            special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "print(tokenizer)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print(output)   # Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                            special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "DATAFILE = 'pg16457.txt'\n",
      "MODELDIR = 'models'\n",
      "\n",
      "input_text = 'This is a test'\n",
      "\n",
      "# Training the tokenizers\n",
      "\n",
      "print(\"========= CharBPETokenizer ==========\")\n",
      "# CharBPETokenizer\n",
      "tokenizer = CharBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'char_bpe')\n",
      "\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "\n",
      "print(\"========= ByteLevelBPETokenizer ==========\")\n",
      "# ByteLevelBPETokenizer\n",
      "tokenizer = ByteLevelBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'byte_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "\n",
      "print(\"========= SentencePieceBPETokenizer ==========\")\n",
      "# SentencePieceBPETokenizer\n",
      "tokenizer = SentencePieceBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "\n",
      "print(\"========= BertWordPieceTokenizer ==========\")\n",
      "# BertWordPieceTokenizer\n",
      "tokenizer = BertWordPieceTokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'bert_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']\n",
      "DATAFILE = 'pg16457.txt'\n",
      "MODELDIR = ''\n",
      "\n",
      "input_text = 'This is a test'\n",
      "\n",
      "# Training the tokenizers\n",
      "\n",
      "print(\"========= CharBPETokenizer ==========\")\n",
      "# CharBPETokenizer\n",
      "tokenizer = CharBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'char_bpe')\n",
      "\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "\n",
      "print(\"========= ByteLevelBPETokenizer ==========\")\n",
      "# ByteLevelBPETokenizer\n",
      "tokenizer = ByteLevelBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'byte_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "\n",
      "print(\"========= SentencePieceBPETokenizer ==========\")\n",
      "# SentencePieceBPETokenizer\n",
      "tokenizer = SentencePieceBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "\n",
      "print(\"========= BertWordPieceTokenizer ==========\")\n",
      "# BertWordPieceTokenizer\n",
      "tokenizer = BertWordPieceTokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'bert_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']\n",
      "DATAFILE = 'pg16457.txt'\n",
      "MODELDIR = ''\n",
      "\n",
      "input_text = 'This is a test'\n",
      "\n",
      "# Training the tokenizers\n",
      "\n",
      "print(\"========= CharBPETokenizer ==========\")\n",
      "# CharBPETokenizer\n",
      "tokenizer = CharBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "tokenizer.save(MODELDIR, 'char_bpe')\n",
      "\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "\n",
      "print(\"========= ByteLevelBPETokenizer ==========\")\n",
      "# ByteLevelBPETokenizer\n",
      "tokenizer = ByteLevelBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'byte_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "\n",
      "print(\"========= SentencePieceBPETokenizer ==========\")\n",
      "# SentencePieceBPETokenizer\n",
      "tokenizer = SentencePieceBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "\n",
      "print(\"========= BertWordPieceTokenizer ==========\")\n",
      "# BertWordPieceTokenizer\n",
      "tokenizer = BertWordPieceTokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'bert_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']\n",
      "#tokenizer.save(MODELDIR, 'char_bpe')\n",
      "\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "\n",
      "print(\"========= ByteLevelBPETokenizer ==========\")\n",
      "# ByteLevelBPETokenizer\n",
      "tokenizer = ByteLevelBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'byte_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "\n",
      "print(\"========= SentencePieceBPETokenizer ==========\")\n",
      "# SentencePieceBPETokenizer\n",
      "tokenizer = SentencePieceBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "\n",
      "print(\"========= BertWordPieceTokenizer ==========\")\n",
      "# BertWordPieceTokenizer\n",
      "tokenizer = BertWordPieceTokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'bert_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']\n",
      "#tokenizer.save(MODELDIR, 'char_bpe')\n",
      "\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "\n",
      "print(\"========= ByteLevelBPETokenizer ==========\")\n",
      "# ByteLevelBPETokenizer\n",
      "tokenizer = ByteLevelBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "tokenizer.save(MODELDIR, 'byte_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "\n",
      "print(\"========= SentencePieceBPETokenizer ==========\")\n",
      "# SentencePieceBPETokenizer\n",
      "tokenizer = SentencePieceBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "\n",
      "print(\"========= BertWordPieceTokenizer ==========\")\n",
      "# BertWordPieceTokenizer\n",
      "tokenizer = BertWordPieceTokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'bert_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']\n",
      "#tokenizer.save(MODELDIR, 'char_bpe')\n",
      "\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "\n",
      "print(\"========= ByteLevelBPETokenizer ==========\")\n",
      "# ByteLevelBPETokenizer\n",
      "tokenizer = ByteLevelBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'byte_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "\n",
      "print(\"========= SentencePieceBPETokenizer ==========\")\n",
      "# SentencePieceBPETokenizer\n",
      "tokenizer = SentencePieceBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "\n",
      "print(\"========= BertWordPieceTokenizer ==========\")\n",
      "# BertWordPieceTokenizer\n",
      "tokenizer = BertWordPieceTokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'bert_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "print(tokenizer)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print(output)   # Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                            special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "DATAFILE = 'pg16457.txt'\n",
      "MODELDIR = ''\n",
      "\n",
      "input_text = 'This is a test'\n",
      "\n",
      "# Training the tokenizers\n",
      "\n",
      "print(\"========= CharBPETokenizer ==========\")\n",
      "# CharBPETokenizer\n",
      "tokenizer = CharBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "tokenizer.save(MODELDIR, 'char_bpe')\n",
      "\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "\n",
      "print(\"========= ByteLevelBPETokenizer ==========\")\n",
      "# ByteLevelBPETokenizer\n",
      "tokenizer = ByteLevelBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'byte_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "\n",
      "print(\"========= SentencePieceBPETokenizer ==========\")\n",
      "# SentencePieceBPETokenizer\n",
      "tokenizer = SentencePieceBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "\n",
      "print(\"========= BertWordPieceTokenizer ==========\")\n",
      "# BertWordPieceTokenizer\n",
      "tokenizer = BertWordPieceTokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'bert_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']\n",
      "#tokenizer.save(MODELDIR, 'char_bpe')\n",
      "\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "\n",
      "print(\"========= ByteLevelBPETokenizer ==========\")\n",
      "# ByteLevelBPETokenizer\n",
      "tokenizer = ByteLevelBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'byte_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "\n",
      "print(\"========= SentencePieceBPETokenizer ==========\")\n",
      "# SentencePieceBPETokenizer\n",
      "tokenizer = SentencePieceBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "\n",
      "print(\"========= BertWordPieceTokenizer ==========\")\n",
      "# BertWordPieceTokenizer\n",
      "tokenizer = BertWordPieceTokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'bert_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "print('tokenizer',tokenizer)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print(output)   # Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                            special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "print('tokenizer',tokenizer)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print('output',output)   # Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                           special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "tokenizer1 = SentencePieceBPETokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "tokenizer2 = ByteLevelBPETokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "tokenizer3 = CharBPETokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "print('tokenizer',tokenizer)\n",
      "print('tokenizer1',tokenizer1)\n",
      "print('tokenizer2',tokenizer2)\n",
      "print('tokenizer3',tokenizer3)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print('output',output)   # Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                           special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "tokenizer1 = SentencePieceBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer2 = ByteLevelBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer3 = CharBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "print('tokenizer',tokenizer)\n",
      "print('tokenizer1',tokenizer1)\n",
      "print('tokenizer2',tokenizer2)\n",
      "print('tokenizer3',tokenizer3)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print('output',output)   # Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                           special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "tokenizer1 = SentencePieceBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer2 = ByteLevelBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer3 = CharBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "print('tokenizer',tokenizer)\n",
      "print('tokenizer1',tokenizer1)\n",
      "print('tokenizer2',tokenizer2)\n",
      "print('tokenizer3',tokenizer3)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "output1 = tokenizer1.encode(\"Hello, y'all! How are you?\")\n",
      "output2 = tokenizer2.encode(\"Hello, y'all! How are you?\")\n",
      "output3 = tokenizer3.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print('output',output) \n",
      "print('output1',output1)\n",
      "print('output2',output2)\n",
      "print('output3',output3)\n",
      "\n",
      "\n",
      "\n",
      "# Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                            special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "DATAFILE = 'pg16457.txt'\n",
      "MODELDIR = ''\n",
      "\n",
      "input_text = 'This is a test'\n",
      "\n",
      "# Training the tokenizers\n",
      "\n",
      "print(\"========= CharBPETokenizer ==========\")\n",
      "# CharBPETokenizer\n",
      "tokenizer = CharBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "tokenizer1 = SentencePieceBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer2 = ByteLevelBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer3 = CharBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "print('tokenizer',tokenizer)\n",
      "print('tokenizer1',tokenizer1)\n",
      "print('tokenizer2',tokenizer2)\n",
      "print('tokenizer3',tokenizer3)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "output1 = tokenizer1.encode(\"Hello, y'all! How are you?\")\n",
      "output2 = tokenizer2.encode(\"Hello, y'all! How are you?\")\n",
      "output3 = tokenizer3.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print('output',output.tokens()) \n",
      "print('output1',output1.tokens())\n",
      "print('output2',output2.tokens())\n",
      "print('output3',output3.tokens())\n",
      "\n",
      "\n",
      "\n",
      "# Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                            special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "tokenizer1 = SentencePieceBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer2 = ByteLevelBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer3 = CharBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "print('tokenizer',tokenizer)\n",
      "print('tokenizer1',tokenizer1)\n",
      "print('tokenizer2',tokenizer2)\n",
      "print('tokenizer3',tokenizer3)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "output1 = tokenizer1.encode(\"Hello, y'all! How are you?\")\n",
      "output2 = tokenizer2.encode(\"Hello, y'all! How are you?\")\n",
      "output3 = tokenizer3.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print('output',output) \n",
      "print('output1',output1)\n",
      "print('output2',output2)\n",
      "print('output3',output3)\n",
      "\n",
      "\n",
      "\n",
      "# Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                            special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "print('output',output.tokens) \n",
      "print('output1',output1.tokens)\n",
      "print('output2',output2.tokens)\n",
      "print('output3',output3.tokens)\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "tokenizer1 = SentencePieceBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer2 = ByteLevelBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer3 = CharBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "print('tokenizer',tokenizer)\n",
      "print('tokenizer1',tokenizer1)\n",
      "print('tokenizer2',tokenizer2)\n",
      "print('tokenizer3',tokenizer3)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "output1 = tokenizer1.encode(\"Hello, y'all! How are you?\")\n",
      "output2 = tokenizer2.encode(\"Hello, y'all! How are you?\")\n",
      "output3 = tokenizer3.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print('output',output) \n",
      "print('output1',output1)\n",
      "print('output2',output2)\n",
      "print('output3',output3)\n",
      "\n",
      "\n",
      "\n",
      "# Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                            special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "print('output',output.tokens) \n",
      "print('output1',output1.tokens)\n",
      "print('output2',output2.tokens)\n",
      "print('output3',output3.tokens)\n",
      "DATAFILE = 'pg16457.txt'\n",
      "MODELDIR = ''\n",
      "\n",
      "input_text = 'This is a test'\n",
      "\n",
      "# Training the tokenizers\n",
      "\n",
      "print(\"========= CharBPETokenizer ==========\")\n",
      "# CharBPETokenizer\n",
      "tokenizer = CharBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "#tokenizer.save(MODELDIR, 'char_bpe')\n",
      "\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "\n",
      "print(\"========= ByteLevelBPETokenizer ==========\")\n",
      "# ByteLevelBPETokenizer\n",
      "tokenizer = ByteLevelBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'byte_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "\n",
      "print(\"========= SentencePieceBPETokenizer ==========\")\n",
      "# SentencePieceBPETokenizer\n",
      "tokenizer = SentencePieceBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "\n",
      "print(\"========= BertWordPieceTokenizer ==========\")\n",
      "# BertWordPieceTokenizer\n",
      "tokenizer = BertWordPieceTokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'bert_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']\n",
      "from tokenizers import (BertWordPieceTokenizer,\n",
      "                        SentencePieceBPETokenizer,\n",
      "                        ByteLevelBPETokenizer,\n",
      "                        CharBPETokenizer)\n",
      "\n",
      "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
      "\n",
      "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
      "tokenizer1 = SentencePieceBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer2 = ByteLevelBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "tokenizer3 = CharBPETokenizer(\"bert-base-uncased-vocab.txt\")\n",
      "print('tokenizer',tokenizer)\n",
      "print('tokenizer1',tokenizer1)\n",
      "print('tokenizer2',tokenizer2)\n",
      "print('tokenizer3',tokenizer3)\n",
      "# Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK],\n",
      "# sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK],\n",
      "# clean_text=True, handle_chinese_chars=True, strip_accents=True,\n",
      "# lowercase=True, wordpieces_prefix=##)\n",
      "\n",
      "# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n",
      "# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n",
      "output = tokenizer.encode(\"Hello, y'all! How are you?\")\n",
      "output1 = tokenizer1.encode(\"Hello, y'all! How are you?\")\n",
      "output2 = tokenizer2.encode(\"Hello, y'all! How are you?\")\n",
      "output3 = tokenizer3.encode(\"Hello, y'all! How are you?\")\n",
      "\n",
      "print('output',output) \n",
      "print('output1',output1)\n",
      "print('output2',output2)\n",
      "print('output3',output3)\n",
      "\n",
      "\n",
      "\n",
      "# Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask,                            special_tokens_mask, overflowing])\n",
      "print(f\"ids: {output.ids}\") # [101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 1029, 102]\n",
      "print(f\"type_ids: {output.type_ids}\")   # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "print(f\"tokens: {output.tokens}\")   # ['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are',                                           'you', '?', '[SEP]']\n",
      "print(f\"offsets: {output.offsets}\") # [(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), \n",
      "                                    #  (14,17), (18, 21), (22, 25), (25, 26), (0, 0)]\n",
      "print(f\"attention_mask: {output.attention_mask}\")   # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "print(f\"special_tokens_mask: {output.special_tokens_mask}\") # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "print(f\"overflowing: {output.overflowing}\") # []\n",
      "\n",
      "# Provided tokenizers\n",
      "# CharBPETokenizer: The original BPE\n",
      "# ByteLevelBPETokenizer: The byte level version of the BPE\n",
      "# SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
      "# BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
      "DATAFILE = 'pg16457.txt'\n",
      "MODELDIR = ''\n",
      "\n",
      "input_text = 'This is a test'\n",
      "\n",
      "# Training the tokenizers\n",
      "\n",
      "print(\"========= CharBPETokenizer ==========\")\n",
      "# CharBPETokenizer\n",
      "tokenizer = CharBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "#tokenizer.save(MODELDIR, 'char_bpe')\n",
      "\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'his</w>', 'is</w>', 'a</w>', 't', 'est</w>']\n",
      "\n",
      "print(\"========= ByteLevelBPETokenizer ==========\")\n",
      "# ByteLevelBPETokenizer\n",
      "tokenizer = ByteLevelBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'byte_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['T', 'h', 'is', 'Ġis', 'Ġa', 'Ġt', 'est']\n",
      "\n",
      "print(\"========= SentencePieceBPETokenizer ==========\")\n",
      "# SentencePieceBPETokenizer\n",
      "tokenizer = SentencePieceBPETokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'tok_sp_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "\n",
      "print(\"========= BertWordPieceTokenizer ==========\")\n",
      "# BertWordPieceTokenizer\n",
      "tokenizer = BertWordPieceTokenizer()\n",
      "tokenizer.train([DATAFILE], vocab_size=500)\n",
      "\n",
      "#tokenizer.save(MODELDIR, 'bert_bpe')\n",
      "output = tokenizer.encode(input_text)\n",
      "print(output.tokens)    # ['this', 'is', 'a', 't', '##est']\n",
      "%history\n"
     ]
    }
   ],
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
