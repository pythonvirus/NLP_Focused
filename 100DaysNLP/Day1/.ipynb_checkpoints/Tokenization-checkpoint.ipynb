{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T13:38:51.573861Z",
     "start_time": "2020-07-26T13:38:24.893453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/03/6cd0c8340ebcecf45f12540a852aede273263f0c757a4a8cea4042fbf715/sentencepiece-0.1.92-cp37-cp37m-win_amd64.whl (1.2MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.92\n"
     ]
    }
   ],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T13:51:18.989768Z",
     "start_time": "2020-07-26T13:51:18.960746Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T13:51:19.367439Z",
     "start_time": "2020-07-26T13:51:19.362448Z"
    }
   },
   "outputs": [],
   "source": [
    "DATAFILE = 'E:/Sachin/Learning/AI_Learning/7.NLP/100DaysNLP/100-Days-of-NLP-master/100-Days-of-NLP-master/data/pg16457.txt'\n",
    "MODELDIR = 'models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T13:52:18.891071Z",
     "start_time": "2020-07-26T13:52:14.681070Z"
    }
   },
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(f'''\\\n",
    "    --model_type=unigram\\\n",
    "    --input={DATAFILE}\\\n",
    "    --model_prefix={MODELDIR}/uni\\\n",
    "    --vocab_size=500''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T13:59:34.433790Z",
     "start_time": "2020-07-26T13:59:34.396790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(os.path.join(MODELDIR, 'uni.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T13:59:47.910037Z",
     "start_time": "2020-07-26T13:59:47.906043Z"
    }
   },
   "outputs": [],
   "source": [
    "input_string = \"This is a test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:01:48.053019Z",
     "start_time": "2020-07-26T14:01:48.045013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁t', 'est']\n",
      "[371, 77, 13, 101, 181]\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "#Space is encoded \"_\"\n",
    "# by default a space is added at the start of the input sentence\n",
    "print(sp.encode_as_pieces(input_string))    # ['▁This', '▁is', '▁a', '▁t', 'est']\n",
    "print(sp.encode_as_ids(input_string))       # [371, 77, 13, 101, 181]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:03:17.138344Z",
     "start_time": "2020-07-26T14:03:17.127366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test\n",
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "# decode: id => text\n",
    "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))    # This is a test\n",
    "print(sp.decode_ids([371, 77, 13, 101, 181]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:03:41.540212Z",
     "start_time": "2020-07-26T14:03:41.534210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 500\n"
     ]
    }
   ],
   "source": [
    "# returns vocab size\n",
    "print(f\"vocab size: {sp.get_piece_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:03:59.422592Z",
     "start_time": "2020-07-26T14:03:59.414600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 371 to piece: ▁This\n",
      "Piece ▁This to id: 371\n"
     ]
    }
   ],
   "source": [
    "# id <=> piece conversion\n",
    "print(f\"id 371 to piece: {sp.id_to_piece(371)}\")\n",
    "print(f\"Piece ▁This to id: {sp.piece_to_id('▁This')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- This is important since SentencePiece enables the subword process to be reversible.\n",
    "-  You can encode your test sentence in ID’s or in subword tokens; what you use is up to you.\n",
    "-  The key is that you can decode either the IDs or the tokens perfectly back into the original sentences,\n",
    "-  including the original spaces. Previously this was not possible with other tokenizers since they just provided the tokens and it was not clear exactly what encoding scheme was used,\n",
    "-  e.g. how did they deal with spaces or punctuation? This is a big selling point for SentencePiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:05:02.688266Z",
     "start_time": "2020-07-26T14:05:02.681258Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tokens = ['▁This', '▁is', '▁a', '▁t', 'est']\n",
    "merged = \"\".join(tokens).replace('▁', \" \").strip()\n",
    "assert merged == input_string, \"Input string and detokenized sentence didn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:05:09.975934Z",
     "start_time": "2020-07-26T14:05:09.966951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:17:38.478756Z",
     "start_time": "2020-07-26T14:17:38.469757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[371, 77, 94, 21, 9]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.tokenize('This is demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:18:04.614397Z",
     "start_time": "2020-07-26T14:18:04.606393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[138, 11, 110, 39, 323, 272, 8, 11]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.tokenize('Sachin Gupta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:21:58.554855Z",
     "start_time": "2020-07-26T14:21:58.545852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "a\n",
      "ch\n",
      "in\n",
      "G\n",
      "up\n",
      "t\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "#It will not out of bad error....like other tokenizers\n",
    "for i in [138, 11, 110, 39, 323, 272, 8, 11]:\n",
    "    print(sp.decode_ids([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:24:00.903134Z",
     "start_time": "2020-07-26T14:24:00.895133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "an\n",
      "c\n",
      "t\n",
      "ec\n",
      "D\n",
      "at\n",
      "as\n",
      "ci\n",
      "ence\n",
      "T\n",
      "e\n",
      "a\n",
      "m\n",
      "\n",
      "R\n",
      "o\n",
      "ck\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "for i in sp.tokenize('Banctec Datascience Team Rocks'):\n",
    "    print(sp.decode_ids([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
