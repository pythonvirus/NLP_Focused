{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Basic Thoery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A Quick Rundown of Tokenization Tokenization is a common task in Natural Language Processing (NLP). It’s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like Transformers. Tokens are the building blocks of Natural Language.\n",
    "2. Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why :\n",
    "\n",
    "Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\n",
    "\n",
    "And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where we use it:\n",
    "- In NLP before we use it for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What it does\n",
    "- As tokens are the building blocks of Natural Language, the most common way of processing the raw text happens at the token level.\n",
    "- Does all the pre-processing: Truncate, Pad, add the special tokens your model needs.\n",
    "- Creating Vocabulary is the ultimate goal of Tokenization.\n",
    "- Traditional NLP approaches such as Count Vectorizer and TF-IDF use vocabulary as features. Each word in the vocabulary is treated as a unique feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Tokenization\n",
    "### Word tokens\n",
    "1. Word Tokenization is the most commonly used tokenization algorithm. It splits a piece of text into individual words based on a certain delimiter. Depending upon delimiters, different word-level tokens are formed. Pretrained Word Embeddings such as Word2Vec and GloVe comes under word tokenization.\n",
    "2. One of the major issues with word tokens is dealing with Out Of Vocabulary (OOV) words. OOV words refer to the new words which are encountered at testing. These new words do not exist in the vocabulary. Hence, these methods fail in handling OOV words.\n",
    "3. A small trick can rescue word tokenizers from OOV words. The trick is to form the vocabulary with the Top K Frequent Words and replace the rare words in training data with unknown tokens (UNK). This helps the model to learn the representation of OOV words in terms of UNK tokens\n",
    "4. every out of word is same so they loose the sense of meaning, large in size \n",
    "5. Thats why we have character tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character tokenization\n",
    "1. Character Tokenization splits apiece of text into a set of characters. It overcomes the drawbacks we saw above about Word Tokenization.\n",
    "\n",
    "2. Character Tokenizers handles OOV words coherently by preserving the information of the word. It breaks down the OOV word into characters and represents the word in terms of these characters\n",
    "3. It also limits the size of the vocabulary. Want to talk a guess on the size of the vocabulary? 26 since the vocabulary contains a unique set of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword tokenization\n",
    "\n",
    "1. Character tokens solve the OOV problem but the length of the input and output sentences increases rapidly as we are representing a sentence as a sequence of characters. As a result, it becomes challenging to learn the relationship between the characters to form meaningful words.\n",
    "\n",
    "2. This brings us to another tokenization known as Subword Tokenization which is in between a Word and Character \n",
    "3. Subword Tokenization splits the piece of text into subwords (or n-gram characters). For example, words like lower can be segmented as low-er, smartest as smart-est, and so on.\n",
    "\n",
    "4. Transformed based models – the SOTA in NLP – rely on Subword Tokenization algorithms for preparing vocabulary. Now, I will discuss one of the most popular Subword Tokenization algorithm known as Byte Pair Encoding (BPE).tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At test time, the OOV word is split into sequences of characters. Then the learned operations are applied to merge the characters into larger known symbols.\n",
    "\n",
    "– Neural Machine Translation of Rare Words with Subword Units, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:06:37.094301Z",
     "start_time": "2020-08-07T02:05:44.706827Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:13:24.024187Z",
     "start_time": "2020-08-07T02:13:22.865864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'is', 'looking', 'at', 'buying', '\"', 'U.K.', '\"', 'startup', 'for', '$', '1', 'billion', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "# direct use\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = '''Apple is looking at buying \"U.K.\" startup for $1 billion!'''\n",
    "doc = nlp(text)\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:10:01.648839Z",
     "start_time": "2020-08-07T02:10:01.616189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sachin_gupta is working as Leaddatascientist"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2=nlp(\"Sachin_gupta is working as Leaddatascientist\")\n",
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:10:19.137118Z",
     "start_time": "2020-08-07T02:10:19.130115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Tokens =======\n",
      "Sachin_gupta\n",
      "is\n",
      "working\n",
      "as\n",
      "Leaddatascientist\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n======= Tokens =======\")\n",
    "# tokens\n",
    "for token in doc2:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:08:02.717553Z",
     "start_time": "2020-08-07T02:08:02.710550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Tokens =======\n",
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "\"\n",
      "U.K.\n",
      "\"\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n======= Tokens =======\")\n",
    "# tokens\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:08:40.526651Z",
     "start_time": "2020-08-07T02:08:40.341130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Tokenization explaination =======\n",
      "Apple \t TOKEN\n",
      "is \t TOKEN\n",
      "looking \t TOKEN\n",
      "at \t TOKEN\n",
      "buying \t TOKEN\n",
      "\" \t PREFIX\n",
      "U.K. \t TOKEN\n",
      "\" \t SUFFIX\n",
      "startup \t TOKEN\n",
      "for \t TOKEN\n",
      "$ \t PREFIX\n",
      "1 \t TOKEN\n",
      "billion \t TOKEN\n",
      "! \t SUFFIX\n"
     ]
    }
   ],
   "source": [
    "# token explaination\n",
    "print(\"\\n======= Tokenization explaination =======\")\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\t\", t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:10:54.845336Z",
     "start_time": "2020-08-07T02:10:54.767037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Tokens information =======\n",
      "token: Apple,    lemmatization: Apple,    pos: PROPN,    is_alpha: True,    is_stopword: False\n",
      "token: is,    lemmatization: be,    pos: AUX,    is_alpha: True,    is_stopword: True\n",
      "token: looking,    lemmatization: look,    pos: VERB,    is_alpha: True,    is_stopword: False\n",
      "token: at,    lemmatization: at,    pos: ADP,    is_alpha: True,    is_stopword: True\n",
      "token: buying,    lemmatization: buy,    pos: VERB,    is_alpha: True,    is_stopword: False\n",
      "token: \",    lemmatization: \",    pos: PUNCT,    is_alpha: False,    is_stopword: False\n",
      "token: U.K.,    lemmatization: U.K.,    pos: PROPN,    is_alpha: False,    is_stopword: False\n",
      "token: \",    lemmatization: \",    pos: PUNCT,    is_alpha: False,    is_stopword: False\n",
      "token: startup,    lemmatization: startup,    pos: NOUN,    is_alpha: True,    is_stopword: False\n",
      "token: for,    lemmatization: for,    pos: ADP,    is_alpha: True,    is_stopword: True\n",
      "token: $,    lemmatization: $,    pos: SYM,    is_alpha: False,    is_stopword: False\n",
      "token: 1,    lemmatization: 1,    pos: NUM,    is_alpha: False,    is_stopword: False\n",
      "token: billion,    lemmatization: billion,    pos: NUM,    is_alpha: True,    is_stopword: False\n",
      "token: !,    lemmatization: !,    pos: PUNCT,    is_alpha: False,    is_stopword: False\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Detokenization without doc is difficult in spacy. \n",
    "\n",
    "print(\"\\n======= Tokens information =======\")\n",
    "# spacy offers a lot of other information along with tokens\n",
    "for token in doc:\n",
    "    print(f\"\"\"token: {token.text},\\\n",
    "    lemmatization: {token.lemma_},\\\n",
    "    pos: {token.pos_},\\\n",
    "    is_alpha: {token.is_alpha},\\\n",
    "    is_stopword: {token.is_stop}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:11:22.270042Z",
     "start_time": "2020-08-07T02:11:22.240025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Customization =======\n",
      "['gimme', 'that']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n======= Customization =======\")\n",
    "# customization\n",
    "doc = nlp(\"gimme that\")  # phrase to tokenize\n",
    "print([w.text for w in doc])  # ['gimme', 'that']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:11:30.453966Z",
     "start_time": "2020-08-07T02:11:30.446965Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add special case rule\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:11:35.308218Z",
     "start_time": "2020-08-07T02:11:35.279218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gim', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "# Check new tokenization\n",
    "print([w.text for w in nlp(\"gimme that\")])  # ['gim', 'me', 'that']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T02:12:05.934741Z",
     "start_time": "2020-08-07T02:12:05.906760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.....', 'gim', 'me', '!', '!', '!', '!', 'that']\n"
     ]
    }
   ],
   "source": [
    "# The special case rules have precedence over the punctuation splitting\n",
    "doc = nlp(\".....gimme!!!! that\")    # phrase to tokenize\n",
    "print([w.text for w in doc])    # ['.....', 'gim', 'me', '!', '!', '!', '!', 'that']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchText Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-09T08:16:19.817Z"
    }
   },
   "outputs": [],
   "source": [
    "#Setting up pytorch environment\n",
    "!pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.\n",
    "\n",
    "#Then install the tourchtext\n",
    "pip install tourchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T10:52:36.755829Z",
     "start_time": "2020-08-09T10:52:31.804928Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "spacy_tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
    "print(f\"Spacy tokens: {spacy_tokens}\")  # ['You', 'can', 'now', 'install', 'TorchText', 'using', 'pip', '!']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "basic_english_tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
    "print(f\"Basic English tokens: {basic_english_tokens}\") # ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n",
    "# note that all the tokens are converted into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = get_tokenizer(\"moses\")\n",
    "moses_tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
    "print(f\"Moses tokens: {moses_tokens}\")  # ['You', 'can', 'now', 'install', 'TorchText', 'using', 'pip', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom tokenizer\n",
    "let's see how to configure sentencepiece tokenizer to torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-07T02:22:16.241Z"
    }
   },
   "outputs": [],
   "source": [
    "DATAFILE = '../data/pg16457.txt'\n",
    "MODELDIR = 'models'\n",
    "\n",
    "spm.SentencePieceTrainer.train(f'''\\\n",
    "    --model_type=bpe\\\n",
    "    --input={DATAFILE}\\\n",
    "    --model_prefix={MODELDIR}/bpe\\\n",
    "    --vocab_size=500''')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(os.path.join(MODELDIR, 'bpe.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(sentence):\n",
    "    return sp.encode_as_pieces(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-order to provide a custom tokenizer, it must have the functionality \n",
    "# of taking a single string and should provide the tokens for the string\n",
    "tokenizer = get_tokenizer(custom_tokenizer)\n",
    "sp_tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
    "print(f\"sp tokens: {sp_tokens}\")  # ['▁', 'Y', 'ou', '▁can', '▁now', '▁in', 'st', 'all', '▁T', 'or', 'ch', 'T', 'e', 'x', 't', '▁us', 'ing', '▁p', 'i', 'p', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T13:38:51.573861Z",
     "start_time": "2020-07-26T13:38:24.893453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/03/6cd0c8340ebcecf45f12540a852aede273263f0c757a4a8cea4042fbf715/sentencepiece-0.1.92-cp37-cp37m-win_amd64.whl (1.2MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.92\n"
     ]
    }
   ],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T10:06:49.744276Z",
     "start_time": "2020-07-27T10:06:49.700275Z"
    }
   },
   "source": [
    "Type of Subword Embedding\n",
    "- https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:02:11.545024Z",
     "start_time": "2020-08-03T04:02:11.334382Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:02:12.197081Z",
     "start_time": "2020-08-03T04:02:12.192077Z"
    }
   },
   "outputs": [],
   "source": [
    "DATAFILE = 'E:/Sachin/Learning/AI_Learning/7.NLP/100DaysNLP/100-Days-of-NLP-master/100-Days-of-NLP-master/data/pg16457.txt'\n",
    "MODELDIR = 'models'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T03:55:58.742607Z",
     "start_time": "2020-08-03T03:55:58.730615Z"
    }
   },
   "source": [
    "# Subword embedding \n",
    "It help to resolve the below two problems\n",
    "1. Out of Vocabulary error\n",
    "2. Seplling mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:02:15.941406Z",
     "start_time": "2020-08-03T04:02:14.773274Z"
    }
   },
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(f'''\\\n",
    "    --model_type=bpe\\\n",
    "    --input={DATAFILE}\\\n",
    "    --model_prefix={MODELDIR}/bpe\\\n",
    "    --vocab_size=500''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:02:16.755105Z",
     "start_time": "2020-08-03T04:02:16.730103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(os.path.join(MODELDIR, 'bpe.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:02:18.440601Z",
     "start_time": "2020-08-03T04:02:18.431598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
      "[72, 435, 26, 101, 5, 3, 153]\n"
     ]
    }
   ],
   "source": [
    "input_string = \"This is a test\"\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces(input_string))    # ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
    "print(sp.encode_as_ids(input_string))       # [72, 435, 26, 101, 5, 3, 153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:03:06.938628Z",
     "start_time": "2020-08-03T04:03:06.929625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test\n",
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "# decode: id => text\n",
    "print(sp.decode_pieces(['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']))    # This is a test\n",
    "print(sp.decode_ids([72, 435, 26, 101, 5, 3, 153]))                       # This is a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:03:12.587475Z",
     "start_time": "2020-08-03T04:03:12.580465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 500\n",
      "id 101 to piece: ▁is\n",
      "Piece ▁is to id: 101\n"
     ]
    }
   ],
   "source": [
    "# returns vocab size\n",
    "print(f\"vocab size: {sp.get_piece_size()}\")\n",
    "\n",
    "# id <=> piece conversion\n",
    "print(f\"id 101 to piece: {sp.id_to_piece(101)}\")\n",
    "print(f\"Piece ▁is to id: {sp.piece_to_id('▁is')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T03:54:04.475806Z",
     "start_time": "2020-08-03T03:54:04.458812Z"
    }
   },
   "source": [
    "- You can see from the code that we used the “id_to_piece” function which turns the ID of a token into its corresponding textual representation.\n",
    "\n",
    "- This is important since SentencePiece enables the subword process to be reversible.\n",
    "- You can encode your test sentence in ID’s or in subword tokens; what you use is up to you.\n",
    "- The key is that you can decode either the IDs or the tokens perfectly back into the original sentences,\n",
    "- Including the original spaces. Previously this was not possible with other tokenizers since they just provided the tokens and it was not clear exactly what encoding scheme was used, e.g. how did they deal with spaces or punctuation? This is a big selling point for SentencePiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['▁T', 'h', 'is', '▁is', '▁a', '▁t', 'est']\n",
    "merged = \"\".join(tokens).replace('▁', \" \").strip()\n",
    "assert merged == input_string, \"Input string and detokenized sentence didn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:06:57.217863Z",
     "start_time": "2020-08-03T04:06:57.208883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> False\n",
      "<s> True\n",
      "</s> True\n",
      "▁t False\n",
      "he False\n"
     ]
    }
   ],
   "source": [
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "# control symbol: We only reserve ids for these tokens. Even if these tokens appear in the input text, \n",
    "#they are not handled as one token. User needs to insert ids explicitly after encoding.\n",
    "for id in range(5):\n",
    "  print(sp.id_to_piece(id), sp.is_control(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T03:58:02.080587Z",
     "start_time": "2020-08-03T03:58:02.061588Z"
    }
   },
   "source": [
    "- We can define special tokens (symbols) to tweak the DNN behavior through the tokens. Typical examples are BERT's special symbols., e.g., [SEP] and [CLS].\n",
    "\n",
    "- There are two types of special tokens:\n",
    "\n",
    "- user defined symbols: Always treated as one token in any context. These symbols can appear in the input sentence.\n",
    "- control symbol: We only reserve ids for these tokens. Even if these tokens appear in the input text, they are not handled as one token. User needs to insert ids explicitly after encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer to this for more details: https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb#scrollTo=dngckiPMcWbA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:39:15.286571Z",
     "start_time": "2020-08-03T04:39:14.227574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Example of user defined symbols\n",
    "spm.SentencePieceTrainer.train(f'''\\\n",
    "    --model_type=bpe\\\n",
    "    --input={DATAFILE}\\\n",
    "    --model_prefix={MODELDIR}/bpe_user\\\n",
    "    --user_defined_symbols=<sep>,<cls>\\\n",
    "    --vocab_size=500''')\n",
    "sp_user = spm.SentencePieceProcessor()\n",
    "sp_user.load(os.path.join(MODELDIR, 'bpe_user.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:39:16.487277Z",
     "start_time": "2020-08-03T04:39:16.480276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁this', '▁is', '▁a', '▁t', 'est', '<sep>', '▁he', 'll', 'o', '▁wor', 'ld', '<cls>']\n",
      "3\n",
      "4\n",
      "3= <sep>\n",
      "4= <cls>\n"
     ]
    }
   ],
   "source": [
    "# ids are reserved in both mode.\n",
    "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
    "# user defined symbols allow these symbol to apper in the text.\n",
    "print(sp_user.encode_as_pieces('this is a test<sep> hello world<cls>')) # ['▁this', '▁is', '▁a', '▁t', 'est', '<sep>', '▁he', 'll', 'o', '▁wor', 'ld', '<cls>']\n",
    "print(sp_user.piece_to_id('<sep>'))  # 3\n",
    "print(sp_user.piece_to_id('<cls>'))  # 4\n",
    "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
    "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:39:46.289737Z",
     "start_time": "2020-08-03T04:39:46.280738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos= 1\n",
      "eos= 2\n",
      "unk= 0\n",
      "pad= -1\n"
     ]
    }
   ],
   "source": [
    "print('bos=', sp_user.bos_id())     # 1\n",
    "print('eos=', sp_user.eos_id())     # 2\n",
    "print('unk=', sp_user.unk_id())     # 0\n",
    "print('pad=', sp_user.pad_id())     # -1, disabled by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:40:05.715174Z",
     "start_time": "2020-08-03T04:40:05.708172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189, 320, 430, 233, 71]\n"
     ]
    }
   ],
   "source": [
    "print(sp_user.encode_as_ids('Hello world'))     # [189, 320, 430, 233, 71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T04:40:12.636046Z",
     "start_time": "2020-08-03T04:40:12.628052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 189, 320, 430, 233, 71, 2]\n"
     ]
    }
   ],
   "source": [
    "# Prepend or append bos/eos ids.\n",
    "print([sp_user.bos_id()] + sp_user.encode_as_ids('Hello world') + [sp_user.eos_id()])   # [1, 189, 320, 430, 233, 71, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Example in BERT Tokenization\n",
    "https://colab.research.google.com/github/pythonvirus/AI-Learning/blob/master/Inspect_BERT_Vocabulary.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UniGram Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T13:32:23.440390Z",
     "start_time": "2020-07-27T13:32:08.929601Z"
    }
   },
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(f'''\\\n",
    "    --model_type=unigram\\\n",
    "    --input={DATAFILE}\\\n",
    "    --model_prefix={MODELDIR}/uni\\\n",
    "    --vocab_size=500''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T13:32:26.622947Z",
     "start_time": "2020-07-27T13:32:26.551951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(os.path.join(MODELDIR, 'uni.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T13:34:57.083457Z",
     "start_time": "2020-07-27T13:34:57.077457Z"
    }
   },
   "outputs": [],
   "source": [
    "input_string = \"This is a test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T13:34:58.442279Z",
     "start_time": "2020-07-27T13:34:58.431275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁t', 'est']\n",
      "[371, 77, 13, 101, 181]\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "#Space is encoded \"_\"\n",
    "# by default a space is added at the start of the input sentence\n",
    "print(sp.encode_as_pieces(input_string))    # ['▁This', '▁is', '▁a', '▁t', 'est']\n",
    "print(sp.encode_as_ids(input_string))       # [371, 77, 13, 101, 181]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:03:17.138344Z",
     "start_time": "2020-07-26T14:03:17.127366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test\n",
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "# decode: id => text\n",
    "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))    # This is a test\n",
    "print(sp.decode_ids([371, 77, 13, 101, 181]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:03:41.540212Z",
     "start_time": "2020-07-26T14:03:41.534210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 500\n"
     ]
    }
   ],
   "source": [
    "# returns vocab size\n",
    "print(f\"vocab size: {sp.get_piece_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:03:59.422592Z",
     "start_time": "2020-07-26T14:03:59.414600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 371 to piece: ▁This\n",
      "Piece ▁This to id: 371\n"
     ]
    }
   ],
   "source": [
    "# id <=> piece conversion\n",
    "print(f\"id 371 to piece: {sp.id_to_piece(371)}\")\n",
    "print(f\"Piece ▁This to id: {sp.piece_to_id('▁This')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- This is important since SentencePiece enables the subword process to be reversible.\n",
    "-  You can encode your test sentence in ID’s or in subword tokens; what you use is up to you.\n",
    "-  The key is that you can decode either the IDs or the tokens perfectly back into the original sentences,\n",
    "-  including the original spaces. Previously this was not possible with other tokenizers since they just provided the tokens and it was not clear exactly what encoding scheme was used,\n",
    "-  e.g. how did they deal with spaces or punctuation? This is a big selling point for SentencePiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:05:02.688266Z",
     "start_time": "2020-07-26T14:05:02.681258Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tokens = ['▁This', '▁is', '▁a', '▁t', 'est']\n",
    "merged = \"\".join(tokens).replace('▁', \" \").strip()\n",
    "assert merged == input_string, \"Input string and detokenized sentence didn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T14:05:09.975934Z",
     "start_time": "2020-07-26T14:05:09.966951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T13:37:36.276624Z",
     "start_time": "2020-07-27T13:37:36.266636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[371, 77, 94, 21, 9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.tokenize('This is demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T13:38:03.948911Z",
     "start_time": "2020-07-27T13:38:03.937912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[138, 11, 110, 39, 323, 272, 8, 11]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.tokenize('Sachin Gupta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T13:38:09.965915Z",
     "start_time": "2020-07-27T13:38:09.954911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "a\n",
      "ch\n",
      "in\n",
      "G\n",
      "up\n",
      "t\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "#It will not out of bad error....like other tokenizers\n",
    "for i in [138, 11, 110, 39, 323, 272, 8, 11]:\n",
    "    print(sp.decode_ids([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T13:39:07.245585Z",
     "start_time": "2020-07-27T13:39:07.234580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "an\n",
      "c\n",
      "t\n",
      "ec\n",
      "D\n",
      "at\n",
      "as\n",
      "ci\n",
      "ence\n",
      "T\n",
      "e\n",
      "a\n",
      "m\n",
      "\n",
      "R\n",
      "o\n",
      "ck\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "for i in sp.tokenize('Banctec Datascience Team Rocks'):\n",
    "    print(sp.decode_ids([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
