{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:24:50.382598Z",
     "start_time": "2020-04-01T06:24:50.372604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Disable Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP functionalities and libraries\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Classification evaluation\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, \n",
    "                             recall_score, f1_score)\n",
    "\n",
    "# Word Cloud\n",
    "#from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:24:58.036612Z",
     "start_time": "2020-04-01T06:24:57.989604Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', \n",
    "                      quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:24:58.390610Z",
     "start_time": "2020-04-01T06:24:58.355600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:28:38.597776Z",
     "start_time": "2020-04-01T06:28:38.516793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow    Loved this place '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][0])\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:28:52.671083Z",
     "start_time": "2020-04-01T06:28:52.662080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow', 'loved', 'this', 'place']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = review.lower()\n",
    "review = review.split()\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:29:26.679009Z",
     "start_time": "2020-04-01T06:29:25.989697Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sachin.gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:29:32.001549Z",
     "start_time": "2020-04-01T06:29:31.993548Z"
    }
   },
   "outputs": [],
   "source": [
    "stopword_list=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow', 'love', 'place']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow love place'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = ' '.join(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:35:41.777522Z",
     "start_time": "2020-04-01T06:35:27.360960Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:47:23.953134Z",
     "start_time": "2020-04-01T06:47:23.945138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow love place',\n",
       " 'crust good',\n",
       " 'tasti textur nasti',\n",
       " 'stop late may bank holiday rick steve recommend love',\n",
       " 'select menu great price']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:38:41.934847Z",
     "start_time": "2020-04-01T09:38:41.909856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model with Count Vectors\n",
    "cv = CountVectorizer(max_features = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:38:42.653846Z",
     "start_time": "2020-04-01T09:38:42.432851Z"
    }
   },
   "outputs": [],
   "source": [
    "#vocab = cv.vocabulary_\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:38:43.184029Z",
     "start_time": "2020-04-01T09:38:43.179029Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_cv = cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:38:43.989557Z",
     "start_time": "2020-04-01T09:38:43.949561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wow': 792,\n",
       " 'love': 406,\n",
       " 'place': 505,\n",
       " 'crust': 163,\n",
       " 'good': 310,\n",
       " 'tasti': 712,\n",
       " 'textur': 718,\n",
       " 'nasti': 449,\n",
       " 'stop': 681,\n",
       " 'late': 384,\n",
       " 'may': 420,\n",
       " 'rick': 555,\n",
       " 'steve': 674,\n",
       " 'recommend': 544,\n",
       " 'select': 592,\n",
       " 'menu': 429,\n",
       " 'great': 313,\n",
       " 'price': 521,\n",
       " 'get': 303,\n",
       " 'want': 768,\n",
       " 'damn': 168,\n",
       " 'pho': 499,\n",
       " 'tast': 710,\n",
       " 'fresh': 292,\n",
       " 'potato': 517,\n",
       " 'like': 394,\n",
       " 'rubber': 569,\n",
       " 'could': 152,\n",
       " 'tell': 714,\n",
       " 'made': 411,\n",
       " 'time': 728,\n",
       " 'kept': 375,\n",
       " 'fri': 293,\n",
       " 'touch': 737,\n",
       " 'servic': 596,\n",
       " 'would': 791,\n",
       " 'go': 307,\n",
       " 'back': 44,\n",
       " 'cashier': 110,\n",
       " 'care': 109,\n",
       " 'ever': 248,\n",
       " 'say': 583,\n",
       " 'still': 676,\n",
       " 'end': 239,\n",
       " 'overpr': 478,\n",
       " 'tri': 741,\n",
       " 'chicken': 120,\n",
       " 'mmmm': 437,\n",
       " 'disgust': 212,\n",
       " 'pretti': 520,\n",
       " 'sure': 699,\n",
       " 'human': 354,\n",
       " 'hair': 324,\n",
       " 'sign': 605,\n",
       " 'highli': 339,\n",
       " 'waitress': 765,\n",
       " 'littl': 397,\n",
       " 'slow': 617,\n",
       " 'worth': 790,\n",
       " 'let': 391,\n",
       " 'vega': 756,\n",
       " 'food': 290,\n",
       " 'amaz': 9,\n",
       " 'also': 6,\n",
       " 'cute': 167,\n",
       " 'beauti': 59,\n",
       " 'right': 557,\n",
       " 'red': 545,\n",
       " 'cake': 102,\n",
       " 'stuff': 690,\n",
       " 'never': 453,\n",
       " 'brought': 90,\n",
       " 'salad': 574,\n",
       " 'ask': 24,\n",
       " 'wall': 767,\n",
       " 'mexican': 430,\n",
       " 'street': 686,\n",
       " 'taco': 704,\n",
       " 'friendli': 295,\n",
       " 'staff': 662,\n",
       " 'took': 733,\n",
       " 'hour': 350,\n",
       " 'tabl': 703,\n",
       " 'restaur': 550,\n",
       " 'warm': 769,\n",
       " 'sever': 598,\n",
       " 'run': 571,\n",
       " 'around': 21,\n",
       " 'total': 736,\n",
       " 'overwhelm': 479,\n",
       " 'worst': 789,\n",
       " 'salmon': 575,\n",
       " 'sashimi': 579,\n",
       " 'combo': 139,\n",
       " 'burger': 97,\n",
       " 'beer': 62,\n",
       " 'decent': 173,\n",
       " 'deal': 172,\n",
       " 'final': 277,\n",
       " 'blow': 77,\n",
       " 'found': 291,\n",
       " 'seem': 590,\n",
       " 'quick': 530,\n",
       " 'bite': 74,\n",
       " 'familiar': 264,\n",
       " 'favor': 270,\n",
       " 'look': 403,\n",
       " 'elsewher': 236,\n",
       " 'overal': 476,\n",
       " 'lot': 405,\n",
       " 'qualiti': 529,\n",
       " 'inexpens': 366,\n",
       " 'portion': 515,\n",
       " 'poor': 512,\n",
       " 'waiter': 764,\n",
       " 'feel': 272,\n",
       " 'everi': 249,\n",
       " 'came': 104,\n",
       " 'first': 281,\n",
       " 'visit': 762,\n",
       " 'delight': 184,\n",
       " 'suck': 694,\n",
       " 'shrimp': 602,\n",
       " 'tender': 716,\n",
       " 'moist': 438,\n",
       " 'enough': 241,\n",
       " 'establish': 245,\n",
       " 'hard': 330,\n",
       " 'judg': 374,\n",
       " 'side': 604,\n",
       " 'gross': 318,\n",
       " 'melt': 428,\n",
       " 'eat': 229,\n",
       " 'sick': 603,\n",
       " 'note': 461,\n",
       " 'server': 595,\n",
       " 'attent': 31,\n",
       " 'provid': 525,\n",
       " 'frozen': 297,\n",
       " 'peopl': 494,\n",
       " 'behind': 63,\n",
       " 'thing': 721,\n",
       " 'dessert': 195,\n",
       " 'bad': 46,\n",
       " 'gener': 302,\n",
       " 'beef': 61,\n",
       " 'cook': 149,\n",
       " 'sandwich': 578,\n",
       " 'greek': 314,\n",
       " 'dress': 223,\n",
       " 'pita': 503,\n",
       " 'hummu': 355,\n",
       " 'order': 471,\n",
       " 'duck': 228,\n",
       " 'rare': 533,\n",
       " 'insid': 367,\n",
       " 'nice': 456,\n",
       " 'char': 111,\n",
       " 'outsid': 473,\n",
       " 'us': 752,\n",
       " 'realiz': 539,\n",
       " 'husband': 356,\n",
       " 'left': 388,\n",
       " 'chow': 126,\n",
       " 'horribl': 348,\n",
       " 'attitud': 32,\n",
       " 'toward': 738,\n",
       " 'custom': 165,\n",
       " 'talk': 706,\n",
       " 'one': 468,\n",
       " 'enjoy': 240,\n",
       " 'huge': 353,\n",
       " 'wonder': 783,\n",
       " 'imagin': 359,\n",
       " 'heart': 335,\n",
       " 'attack': 30,\n",
       " 'grill': 317,\n",
       " 'downtown': 221,\n",
       " 'absolut': 0,\n",
       " 'flat': 283,\n",
       " 'excus': 253,\n",
       " 'much': 443,\n",
       " 'seafood': 585,\n",
       " 'pasta': 490,\n",
       " 'amount': 12,\n",
       " 'sauc': 582,\n",
       " 'scallop': 584,\n",
       " 'perfectli': 497,\n",
       " 'rip': 560,\n",
       " 'tasteless': 711,\n",
       " 'least': 386,\n",
       " 'think': 722,\n",
       " 'refil': 546,\n",
       " 'water': 772,\n",
       " 'minut': 434,\n",
       " 'receiv': 542,\n",
       " 'star': 665,\n",
       " 'appet': 19,\n",
       " 'cocktail': 134,\n",
       " 'delici': 182,\n",
       " 'definit': 178,\n",
       " 'glad': 306,\n",
       " 'give': 304,\n",
       " 'alway': 8,\n",
       " 'went': 776,\n",
       " 'second': 588,\n",
       " 'got': 311,\n",
       " 'heard': 334,\n",
       " 'salt': 577,\n",
       " 'batter': 55,\n",
       " 'fish': 282,\n",
       " 'chewi': 119,\n",
       " 'way': 773,\n",
       " 'finish': 280,\n",
       " 'includ': 363,\n",
       " 'drink': 226,\n",
       " 'beyond': 68,\n",
       " 'expect': 254,\n",
       " 'realli': 540,\n",
       " 'rice': 553,\n",
       " 'meh': 427,\n",
       " 'min': 432,\n",
       " 'noth': 462,\n",
       " 'guess': 320,\n",
       " 'known': 379,\n",
       " 'use': 753,\n",
       " 'dish': 213,\n",
       " 'quit': 532,\n",
       " 'valu': 755,\n",
       " 'well': 775,\n",
       " 'sweet': 702,\n",
       " 'season': 586,\n",
       " 'today': 731,\n",
       " 'lunch': 410,\n",
       " 'buffet': 93,\n",
       " 'wast': 770,\n",
       " 'opportun': 469,\n",
       " 'compani': 142,\n",
       " 'come': 140,\n",
       " 'experienc': 256,\n",
       " 'underwhelm': 749,\n",
       " 'parti': 488,\n",
       " 'wait': 763,\n",
       " 'person': 498,\n",
       " 'break': 86,\n",
       " 'walk': 766,\n",
       " 'smell': 622,\n",
       " 'old': 466,\n",
       " 'greas': 312,\n",
       " 'other': 472,\n",
       " 'roast': 562,\n",
       " 'bland': 76,\n",
       " 'everyon': 250,\n",
       " 'rave': 536,\n",
       " 'sugari': 695,\n",
       " 'disast': 209,\n",
       " 'six': 613,\n",
       " 'year': 795,\n",
       " 'spring': 660,\n",
       " 'roll': 564,\n",
       " 'oh': 464,\n",
       " 'yummi': 798,\n",
       " 'meat': 425,\n",
       " 'die': 198,\n",
       " 'everyth': 251,\n",
       " 'larg': 382,\n",
       " 'disappoint': 207,\n",
       " 'dine': 201,\n",
       " 'experi': 255,\n",
       " 'mouth': 442,\n",
       " 'rock': 563,\n",
       " 'step': 673,\n",
       " 'best': 66,\n",
       " 'breakfast': 87,\n",
       " 'bye': 100,\n",
       " 'tip': 729,\n",
       " 'ladi': 381,\n",
       " 'arriv': 22,\n",
       " 'quickli': 531,\n",
       " 'cafe': 101,\n",
       " 'serv': 594,\n",
       " 'fantast': 266,\n",
       " 'wife': 779,\n",
       " 'garlic': 300,\n",
       " 'marrow': 419,\n",
       " 'ad': 3,\n",
       " 'extra': 257,\n",
       " 'meal': 423,\n",
       " 'anoth': 13,\n",
       " 'help': 337,\n",
       " 'mari': 418,\n",
       " 'town': 739,\n",
       " 'cannot': 106,\n",
       " 'beat': 58,\n",
       " 'wine': 780,\n",
       " 'better': 67,\n",
       " 'bartend': 51,\n",
       " 'ambienc': 11,\n",
       " 'music': 446,\n",
       " 'play': 507,\n",
       " 'next': 455,\n",
       " 'trip': 743,\n",
       " 'sooooo': 639,\n",
       " 'real': 538,\n",
       " 'sushi': 701,\n",
       " 'lover': 407,\n",
       " 'honest': 345,\n",
       " 'pass': 489,\n",
       " 'busi': 98,\n",
       " 'thai': 719,\n",
       " 'spici': 654,\n",
       " 'check': 115,\n",
       " 'atmospher': 27,\n",
       " 'kind': 377,\n",
       " 'steak': 670,\n",
       " 'although': 7,\n",
       " 'sound': 643,\n",
       " 'actual': 2,\n",
       " 'bit': 73,\n",
       " 'know': 378,\n",
       " 'manag': 415,\n",
       " 'eaten': 230,\n",
       " 'prepar': 518,\n",
       " 'indian': 365,\n",
       " 'boot': 79,\n",
       " 'fine': 279,\n",
       " 'guy': 322,\n",
       " 'son': 635,\n",
       " 'said': 573,\n",
       " 'thought': 725,\n",
       " 'ventur': 759,\n",
       " 'away': 39,\n",
       " 'hit': 342,\n",
       " 'spot': 658,\n",
       " 'night': 458,\n",
       " 'lack': 380,\n",
       " 'word': 784,\n",
       " 'reason': 541,\n",
       " 'review': 552,\n",
       " 'leav': 387,\n",
       " 'ambianc': 10,\n",
       " 'return': 551,\n",
       " 'strip': 688,\n",
       " 'pork': 514,\n",
       " 'belli': 65,\n",
       " 'mediocr': 426,\n",
       " 'excel': 252,\n",
       " 'crispi': 161,\n",
       " 'wrap': 793,\n",
       " 'delish': 185,\n",
       " 'tuna': 744,\n",
       " 'rude': 570,\n",
       " 'bagel': 47,\n",
       " 'cream': 159,\n",
       " 'chees': 116,\n",
       " 'even': 246,\n",
       " 'subway': 693,\n",
       " 'fact': 260,\n",
       " 'serious': 593,\n",
       " 'solid': 628,\n",
       " 'bar': 49,\n",
       " 'extrem': 258,\n",
       " 'mani': 416,\n",
       " 'empti': 238,\n",
       " 'suggest': 696,\n",
       " 'ate': 26,\n",
       " 'curri': 164,\n",
       " 'top': 734,\n",
       " 'done': 216,\n",
       " 'cover': 155,\n",
       " 'bathroom': 54,\n",
       " 'clean': 132,\n",
       " 'decor': 175,\n",
       " 'consid': 146,\n",
       " 'pace': 481,\n",
       " 'thumb': 727,\n",
       " 'watch': 771,\n",
       " 'pay': 492,\n",
       " 'ignor': 358,\n",
       " 'day': 171,\n",
       " 'greet': 316,\n",
       " 'seat': 587,\n",
       " 'bay': 56,\n",
       " 'stale': 663,\n",
       " 'highlight': 340,\n",
       " 'joint': 372,\n",
       " 'differ': 199,\n",
       " 'cut': 166,\n",
       " 'piec': 502,\n",
       " 'flavor': 284,\n",
       " 'sinc': 610,\n",
       " 'ago': 4,\n",
       " 'unfortun': 750,\n",
       " 'must': 447,\n",
       " 'bakeri': 48,\n",
       " 'impress': 362,\n",
       " 'immedi': 360,\n",
       " 'avoid': 37,\n",
       " 'full': 298,\n",
       " 'hand': 326,\n",
       " 'phoenix': 500,\n",
       " 'area': 20,\n",
       " 'treat': 740,\n",
       " 'bacon': 45,\n",
       " 'spinach': 656,\n",
       " 'avocado': 36,\n",
       " 'sad': 572,\n",
       " 'liter': 396,\n",
       " 'zero': 799,\n",
       " 'list': 395,\n",
       " 'soi': 627,\n",
       " 'miss': 435,\n",
       " 'vegetarian': 758,\n",
       " 'desir': 192,\n",
       " 'hip': 341,\n",
       " 'sat': 580,\n",
       " 'take': 705,\n",
       " 'overcook': 477,\n",
       " 'charcoal': 112,\n",
       " 'decid': 174,\n",
       " 'probabl': 522,\n",
       " 'dirt': 204,\n",
       " 'someth': 632,\n",
       " 'healthi': 333,\n",
       " 'ice': 357,\n",
       " 'incred': 364,\n",
       " 'interest': 369,\n",
       " 'station': 668,\n",
       " 'hot': 349,\n",
       " 'bread': 85,\n",
       " 'butter': 99,\n",
       " 'home': 343,\n",
       " 'chip': 122,\n",
       " 'egg': 232,\n",
       " 'gyro': 323,\n",
       " 'wing': 781,\n",
       " 'satisfi': 581,\n",
       " 'dog': 215,\n",
       " 'valley': 754,\n",
       " 'bowl': 81,\n",
       " 'live': 398,\n",
       " 'insult': 368,\n",
       " 'felt': 274,\n",
       " 'disrespect': 214,\n",
       " 'drive': 227,\n",
       " 'hope': 347,\n",
       " 'brunch': 91,\n",
       " 'last': 383,\n",
       " 'mushroom': 445,\n",
       " 'gold': 308,\n",
       " 'pure': 527,\n",
       " 'white': 777,\n",
       " 'bug': 94,\n",
       " 'show': 600,\n",
       " 'given': 305,\n",
       " 'soon': 637,\n",
       " 'friend': 294,\n",
       " 'tartar': 709,\n",
       " 'though': 724,\n",
       " 'soggi': 626,\n",
       " 'small': 618,\n",
       " 'rich': 554,\n",
       " 'shower': 601,\n",
       " 'rins': 559,\n",
       " 'unless': 751,\n",
       " 'mind': 433,\n",
       " 'see': 589,\n",
       " 'lobster': 399,\n",
       " 'bisqu': 72,\n",
       " 'sprout': 661,\n",
       " 'risotto': 561,\n",
       " 'filet': 275,\n",
       " 'need': 450,\n",
       " 'pepper': 495,\n",
       " 'cours': 154,\n",
       " 'none': 460,\n",
       " 'someon': 631,\n",
       " 'either': 234,\n",
       " 'cold': 136,\n",
       " 'date': 170,\n",
       " 'unbeliev': 747,\n",
       " 'folk': 289,\n",
       " 'make': 413,\n",
       " 'special': 650,\n",
       " 'main': 412,\n",
       " 'world': 787,\n",
       " 'fun': 299,\n",
       " 'chef': 118,\n",
       " 'doubl': 219,\n",
       " 'cheeseburg': 117,\n",
       " 'singl': 611,\n",
       " 'pictur': 501,\n",
       " 'coupl': 153,\n",
       " 'sport': 657,\n",
       " 'event': 247,\n",
       " 'possibl': 516,\n",
       " 'descript': 190,\n",
       " 'yum': 797,\n",
       " 'yet': 796,\n",
       " 'mayo': 422,\n",
       " 'honestli': 346,\n",
       " 'eye': 259,\n",
       " 'stay': 669,\n",
       " 'money': 440,\n",
       " 'almost': 5,\n",
       " 'build': 95,\n",
       " 'close': 133,\n",
       " 'point': 511,\n",
       " 'light': 393,\n",
       " 'dark': 169,\n",
       " 'set': 597,\n",
       " 'mood': 441,\n",
       " 'sub': 692,\n",
       " 'par': 486,\n",
       " 'owner': 480,\n",
       " 'work': 785,\n",
       " 'creami': 160,\n",
       " 'similar': 606,\n",
       " 'complaint': 144,\n",
       " 'pizza': 504,\n",
       " 'peanut': 493,\n",
       " 'fast': 269,\n",
       " 'stick': 675,\n",
       " 'charg': 113,\n",
       " 'tap': 707,\n",
       " 'plu': 510,\n",
       " 'buck': 92,\n",
       " 'far': 267,\n",
       " 'twice': 745,\n",
       " 'coffe': 135,\n",
       " 'cant': 107,\n",
       " 'wrong': 794,\n",
       " 'job': 371,\n",
       " 'boba': 78,\n",
       " 'tea': 713,\n",
       " 'patio': 491,\n",
       " 'outstand': 474,\n",
       " 'skimp': 614,\n",
       " 'bachi': 43,\n",
       " 'stink': 677,\n",
       " 'hate': 331,\n",
       " 'disagre': 206,\n",
       " 'later': 385,\n",
       " 'neighborhood': 452,\n",
       " 'conveni': 148,\n",
       " 'locat': 400,\n",
       " 'pull': 526,\n",
       " 'soooo': 638,\n",
       " 'gave': 301,\n",
       " 'rate': 534,\n",
       " 'pleas': 508,\n",
       " 'third': 723,\n",
       " 'stir': 678,\n",
       " 'box': 82,\n",
       " 'dime': 200,\n",
       " 'atroci': 28,\n",
       " 'summer': 697,\n",
       " 'toast': 730,\n",
       " 'high': 338,\n",
       " 'hous': 351,\n",
       " 'boy': 83,\n",
       " 'basic': 52,\n",
       " 'joke': 373,\n",
       " 'fare': 268,\n",
       " 'two': 746,\n",
       " 'happi': 329,\n",
       " 'without': 782,\n",
       " 'doubt': 220,\n",
       " 'favorit': 271,\n",
       " 'black': 75,\n",
       " 'seen': 591,\n",
       " 'especi': 244,\n",
       " 'mom': 439,\n",
       " 'pleasant': 509,\n",
       " 'dirti': 205,\n",
       " 'standard': 664,\n",
       " 'omg': 467,\n",
       " 'delicioso': 183,\n",
       " 'authent': 34,\n",
       " 'spaghetti': 649,\n",
       " 'veget': 757,\n",
       " 'brick': 88,\n",
       " 'oven': 475,\n",
       " 'multipl': 444,\n",
       " 'ten': 715,\n",
       " 'terribl': 717,\n",
       " 'equal': 243,\n",
       " 'pancak': 484,\n",
       " 'life': 392,\n",
       " 'door': 218,\n",
       " 'offer': 463,\n",
       " 'cool': 150,\n",
       " 'els': 235,\n",
       " 'handl': 327,\n",
       " 'rowdi': 568,\n",
       " 'find': 278,\n",
       " 'despic': 193,\n",
       " 'soup': 645,\n",
       " 'lukewarm': 409,\n",
       " 'deserv': 191,\n",
       " 'stomach': 679,\n",
       " 'space': 648,\n",
       " 'comfort': 141,\n",
       " 'eggplant': 233,\n",
       " 'green': 315,\n",
       " 'bean': 57,\n",
       " 'part': 487,\n",
       " 'dinner': 202,\n",
       " 'told': 732,\n",
       " 'happen': 328,\n",
       " 'car': 108,\n",
       " 'front': 296,\n",
       " 'starv': 667,\n",
       " 'disgrac': 211,\n",
       " 'def': 176,\n",
       " 'anyon': 14,\n",
       " 'stuf': 689,\n",
       " 'shop': 599,\n",
       " 'mall': 414,\n",
       " 'bring': 89,\n",
       " 'kid': 376,\n",
       " 'option': 470,\n",
       " 'perfect': 496,\n",
       " 'famili': 263,\n",
       " 'impecc': 361,\n",
       " 'simpli': 609,\n",
       " 'remind': 549,\n",
       " 'pop': 513,\n",
       " 'assur': 25,\n",
       " 'sore': 641,\n",
       " 'complet': 145,\n",
       " 'becom': 60,\n",
       " 'regular': 547,\n",
       " 'profession': 523,\n",
       " 'smear': 621,\n",
       " 'mistak': 436,\n",
       " 'nicest': 457,\n",
       " 'biscuit': 71,\n",
       " 'awkward': 41,\n",
       " 'cow': 156,\n",
       " 'steiner': 672,\n",
       " 'anyway': 17,\n",
       " 'week': 774,\n",
       " 'combin': 138,\n",
       " 'big': 69,\n",
       " 'spicier': 655,\n",
       " 'anytim': 16,\n",
       " 'contain': 147,\n",
       " 'driest': 225,\n",
       " 'relax': 548,\n",
       " 'group': 319,\n",
       " 'tot': 735,\n",
       " 'southwest': 647,\n",
       " 'paid': 483,\n",
       " 'smooth': 624,\n",
       " 'choux': 125,\n",
       " 'new': 454,\n",
       " 'acknowledg': 1,\n",
       " 'margarita': 417,\n",
       " 'rather': 535,\n",
       " 'camelback': 105,\n",
       " 'flower': 287,\n",
       " 'claim': 130,\n",
       " 'bill': 70,\n",
       " 'crab': 157,\n",
       " 'leg': 389,\n",
       " 'slice': 616,\n",
       " 'dont': 217,\n",
       " 'long': 401,\n",
       " 'attach': 29,\n",
       " 'awesom': 40,\n",
       " 'wors': 788,\n",
       " 'worker': 786,\n",
       " 'bunch': 96,\n",
       " 'call': 103,\n",
       " 'fill': 276,\n",
       " 'homemad': 344,\n",
       " 'thin': 720,\n",
       " 'gone': 309,\n",
       " 'similarli': 607,\n",
       " 'deliveri': 187,\n",
       " 'apolog': 18,\n",
       " 'pack': 482,\n",
       " 'whole': 778,\n",
       " 'choos': 124,\n",
       " 'entre': 242,\n",
       " 'stranger': 684,\n",
       " 'strang': 683,\n",
       " 'boyfriend': 84,\n",
       " 'recent': 543,\n",
       " 'room': 565,\n",
       " 'mayb': 421,\n",
       " 'howev': 352,\n",
       " 'tapa': 708,\n",
       " 'vinegrett': 761,\n",
       " 'babi': 42,\n",
       " 'believ': 64,\n",
       " 'guest': 321,\n",
       " 'super': 698,\n",
       " 'fan': 265,\n",
       " 'dri': 224,\n",
       " 'cheap': 114,\n",
       " 'present': 519,\n",
       " 'color': 137,\n",
       " 'sit': 612,\n",
       " 'fairli': 262,\n",
       " 'soooooo': 640,\n",
       " 'paper': 485,\n",
       " 'ok': 465,\n",
       " 'sorri': 642,\n",
       " 'despit': 194,\n",
       " 'mean': 424,\n",
       " 'half': 325,\n",
       " 'somehow': 630,\n",
       " 'non': 459,\n",
       " 'focus': 288,\n",
       " 'vibe': 760,\n",
       " 'somewhat': 634,\n",
       " 'edibl': 231,\n",
       " 'promis': 524,\n",
       " 'fail': 261,\n",
       " 'deliv': 186,\n",
       " 'averag': 35,\n",
       " 'italian': 370,\n",
       " 'legit': 390,\n",
       " 'somethat': 633,\n",
       " 'song': 636,\n",
       " 'ring': 558,\n",
       " 'smashburg': 620,\n",
       " 'spend': 652,\n",
       " 'slaw': 615,\n",
       " 'drench': 222,\n",
       " 'soundtrack': 644,\n",
       " 'plate': 506,\n",
       " 'auju': 33,\n",
       " 'smoothi': 625,\n",
       " 'needless': 451,\n",
       " 'cibo': 128,\n",
       " 'longer': 402,\n",
       " 'read': 537,\n",
       " 'simpl': 608,\n",
       " 'classic': 131,\n",
       " 'fli': 286,\n",
       " 'bare': 50,\n",
       " 'chines': 121,\n",
       " 'speedi': 651,\n",
       " 'anyth': 15,\n",
       " 'complain': 143,\n",
       " 'stood': 680,\n",
       " 'smaller': 619,\n",
       " 'spice': 653,\n",
       " 'crowd': 162,\n",
       " 'mid': 431,\n",
       " 'defin': 177,\n",
       " 'low': 408,\n",
       " 'sour': 646,\n",
       " 'style': 691,\n",
       " 'bother': 80,\n",
       " 'flavorless': 285,\n",
       " 'describ': 189,\n",
       " 'nacho': 448,\n",
       " 'crazi': 158,\n",
       " 'tribut': 742,\n",
       " 'salsa': 576,\n",
       " 'surpris': 700,\n",
       " 'fell': 273,\n",
       " 'devin': 197,\n",
       " 'employe': 237,\n",
       " 'put': 528,\n",
       " 'delic': 181,\n",
       " 'aw': 38,\n",
       " 'articl': 23,\n",
       " 'head': 332,\n",
       " 'round': 567,\n",
       " 'disbelief': 210,\n",
       " 'heat': 336,\n",
       " 'start': 666,\n",
       " 'store': 682,\n",
       " 'three': 726,\n",
       " 'rotat': 566,\n",
       " 'strawberri': 685,\n",
       " 'correct': 151,\n",
       " 'steakhous': 671,\n",
       " 'batch': 53,\n",
       " 'christma': 127,\n",
       " 'degre': 179,\n",
       " 'lost': 404,\n",
       " 'denni': 188,\n",
       " 'ridicul': 556,\n",
       " 'stretch': 687,\n",
       " 'undercook': 748,\n",
       " 'chipolt': 123,\n",
       " 'dip': 203,\n",
       " 'spotti': 659,\n",
       " 'deuchebaggeri': 196,\n",
       " 'smoke': 623,\n",
       " 'solidifi': 629,\n",
       " 'del': 180,\n",
       " 'disapppoint': 208,\n",
       " 'circumst': 129}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:57:52.490041Z",
     "start_time": "2020-04-01T10:57:52.459059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:57:53.020172Z",
     "start_time": "2020-04-01T10:57:53.011185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 800)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:57:53.788839Z",
     "start_time": "2020-04-01T10:57:53.776843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:59:53.016906Z",
     "start_time": "2020-04-01T10:59:52.999889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:59:53.697938Z",
     "start_time": "2020-04-01T10:59:53.401888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Reegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_cv = LogisticRegression()\n",
    "lr_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:01.719549Z",
     "start_time": "2020-04-01T11:32:01.712549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 800)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_coeffs = lr_cv.coef_\n",
    "cv_coeffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:02.318549Z",
     "start_time": "2020-04-01T11:32:02.292551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.94470429e-02, -5.61094221e-02, -5.62442838e-02,  3.22852055e-01,\n",
       "       -2.61735079e-02, -2.51271324e-01,  4.54712392e-01, -2.92879665e-01,\n",
       "        4.33586657e-01,  1.76047785e+00,  5.32576886e-01,  4.95837017e-01,\n",
       "        1.93805910e-01, -3.81831961e-01, -6.74344696e-02,  5.49583345e-01,\n",
       "       -7.17251921e-01, -2.63598823e-01, -9.49874146e-02,  3.07743962e-01,\n",
       "        3.87902303e-02, -6.79677621e-02, -6.72201541e-01, -3.41166383e-02,\n",
       "       -2.04446018e-01,  5.17157985e-01, -2.71785282e-02,  4.51705987e-01,\n",
       "       -2.70901648e-02, -4.30586463e-01, -4.00190385e-01,  3.41270485e-01,\n",
       "       -3.06341504e-01,  0.00000000e+00,  3.31668124e-01, -7.54949995e-01,\n",
       "       -2.02449073e-01, -9.14649206e-01, -5.96800997e-01, -3.95487379e-01,\n",
       "        1.38342080e+00,  0.00000000e+00, -1.37700508e-02,  4.48970369e-01,\n",
       "       -2.86663537e-01,  9.76214414e-01, -1.41038345e+00,  2.53709855e-02,\n",
       "       -1.97457784e-01,  4.61244558e-01, -3.91924840e-01,  1.89494057e-01,\n",
       "       -3.14817562e-01,  0.00000000e+00, -1.66232208e-01, -2.29605291e-01,\n",
       "       -2.94567894e-01,  1.94925479e-01,  5.37800572e-02,  6.36705754e-01,\n",
       "       -2.90468089e-02, -4.31343603e-02,  5.51209938e-01, -1.72246594e-01,\n",
       "       -2.71494862e-01,  2.82509161e-01,  8.11387380e-01, -4.80727119e-01,\n",
       "        2.35158587e-01, -1.64179321e-01,  7.62259483e-02, -1.65051262e-01,\n",
       "       -1.08300664e-01, -7.89571790e-01,  8.66149847e-02,  3.20868116e-01,\n",
       "       -1.08450268e+00,  0.00000000e+00, -4.47808672e-01,  1.39747477e-02,\n",
       "       -3.18070175e-01,  2.10040942e-01,  4.77377553e-02,  2.06257968e-01,\n",
       "        2.61959599e-01,  5.90045094e-01, -2.89955066e-01,  7.15819252e-01,\n",
       "        2.41814723e-01,  4.45253406e-03, -2.11394995e-02,  1.15786544e-01,\n",
       "        1.50069572e-01,  7.60764306e-01, -1.18133642e-01, -1.06260991e-01,\n",
       "        1.98175735e-02, -5.22152876e-01, -9.16019259e-01,  2.47375731e-01,\n",
       "       -4.15280192e-01, -6.55950132e-03,  5.38354891e-02, -3.44414767e-01,\n",
       "       -1.48494634e-02, -3.15160854e-02, -2.04873074e-01,  2.02513371e-01,\n",
       "       -5.61094221e-02,  9.78738013e-02,  7.98377546e-02, -3.95774651e-01,\n",
       "       -9.65599510e-02, -2.16103893e-01, -2.04303776e-01,  8.39595877e-01,\n",
       "       -2.12563203e-03, -7.43750181e-02,  6.97094216e-01, -3.50834120e-01,\n",
       "        1.92282764e-01,  3.07727783e-01, -4.01289202e-01,  0.00000000e+00,\n",
       "        1.66894359e-01,  1.61983685e-01,  2.13524860e-01, -2.95430304e-01,\n",
       "       -2.57341308e-01, -7.66060049e-02,  3.51809816e-01, -8.36672914e-03,\n",
       "       -1.43577067e-01, -9.61559461e-02,  2.10560838e-01, -2.26168123e-01,\n",
       "       -5.16105112e-01, -6.83749222e-01,  1.53572338e-01, -1.65488523e-01,\n",
       "       -1.45433826e-01,  0.00000000e+00,  1.84785067e-01,  3.55686853e-01,\n",
       "        3.30629419e-01, -3.30196593e-01,  2.14879092e-01, -5.14073213e-01,\n",
       "        3.21692714e-01,  1.29433635e-01,  2.75408025e-01, -5.10371050e-02,\n",
       "        6.17272566e-01, -4.06633762e-02, -1.58050099e-01, -1.65626715e-01,\n",
       "        0.00000000e+00, -4.32980477e-01, -2.40015055e-01,  6.04498148e-02,\n",
       "        1.61983685e-01,  1.72602875e-01, -3.83570529e-01, -1.87656652e-01,\n",
       "        6.22159795e-02, -2.54307810e-01, -1.54913317e-01,  6.63055899e-01,\n",
       "        4.20747515e-02,  6.44080862e-01,  2.63330840e-01,  1.16138789e-01,\n",
       "       -3.67950307e-01, -2.21034454e-01,  2.37705098e-01,  4.84158453e-01,\n",
       "        0.00000000e+00,  7.86010384e-01,  5.54175575e-01,  0.00000000e+00,\n",
       "       -1.37994663e-01,  0.00000000e+00,  2.10735504e+00,  3.46880107e-01,\n",
       "        6.85751865e-01,  2.51517041e-01,  0.00000000e+00, -9.49874146e-02,\n",
       "       -4.33018553e-01,  0.00000000e+00,  0.00000000e+00, -3.42967190e-01,\n",
       "       -2.52308904e-01, -1.22959200e-01, -4.39446798e-02,  5.98135517e-01,\n",
       "       -9.25132366e-02,  4.78883379e-01,  7.37989293e-01,  1.99407450e-01,\n",
       "       -1.31302460e-01, -2.75215517e-01,  1.43918011e-02,  0.00000000e+00,\n",
       "       -4.26599532e-01, -8.76552352e-01, -2.23309709e-01, -8.87704346e-01,\n",
       "       -1.79371167e-01, -1.13201623e-01, -1.17455744e-01, -3.44433462e-01,\n",
       "       -6.11752242e-01,  2.01834141e-01, -2.32411058e-01, -2.69175679e-01,\n",
       "       -7.82131892e-01, -3.87228694e-01, -1.13667014e-01, -7.43750181e-02,\n",
       "       -2.86723485e-01, -2.19031391e-01,  1.41074090e-01, -2.57359212e-01,\n",
       "       -4.47871480e-01, -2.52964812e-01,  1.35464189e-02,  4.08415598e-01,\n",
       "        1.69692747e-01, -8.28524739e-01,  2.25926715e-01, -2.87238278e-01,\n",
       "       -2.29889087e-02,  3.42797617e-01, -7.46794312e-01, -3.49513249e-01,\n",
       "       -5.14952750e-01, -3.88030190e-01, -4.54356841e-01, -4.68092415e-02,\n",
       "        9.64409284e-01,  2.02114995e-01,  1.44114402e-01,  9.88428082e-02,\n",
       "       -2.83859306e-02, -7.75450826e-01,  2.11800168e-02,  5.99574007e-02,\n",
       "       -2.54023153e-01,  6.75227537e-01,  6.75166316e-02,  4.44938749e-01,\n",
       "        9.22238134e-01, -4.70302715e-01, -1.14312240e-01, -1.03756875e-01,\n",
       "        6.53728334e-02,  2.83076511e-01, -6.98313682e-01, -6.67360472e-02,\n",
       "        2.37443129e-01, -5.22818078e-01, -2.76415075e-01,  1.66440549e-01,\n",
       "        5.55811885e-02,  1.42088448e-01,  1.53923268e+00,  2.11689507e-01,\n",
       "       -3.65000099e-01,  2.56860639e-01, -5.63700605e-01,  7.41616480e-01,\n",
       "       -1.61190329e-01, -7.47604229e-05, -8.52409261e-02, -1.08300664e-01,\n",
       "        6.14392317e-02, -1.66489238e-01, -3.55006512e-01,  1.73853636e-01,\n",
       "        1.14818103e-02,  8.49170302e-01, -2.68269104e-02, -3.69988570e-01,\n",
       "        3.70075820e-02, -7.34756444e-01, -4.57745004e-01, -3.15160854e-02,\n",
       "       -2.16014425e-01,  4.85525977e-01, -2.04973694e-01, -2.53126963e-01,\n",
       "        3.50995105e-01, -2.84002565e-01,  9.24323976e-02,  1.57268611e+00,\n",
       "        0.00000000e+00, -2.05058021e-01,  5.50177434e-01,  7.41105478e-01,\n",
       "        1.13517169e-02,  7.72125408e-02,  2.45073138e-01, -5.20653504e-02,\n",
       "       -2.66329282e-01, -1.94989063e-01,  4.23784422e-01, -8.53556561e-02,\n",
       "        1.34588580e-01,  4.08275916e-01,  1.38945123e+00, -7.47339689e-01,\n",
       "       -2.78585863e-01,  2.81200774e+00,  3.25037776e-01, -2.22876954e-01,\n",
       "        0.00000000e+00,  4.97332682e-02, -8.68918858e-01,  3.51809816e-01,\n",
       "       -3.22378171e-01,  1.66682058e-01, -6.41874681e-02, -1.63594356e-01,\n",
       "       -1.54280397e-01, -6.40919700e-02,  7.06067973e-01,  4.57365831e-01,\n",
       "        0.00000000e+00,  1.36536241e+00, -9.01078632e-01, -5.36256763e-01,\n",
       "       -5.58272268e-01,  4.44366963e-01,  2.38085356e-02, -1.23547853e-01,\n",
       "        1.84013547e-01, -1.63091094e-01,  5.37354902e-02,  3.07942820e-01,\n",
       "        2.14398826e-01, -2.38504871e-01,  1.00082108e-01, -1.66388730e-02,\n",
       "        3.61256011e-01, -1.12424903e-01,  3.14561022e-01,  2.04289056e-02,\n",
       "       -5.08179645e-01,  2.07274897e-02, -5.64219890e-01,  5.74942689e-03,\n",
       "       -4.77627222e-01,  4.59960168e-02, -1.16703291e-01,  3.00882867e-01,\n",
       "        2.64407627e-01,  8.43120840e-01, -2.69947973e-01, -6.80183376e-01,\n",
       "        2.33943282e-01,  2.37502810e-01, -1.31190685e+00,  4.70017654e-01,\n",
       "        4.41173734e-01, -7.39023981e-02,  5.29137897e-01,  8.71226862e-01,\n",
       "       -6.11728680e-01,  4.22662975e-01,  2.75065255e-01, -7.67888803e-02,\n",
       "        3.03652622e-01,  0.00000000e+00, -2.27232870e-01, -4.23995149e-01,\n",
       "       -3.52129474e-01,  6.08689930e-02, -4.76782338e-01, -1.27388034e-01,\n",
       "       -6.51199665e-01, -2.30381358e-02, -1.90710624e-01, -1.18207092e-01,\n",
       "       -3.61412982e-02,  2.34473475e-01,  6.37332579e-02, -3.78669739e-01,\n",
       "        4.42493925e-02, -4.32980477e-01,  3.64984551e-01, -9.01583019e-01,\n",
       "        3.57012442e-01,  3.74167818e-01, -1.81969796e-01,  3.33856825e-02,\n",
       "       -2.88026554e-01,  2.20239874e-02, -6.97143983e-01,  1.34115581e-01,\n",
       "       -7.63212237e-01, -6.35793238e-01, -3.28590533e-01, -1.05056337e+00,\n",
       "       -6.25967516e-01, -1.38057710e-01,  1.64966439e+00, -5.79713680e-03,\n",
       "        1.10569026e-01, -1.28107928e-01, -1.63837802e-01, -2.62007971e-01,\n",
       "       -3.90822325e-01,  1.61602984e-01, -1.00603786e-01, -6.23824615e-01,\n",
       "        3.25162796e-03,  3.86230537e-02, -3.28071479e-01,  1.62544273e-01,\n",
       "        2.09329023e-01, -3.65000099e-01,  1.41074090e-01, -1.27245251e-01,\n",
       "       -2.06535670e-01, -6.04679272e-01, -8.58298183e-01, -4.73253086e-01,\n",
       "        2.48049820e-01,  5.62967179e-01,  1.98175735e-02, -4.46183915e-01,\n",
       "       -4.47182834e-01, -1.31027925e-01, -1.29348661e+00,  5.09223211e-01,\n",
       "       -3.44433462e-01,  4.23898807e-01,  3.50341201e-01, -8.86138198e-02,\n",
       "       -9.53540498e-01,  2.96259846e-01,  5.13144646e-01, -1.14377625e+00,\n",
       "        6.75186153e-02, -8.53900804e-02,  3.37209777e-01, -4.22887053e-01,\n",
       "       -5.88377469e-02, -6.58250134e-01, -3.61770403e-01, -1.74780597e-01,\n",
       "        1.26360664e-01, -8.05104631e-01,  2.14264530e-01,  2.64029112e-02,\n",
       "        1.33525733e+00,  6.73526403e-01,  1.44019101e-01,  5.54878070e-02,\n",
       "       -2.09264047e-01, -2.08181642e-01, -1.11624438e+00, -1.29858010e-01,\n",
       "       -1.38501675e-01, -5.23314202e-01, -7.19054551e-01,  7.89969287e-01,\n",
       "       -5.00838240e-01,  2.67526497e-03,  3.98626418e-01,  6.23736350e-01,\n",
       "       -3.29738472e-01, -5.73544906e-01,  8.10114110e-01,  2.41814723e-01,\n",
       "        1.72356584e-01, -2.06128473e-01, -9.03314115e-01,  9.25841552e-02,\n",
       "        4.46244429e-01,  2.15966923e-01, -6.41805058e-02, -5.07811669e-01,\n",
       "       -2.03616213e-01, -9.11495473e-02, -5.93219214e-01, -1.15288789e-01,\n",
       "        4.55967581e-01, -1.61306539e-01,  2.34972102e-01,  5.60763225e-01,\n",
       "       -3.56275062e-01, -2.17898704e-01, -4.61747851e-01, -1.08300664e-01,\n",
       "        1.37391088e+00,  4.89682965e-01,  4.69829768e-02, -1.23296011e-01,\n",
       "        5.97121823e-01,  1.88804888e-01, -6.53537191e-02,  1.80771489e-01,\n",
       "        2.64960222e-02,  4.35739766e-01, -7.09328227e-02, -3.11685149e-03,\n",
       "        3.52090148e-03, -1.01456295e-01,  5.92655336e-01,  3.32875803e-01,\n",
       "       -1.12003842e+00, -1.12738883e-01,  1.59438087e-01,  1.07437849e-02,\n",
       "       -1.46598342e-01,  7.33496765e-02, -7.24957206e-03,  1.62677145e-02,\n",
       "        6.41451290e-02,  2.05912156e-01, -1.06834130e+00,  1.79618102e-01,\n",
       "        6.34681116e-01,  2.46793977e-01, -1.80707578e-01,  3.66531672e-01,\n",
       "        1.13154229e-01,  2.93638157e-01,  2.98368408e-01, -2.80918317e-01,\n",
       "        2.81669361e-01, -3.19148161e-01, -6.51442386e-01, -1.17323078e-01,\n",
       "       -1.65849180e-01, -3.41166383e-02, -3.84080240e-01,  3.29481109e-01,\n",
       "        3.58907409e-01,  6.09143795e-01,  8.57981543e-02, -1.19163802e-01,\n",
       "        6.63001206e-01, -8.46423056e-02, -7.05393117e-02,  3.85920495e-01,\n",
       "        1.23879600e-02,  1.00338228e-01,  1.28959825e-01, -5.18728878e-01,\n",
       "       -1.52341404e-01, -4.66261544e-01,  3.78041146e-01,  5.88461164e-02,\n",
       "       -3.56431808e-01,  2.24090377e-01,  3.77671777e-02, -1.31027925e-01,\n",
       "       -3.19575993e-01, -1.08300664e-01, -1.64479488e-01, -2.39869364e-02,\n",
       "        1.64374591e-01,  3.21366460e-01,  1.58019064e-01,  1.47250516e-01,\n",
       "        1.05556014e-01, -2.78264921e-01, -1.17300565e+00,  8.31925108e-02,\n",
       "       -7.18412079e-01, -2.30366245e-01,  2.93905102e-01,  1.68218085e-01,\n",
       "       -1.56998457e-01, -1.65332834e-01,  3.27116768e-01, -3.72628030e-01,\n",
       "       -1.27740126e-01,  6.62049141e-01,  4.11353753e-01, -1.28634882e-01,\n",
       "       -2.11799778e-01, -9.81394040e-03,  4.33085756e-01,  1.88621382e-01,\n",
       "        4.89740488e-01, -2.72097015e-01, -2.99590996e-01, -5.03418890e-01,\n",
       "        7.28347460e-01,  6.36314434e-01, -1.46274145e-01, -1.63432930e-01,\n",
       "       -1.79617422e-02, -3.52028939e-01, -9.51643069e-02,  6.28702654e-02,\n",
       "       -5.90668208e-02, -2.62055850e-01, -1.33579767e-01, -8.34253330e-01,\n",
       "        3.95274249e-01, -6.99799624e-01,  1.96359961e-01, -9.49874146e-02,\n",
       "        3.53932378e-01, -3.87928948e-01,  3.57074252e-01, -2.30932783e-01,\n",
       "        9.47641477e-02, -2.70200080e-01,  4.12273957e-01,  1.41074090e-01,\n",
       "        1.06388197e-01, -1.14632668e+00, -4.14539241e-01, -1.99588304e-01,\n",
       "       -1.12738883e-01, -1.49819313e-01, -2.47018373e-01, -9.25132366e-02,\n",
       "        1.61983685e-01,  7.77301412e-02, -7.45102698e-01,  3.13895397e-01,\n",
       "        2.18949806e-01, -9.25132366e-02, -6.54788568e-02, -3.43696910e-01,\n",
       "       -1.04778609e-01,  2.64646323e-01,  0.00000000e+00,  6.39480204e-02,\n",
       "        9.61443956e-03,  1.39805129e-01,  2.00360240e-01,  2.13524860e-01,\n",
       "       -2.04303776e-01, -4.38439626e-01, -2.07377253e-01, -1.22996040e-02,\n",
       "        1.31512785e-01,  1.27024821e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -2.35903098e-01,  4.76081886e-01,  1.34123449e-01,\n",
       "       -4.79902758e-01,  9.29081823e-02,  6.33135063e-01,  2.56793174e-01,\n",
       "       -2.02449073e-01,  5.99574007e-02,  6.97608215e-01, -3.41118114e-01,\n",
       "        1.08789876e-01, -1.08300664e-01,  2.77443634e-01, -7.62198706e-01,\n",
       "       -5.52926048e-02,  3.73007673e-01, -3.42440868e-01,  0.00000000e+00,\n",
       "        7.99135524e-02,  3.17731083e-02,  2.15021732e-01, -3.44414767e-01,\n",
       "        3.47821016e-01, -6.16539778e-01,  5.88461164e-02,  5.62448571e-01,\n",
       "       -1.97230976e-01, -3.41118114e-01,  5.91060200e-02, -4.99401096e-01,\n",
       "       -1.45195797e-01,  4.67905810e-01,  0.00000000e+00, -3.09858970e-01,\n",
       "       -1.54280397e-01,  2.45123516e-01,  0.00000000e+00, -4.82019593e-01,\n",
       "        3.92061012e-01,  3.51156189e-01,  3.23183175e-01, -5.05924672e-01,\n",
       "       -2.16103893e-01, -1.86139605e-01, -8.44902694e-01, -2.74148539e-01,\n",
       "       -2.20127451e-02,  3.43736124e-01, -1.24929195e-02, -5.33677920e-01,\n",
       "        1.33167076e-01, -4.41878280e-01,  7.21440153e-01, -5.07090506e-01,\n",
       "        2.60605471e-01, -2.79605512e-01, -5.04886615e-01,  1.58019064e-01,\n",
       "        1.17972908e-01,  4.35551344e-01, -1.68037639e-02, -7.21487271e-01,\n",
       "        3.75431649e-01, -2.10883375e-01, -5.74523895e-02, -9.59499265e-02,\n",
       "        7.55704435e-01, -1.17969165e+00, -2.56798738e-01,  5.87433728e-01,\n",
       "        1.58822369e-01, -4.31442344e-01, -8.19563780e-01, -2.56908993e-01,\n",
       "        1.85567773e-01,  2.89648940e-01, -1.41865645e-01,  1.43038936e-01,\n",
       "       -1.40808366e-01, -5.77166426e-02,  1.13975922e-01,  3.84451854e-01,\n",
       "        1.99277492e-01, -3.89953439e-01,  5.44340483e-02,  3.77671777e-02,\n",
       "       -5.17043388e-01,  4.57061266e-01, -2.55530635e-01,  7.36206961e-01,\n",
       "        4.48568025e-01,  4.08534310e-01,  0.00000000e+00, -2.47864806e-01,\n",
       "       -1.33941196e-01,  2.72658477e-01, -4.91549655e-01,  4.59903046e-01,\n",
       "       -1.99071593e-01, -2.94081527e-01, -9.82764246e-01, -3.37123687e-01,\n",
       "        1.48866838e-01, -6.01014970e-01, -7.43137987e-01, -4.35614237e-01,\n",
       "        4.91613222e-01, -2.81158582e-01, -1.43324622e-01,  3.09787718e-01,\n",
       "        0.00000000e+00,  1.84548212e-01,  3.75061496e-01, -5.55341530e-01,\n",
       "       -3.69872304e-01,  1.40854692e-01, -2.81399669e-01,  8.90579905e-04,\n",
       "       -1.06391270e-01,  4.28801918e-01, -7.77514197e-01, -1.24465185e-01,\n",
       "       -7.05393117e-02, -8.94634886e-01,  3.58527377e-01,  2.13448175e-01,\n",
       "        4.08336448e-01,  6.87681214e-01, -7.68221240e-02, -2.31674918e-01,\n",
       "       -4.20735193e-01,  2.31723742e-01, -1.54325229e-01,  8.20606867e-01,\n",
       "       -7.40559755e-03, -9.45127504e-02, -4.65217868e-01,  1.20111378e-01,\n",
       "       -2.33366838e-01, -1.32629051e+00, -8.55777760e-02, -1.03629647e+00,\n",
       "        5.48481263e-01, -7.79463994e-02, -3.41823506e-03, -1.39375131e-01,\n",
       "       -9.22923540e-03,  0.00000000e+00,  3.32590026e-01, -7.58889882e-01])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_coeffs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:01:04.843955Z",
     "start_time": "2020-04-01T11:01:04.510956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = lr_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:01:07.632549Z",
     "start_time": "2020-04-01T11:01:07.399553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72        97\n",
      "           1       0.76      0.64      0.69       103\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.72      0.71      0.71       200\n",
      "weighted avg       0.72      0.71      0.71       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "roc = roc_auc_score(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:01:51.606714Z",
     "start_time": "2020-04-01T11:01:51.598719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76, 21],\n",
       "       [37, 66]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:01:52.105720Z",
     "start_time": "2020-04-01T11:01:52.098728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:01:53.145507Z",
     "start_time": "2020-04-01T11:01:53.096514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model Tf-Idf\n",
    "tfidf = TfidfVectorizer(max_features = 800)\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "vocab_tf = tfidf.vocabulary_\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:02:43.315788Z",
     "start_time": "2020-04-01T11:02:43.268788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.51611335, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.37891311, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.76814834, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:03:52.297744Z",
     "start_time": "2020-04-01T11:03:52.283746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:03:52.586746Z",
     "start_time": "2020-04-01T11:03:52.569747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_tf = LogisticRegression()\n",
    "lr_tf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:03:53.674794Z",
     "start_time": "2020-04-01T11:03:53.669810Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_coeffs = lr_tf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:03:54.292792Z",
     "start_time": "2020-04-01T11:03:54.284794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred2 = lr_tf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:03:55.013244Z",
     "start_time": "2020-04-01T11:03:54.991248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.86      0.76        97\n",
      "           1       0.82      0.64      0.72       103\n",
      "\n",
      "    accuracy                           0.74       200\n",
      "   macro avg       0.76      0.75      0.74       200\n",
      "weighted avg       0.76      0.74      0.74       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve\n",
    "cm2 = confusion_matrix(y_test, y_pred2)\n",
    "ac2 = accuracy_score(y_test, y_pred2)\n",
    "roc2 = roc_auc_score(y_test, y_pred2)\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:03:55.638701Z",
     "start_time": "2020-04-01T11:03:55.630712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[83, 14],\n",
       "       [37, 66]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:03:57.146853Z",
     "start_time": "2020-04-01T11:03:57.138847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.745"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:05:19.424932Z",
     "start_time": "2020-04-01T11:05:19.416933Z"
    }
   },
   "outputs": [],
   "source": [
    "common_vocab = list(set(list(vocab_cv.keys())).intersection(set(list(vocab_tf.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:05:19.981949Z",
     "start_time": "2020-04-01T11:05:19.973942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:22:25.616320Z",
     "start_time": "2020-04-01T11:22:25.607320Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feat_dict(feat_list):\n",
    "    feat_dict={}\n",
    "    idx=0\n",
    "    for item in feat_list:\n",
    "        feat_dict[item] = idx\n",
    "        idx = idx+1\n",
    "    return feat_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:22:25.931322Z",
     "start_time": "2020-04-01T11:22:25.924319Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_feat_dict = get_feat_dict(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:22:26.201320Z",
     "start_time": "2020-04-01T11:22:26.194316Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_feat_dict = get_feat_dict(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:22:26.513322Z",
     "start_time": "2020-04-01T11:22:26.477318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wow': 792,\n",
       " 'love': 406,\n",
       " 'place': 505,\n",
       " 'crust': 163,\n",
       " 'good': 310,\n",
       " 'tasti': 712,\n",
       " 'textur': 718,\n",
       " 'nasti': 449,\n",
       " 'stop': 681,\n",
       " 'late': 384,\n",
       " 'may': 420,\n",
       " 'rick': 555,\n",
       " 'steve': 674,\n",
       " 'recommend': 544,\n",
       " 'select': 592,\n",
       " 'menu': 429,\n",
       " 'great': 313,\n",
       " 'price': 521,\n",
       " 'get': 303,\n",
       " 'want': 768,\n",
       " 'damn': 168,\n",
       " 'pho': 499,\n",
       " 'tast': 710,\n",
       " 'fresh': 292,\n",
       " 'potato': 517,\n",
       " 'like': 394,\n",
       " 'rubber': 569,\n",
       " 'could': 152,\n",
       " 'tell': 714,\n",
       " 'made': 411,\n",
       " 'time': 728,\n",
       " 'kept': 375,\n",
       " 'fri': 293,\n",
       " 'touch': 737,\n",
       " 'servic': 596,\n",
       " 'would': 791,\n",
       " 'go': 307,\n",
       " 'back': 44,\n",
       " 'cashier': 110,\n",
       " 'care': 109,\n",
       " 'ever': 248,\n",
       " 'say': 583,\n",
       " 'still': 676,\n",
       " 'end': 239,\n",
       " 'overpr': 478,\n",
       " 'tri': 741,\n",
       " 'chicken': 120,\n",
       " 'mmmm': 437,\n",
       " 'disgust': 212,\n",
       " 'pretti': 520,\n",
       " 'sure': 699,\n",
       " 'human': 354,\n",
       " 'hair': 324,\n",
       " 'sign': 605,\n",
       " 'highli': 339,\n",
       " 'waitress': 765,\n",
       " 'littl': 397,\n",
       " 'slow': 617,\n",
       " 'worth': 790,\n",
       " 'let': 391,\n",
       " 'vega': 756,\n",
       " 'food': 290,\n",
       " 'amaz': 9,\n",
       " 'also': 6,\n",
       " 'cute': 167,\n",
       " 'beauti': 59,\n",
       " 'right': 557,\n",
       " 'red': 545,\n",
       " 'cake': 102,\n",
       " 'stuff': 690,\n",
       " 'never': 453,\n",
       " 'brought': 90,\n",
       " 'salad': 574,\n",
       " 'ask': 24,\n",
       " 'wall': 767,\n",
       " 'mexican': 430,\n",
       " 'street': 686,\n",
       " 'taco': 704,\n",
       " 'friendli': 295,\n",
       " 'staff': 662,\n",
       " 'took': 733,\n",
       " 'hour': 350,\n",
       " 'tabl': 703,\n",
       " 'restaur': 550,\n",
       " 'warm': 769,\n",
       " 'sever': 598,\n",
       " 'run': 571,\n",
       " 'around': 21,\n",
       " 'total': 736,\n",
       " 'overwhelm': 479,\n",
       " 'worst': 789,\n",
       " 'salmon': 575,\n",
       " 'sashimi': 579,\n",
       " 'combo': 139,\n",
       " 'burger': 97,\n",
       " 'beer': 62,\n",
       " 'decent': 173,\n",
       " 'deal': 172,\n",
       " 'final': 277,\n",
       " 'blow': 77,\n",
       " 'found': 291,\n",
       " 'seem': 590,\n",
       " 'quick': 530,\n",
       " 'bite': 74,\n",
       " 'familiar': 264,\n",
       " 'favor': 270,\n",
       " 'look': 403,\n",
       " 'elsewher': 236,\n",
       " 'overal': 476,\n",
       " 'lot': 405,\n",
       " 'qualiti': 529,\n",
       " 'inexpens': 366,\n",
       " 'portion': 515,\n",
       " 'poor': 512,\n",
       " 'waiter': 764,\n",
       " 'feel': 272,\n",
       " 'everi': 249,\n",
       " 'came': 104,\n",
       " 'first': 281,\n",
       " 'visit': 762,\n",
       " 'delight': 184,\n",
       " 'suck': 694,\n",
       " 'shrimp': 602,\n",
       " 'tender': 716,\n",
       " 'moist': 438,\n",
       " 'enough': 241,\n",
       " 'establish': 245,\n",
       " 'hard': 330,\n",
       " 'judg': 374,\n",
       " 'side': 604,\n",
       " 'gross': 318,\n",
       " 'melt': 428,\n",
       " 'eat': 229,\n",
       " 'sick': 603,\n",
       " 'note': 461,\n",
       " 'server': 595,\n",
       " 'attent': 31,\n",
       " 'provid': 525,\n",
       " 'frozen': 297,\n",
       " 'peopl': 494,\n",
       " 'behind': 63,\n",
       " 'thing': 721,\n",
       " 'dessert': 195,\n",
       " 'bad': 46,\n",
       " 'gener': 302,\n",
       " 'beef': 61,\n",
       " 'cook': 149,\n",
       " 'sandwich': 578,\n",
       " 'greek': 314,\n",
       " 'dress': 223,\n",
       " 'pita': 503,\n",
       " 'hummu': 355,\n",
       " 'order': 471,\n",
       " 'duck': 228,\n",
       " 'rare': 533,\n",
       " 'insid': 367,\n",
       " 'nice': 456,\n",
       " 'char': 111,\n",
       " 'outsid': 473,\n",
       " 'us': 752,\n",
       " 'realiz': 539,\n",
       " 'husband': 356,\n",
       " 'left': 388,\n",
       " 'chow': 126,\n",
       " 'horribl': 348,\n",
       " 'attitud': 32,\n",
       " 'toward': 738,\n",
       " 'custom': 165,\n",
       " 'talk': 706,\n",
       " 'one': 468,\n",
       " 'enjoy': 240,\n",
       " 'huge': 353,\n",
       " 'wonder': 783,\n",
       " 'imagin': 359,\n",
       " 'heart': 335,\n",
       " 'attack': 30,\n",
       " 'grill': 317,\n",
       " 'downtown': 221,\n",
       " 'absolut': 0,\n",
       " 'flat': 283,\n",
       " 'excus': 253,\n",
       " 'much': 443,\n",
       " 'seafood': 585,\n",
       " 'pasta': 490,\n",
       " 'amount': 12,\n",
       " 'sauc': 582,\n",
       " 'scallop': 584,\n",
       " 'perfectli': 497,\n",
       " 'rip': 560,\n",
       " 'tasteless': 711,\n",
       " 'least': 386,\n",
       " 'think': 722,\n",
       " 'refil': 546,\n",
       " 'water': 772,\n",
       " 'minut': 434,\n",
       " 'receiv': 542,\n",
       " 'star': 665,\n",
       " 'appet': 19,\n",
       " 'cocktail': 134,\n",
       " 'delici': 182,\n",
       " 'definit': 178,\n",
       " 'glad': 306,\n",
       " 'give': 304,\n",
       " 'alway': 8,\n",
       " 'went': 776,\n",
       " 'second': 588,\n",
       " 'got': 311,\n",
       " 'heard': 334,\n",
       " 'salt': 577,\n",
       " 'batter': 55,\n",
       " 'fish': 282,\n",
       " 'chewi': 119,\n",
       " 'way': 773,\n",
       " 'finish': 280,\n",
       " 'includ': 363,\n",
       " 'drink': 226,\n",
       " 'beyond': 68,\n",
       " 'expect': 254,\n",
       " 'realli': 540,\n",
       " 'rice': 553,\n",
       " 'meh': 427,\n",
       " 'min': 432,\n",
       " 'noth': 462,\n",
       " 'guess': 320,\n",
       " 'known': 379,\n",
       " 'use': 753,\n",
       " 'dish': 213,\n",
       " 'quit': 532,\n",
       " 'valu': 755,\n",
       " 'well': 775,\n",
       " 'sweet': 702,\n",
       " 'season': 586,\n",
       " 'today': 731,\n",
       " 'lunch': 410,\n",
       " 'buffet': 93,\n",
       " 'wast': 770,\n",
       " 'opportun': 469,\n",
       " 'compani': 142,\n",
       " 'come': 140,\n",
       " 'experienc': 256,\n",
       " 'underwhelm': 749,\n",
       " 'parti': 488,\n",
       " 'wait': 763,\n",
       " 'person': 498,\n",
       " 'break': 86,\n",
       " 'walk': 766,\n",
       " 'smell': 622,\n",
       " 'old': 466,\n",
       " 'greas': 312,\n",
       " 'other': 472,\n",
       " 'roast': 562,\n",
       " 'bland': 76,\n",
       " 'everyon': 250,\n",
       " 'rave': 536,\n",
       " 'sugari': 695,\n",
       " 'disast': 209,\n",
       " 'six': 613,\n",
       " 'year': 795,\n",
       " 'spring': 660,\n",
       " 'roll': 564,\n",
       " 'oh': 464,\n",
       " 'yummi': 798,\n",
       " 'meat': 425,\n",
       " 'die': 198,\n",
       " 'everyth': 251,\n",
       " 'larg': 382,\n",
       " 'disappoint': 207,\n",
       " 'dine': 201,\n",
       " 'experi': 255,\n",
       " 'mouth': 442,\n",
       " 'rock': 563,\n",
       " 'step': 673,\n",
       " 'best': 66,\n",
       " 'breakfast': 87,\n",
       " 'bye': 100,\n",
       " 'tip': 729,\n",
       " 'ladi': 381,\n",
       " 'arriv': 22,\n",
       " 'quickli': 531,\n",
       " 'cafe': 101,\n",
       " 'serv': 594,\n",
       " 'fantast': 266,\n",
       " 'wife': 779,\n",
       " 'garlic': 300,\n",
       " 'marrow': 419,\n",
       " 'ad': 3,\n",
       " 'extra': 257,\n",
       " 'meal': 423,\n",
       " 'anoth': 13,\n",
       " 'help': 337,\n",
       " 'mari': 418,\n",
       " 'town': 739,\n",
       " 'cannot': 106,\n",
       " 'beat': 58,\n",
       " 'wine': 780,\n",
       " 'better': 67,\n",
       " 'bartend': 51,\n",
       " 'ambienc': 11,\n",
       " 'music': 446,\n",
       " 'play': 507,\n",
       " 'next': 455,\n",
       " 'trip': 743,\n",
       " 'sooooo': 639,\n",
       " 'real': 538,\n",
       " 'sushi': 701,\n",
       " 'lover': 407,\n",
       " 'honest': 345,\n",
       " 'pass': 489,\n",
       " 'busi': 98,\n",
       " 'thai': 719,\n",
       " 'spici': 654,\n",
       " 'check': 115,\n",
       " 'atmospher': 27,\n",
       " 'kind': 377,\n",
       " 'steak': 670,\n",
       " 'although': 7,\n",
       " 'sound': 643,\n",
       " 'actual': 2,\n",
       " 'bit': 73,\n",
       " 'know': 378,\n",
       " 'manag': 415,\n",
       " 'eaten': 230,\n",
       " 'prepar': 518,\n",
       " 'indian': 365,\n",
       " 'boot': 79,\n",
       " 'fine': 279,\n",
       " 'guy': 322,\n",
       " 'son': 635,\n",
       " 'said': 573,\n",
       " 'thought': 725,\n",
       " 'ventur': 759,\n",
       " 'away': 39,\n",
       " 'hit': 342,\n",
       " 'spot': 658,\n",
       " 'night': 458,\n",
       " 'lack': 380,\n",
       " 'word': 784,\n",
       " 'reason': 541,\n",
       " 'review': 552,\n",
       " 'leav': 387,\n",
       " 'ambianc': 10,\n",
       " 'return': 551,\n",
       " 'strip': 688,\n",
       " 'pork': 514,\n",
       " 'belli': 65,\n",
       " 'mediocr': 426,\n",
       " 'excel': 252,\n",
       " 'crispi': 161,\n",
       " 'wrap': 793,\n",
       " 'delish': 185,\n",
       " 'tuna': 744,\n",
       " 'rude': 570,\n",
       " 'bagel': 47,\n",
       " 'cream': 159,\n",
       " 'chees': 116,\n",
       " 'even': 246,\n",
       " 'subway': 693,\n",
       " 'fact': 260,\n",
       " 'serious': 593,\n",
       " 'solid': 628,\n",
       " 'bar': 49,\n",
       " 'extrem': 258,\n",
       " 'mani': 416,\n",
       " 'empti': 238,\n",
       " 'suggest': 696,\n",
       " 'ate': 26,\n",
       " 'curri': 164,\n",
       " 'top': 734,\n",
       " 'done': 216,\n",
       " 'cover': 155,\n",
       " 'bathroom': 54,\n",
       " 'clean': 132,\n",
       " 'decor': 175,\n",
       " 'consid': 146,\n",
       " 'pace': 481,\n",
       " 'thumb': 727,\n",
       " 'watch': 771,\n",
       " 'pay': 492,\n",
       " 'ignor': 358,\n",
       " 'day': 171,\n",
       " 'greet': 316,\n",
       " 'seat': 587,\n",
       " 'bay': 56,\n",
       " 'stale': 663,\n",
       " 'highlight': 340,\n",
       " 'joint': 372,\n",
       " 'differ': 199,\n",
       " 'cut': 166,\n",
       " 'piec': 502,\n",
       " 'flavor': 284,\n",
       " 'sinc': 610,\n",
       " 'ago': 4,\n",
       " 'unfortun': 750,\n",
       " 'must': 447,\n",
       " 'bakeri': 48,\n",
       " 'impress': 362,\n",
       " 'immedi': 360,\n",
       " 'avoid': 37,\n",
       " 'full': 298,\n",
       " 'hand': 326,\n",
       " 'phoenix': 500,\n",
       " 'area': 20,\n",
       " 'treat': 740,\n",
       " 'bacon': 45,\n",
       " 'spinach': 656,\n",
       " 'avocado': 36,\n",
       " 'sad': 572,\n",
       " 'liter': 396,\n",
       " 'zero': 799,\n",
       " 'list': 395,\n",
       " 'soi': 627,\n",
       " 'miss': 435,\n",
       " 'vegetarian': 758,\n",
       " 'desir': 192,\n",
       " 'hip': 341,\n",
       " 'sat': 580,\n",
       " 'take': 705,\n",
       " 'overcook': 477,\n",
       " 'charcoal': 112,\n",
       " 'decid': 174,\n",
       " 'probabl': 522,\n",
       " 'dirt': 204,\n",
       " 'someth': 632,\n",
       " 'healthi': 333,\n",
       " 'ice': 357,\n",
       " 'incred': 364,\n",
       " 'interest': 369,\n",
       " 'station': 668,\n",
       " 'hot': 349,\n",
       " 'bread': 85,\n",
       " 'butter': 99,\n",
       " 'home': 343,\n",
       " 'chip': 122,\n",
       " 'egg': 232,\n",
       " 'gyro': 323,\n",
       " 'wing': 781,\n",
       " 'satisfi': 581,\n",
       " 'dog': 215,\n",
       " 'valley': 754,\n",
       " 'bowl': 81,\n",
       " 'live': 398,\n",
       " 'insult': 368,\n",
       " 'felt': 274,\n",
       " 'disrespect': 214,\n",
       " 'drive': 227,\n",
       " 'hope': 347,\n",
       " 'brunch': 91,\n",
       " 'last': 383,\n",
       " 'mushroom': 445,\n",
       " 'gold': 308,\n",
       " 'pure': 527,\n",
       " 'white': 777,\n",
       " 'bug': 94,\n",
       " 'show': 600,\n",
       " 'given': 305,\n",
       " 'soon': 637,\n",
       " 'friend': 294,\n",
       " 'tartar': 709,\n",
       " 'though': 724,\n",
       " 'soggi': 626,\n",
       " 'small': 618,\n",
       " 'rich': 554,\n",
       " 'shower': 601,\n",
       " 'rins': 559,\n",
       " 'unless': 751,\n",
       " 'mind': 433,\n",
       " 'see': 589,\n",
       " 'lobster': 399,\n",
       " 'bisqu': 72,\n",
       " 'sprout': 661,\n",
       " 'risotto': 561,\n",
       " 'filet': 275,\n",
       " 'need': 450,\n",
       " 'pepper': 495,\n",
       " 'cours': 154,\n",
       " 'none': 460,\n",
       " 'someon': 631,\n",
       " 'either': 234,\n",
       " 'cold': 136,\n",
       " 'date': 170,\n",
       " 'unbeliev': 747,\n",
       " 'folk': 289,\n",
       " 'make': 413,\n",
       " 'special': 650,\n",
       " 'main': 412,\n",
       " 'world': 787,\n",
       " 'fun': 299,\n",
       " 'chef': 118,\n",
       " 'doubl': 219,\n",
       " 'cheeseburg': 117,\n",
       " 'singl': 611,\n",
       " 'pictur': 501,\n",
       " 'coupl': 153,\n",
       " 'sport': 657,\n",
       " 'event': 247,\n",
       " 'possibl': 516,\n",
       " 'descript': 190,\n",
       " 'yum': 797,\n",
       " 'yet': 796,\n",
       " 'mayo': 422,\n",
       " 'honestli': 346,\n",
       " 'eye': 259,\n",
       " 'stay': 669,\n",
       " 'money': 440,\n",
       " 'almost': 5,\n",
       " 'build': 95,\n",
       " 'close': 133,\n",
       " 'point': 511,\n",
       " 'light': 393,\n",
       " 'dark': 169,\n",
       " 'set': 597,\n",
       " 'mood': 441,\n",
       " 'sub': 692,\n",
       " 'par': 486,\n",
       " 'owner': 480,\n",
       " 'work': 785,\n",
       " 'creami': 160,\n",
       " 'similar': 606,\n",
       " 'complaint': 144,\n",
       " 'pizza': 504,\n",
       " 'peanut': 493,\n",
       " 'fast': 269,\n",
       " 'stick': 675,\n",
       " 'charg': 113,\n",
       " 'tap': 707,\n",
       " 'plu': 510,\n",
       " 'buck': 92,\n",
       " 'far': 267,\n",
       " 'twice': 745,\n",
       " 'coffe': 135,\n",
       " 'cant': 107,\n",
       " 'wrong': 794,\n",
       " 'job': 371,\n",
       " 'boba': 78,\n",
       " 'tea': 713,\n",
       " 'patio': 491,\n",
       " 'outstand': 474,\n",
       " 'skimp': 614,\n",
       " 'bachi': 43,\n",
       " 'stink': 677,\n",
       " 'hate': 331,\n",
       " 'disagre': 206,\n",
       " 'later': 385,\n",
       " 'neighborhood': 452,\n",
       " 'conveni': 148,\n",
       " 'locat': 400,\n",
       " 'pull': 526,\n",
       " 'soooo': 638,\n",
       " 'gave': 301,\n",
       " 'rate': 534,\n",
       " 'pleas': 508,\n",
       " 'third': 723,\n",
       " 'stir': 678,\n",
       " 'box': 82,\n",
       " 'dime': 200,\n",
       " 'atroci': 28,\n",
       " 'summer': 697,\n",
       " 'toast': 730,\n",
       " 'high': 338,\n",
       " 'hous': 351,\n",
       " 'boy': 83,\n",
       " 'basic': 52,\n",
       " 'joke': 373,\n",
       " 'fare': 268,\n",
       " 'two': 746,\n",
       " 'happi': 329,\n",
       " 'without': 782,\n",
       " 'doubt': 220,\n",
       " 'favorit': 271,\n",
       " 'black': 75,\n",
       " 'seen': 591,\n",
       " 'especi': 244,\n",
       " 'mom': 439,\n",
       " 'pleasant': 509,\n",
       " 'dirti': 205,\n",
       " 'standard': 664,\n",
       " 'omg': 467,\n",
       " 'delicioso': 183,\n",
       " 'authent': 34,\n",
       " 'spaghetti': 649,\n",
       " 'veget': 757,\n",
       " 'brick': 88,\n",
       " 'oven': 475,\n",
       " 'multipl': 444,\n",
       " 'ten': 715,\n",
       " 'terribl': 717,\n",
       " 'equal': 243,\n",
       " 'pancak': 484,\n",
       " 'life': 392,\n",
       " 'door': 218,\n",
       " 'offer': 463,\n",
       " 'cool': 150,\n",
       " 'els': 235,\n",
       " 'handl': 327,\n",
       " 'rowdi': 568,\n",
       " 'find': 278,\n",
       " 'despic': 193,\n",
       " 'soup': 645,\n",
       " 'lukewarm': 409,\n",
       " 'deserv': 191,\n",
       " 'stomach': 679,\n",
       " 'space': 648,\n",
       " 'comfort': 141,\n",
       " 'eggplant': 233,\n",
       " 'green': 315,\n",
       " 'bean': 57,\n",
       " 'part': 487,\n",
       " 'dinner': 202,\n",
       " 'told': 732,\n",
       " 'happen': 328,\n",
       " 'car': 108,\n",
       " 'front': 296,\n",
       " 'starv': 667,\n",
       " 'disgrac': 211,\n",
       " 'def': 176,\n",
       " 'anyon': 14,\n",
       " 'stuf': 689,\n",
       " 'shop': 599,\n",
       " 'mall': 414,\n",
       " 'bring': 89,\n",
       " 'kid': 376,\n",
       " 'option': 470,\n",
       " 'perfect': 496,\n",
       " 'famili': 263,\n",
       " 'impecc': 361,\n",
       " 'simpli': 609,\n",
       " 'remind': 549,\n",
       " 'pop': 513,\n",
       " 'assur': 25,\n",
       " 'sore': 641,\n",
       " 'complet': 145,\n",
       " 'becom': 60,\n",
       " 'regular': 547,\n",
       " 'profession': 523,\n",
       " 'smear': 621,\n",
       " 'mistak': 436,\n",
       " 'nicest': 457,\n",
       " 'biscuit': 71,\n",
       " 'awkward': 41,\n",
       " 'cow': 156,\n",
       " 'steiner': 672,\n",
       " 'anyway': 17,\n",
       " 'week': 774,\n",
       " 'combin': 138,\n",
       " 'big': 69,\n",
       " 'spicier': 655,\n",
       " 'anytim': 16,\n",
       " 'contain': 147,\n",
       " 'driest': 225,\n",
       " 'relax': 548,\n",
       " 'group': 319,\n",
       " 'tot': 735,\n",
       " 'southwest': 647,\n",
       " 'paid': 483,\n",
       " 'smooth': 624,\n",
       " 'choux': 125,\n",
       " 'new': 454,\n",
       " 'acknowledg': 1,\n",
       " 'margarita': 417,\n",
       " 'rather': 535,\n",
       " 'camelback': 105,\n",
       " 'flower': 287,\n",
       " 'claim': 130,\n",
       " 'bill': 70,\n",
       " 'crab': 157,\n",
       " 'leg': 389,\n",
       " 'slice': 616,\n",
       " 'dont': 217,\n",
       " 'long': 401,\n",
       " 'attach': 29,\n",
       " 'awesom': 40,\n",
       " 'wors': 788,\n",
       " 'worker': 786,\n",
       " 'bunch': 96,\n",
       " 'call': 103,\n",
       " 'fill': 276,\n",
       " 'homemad': 344,\n",
       " 'thin': 720,\n",
       " 'gone': 309,\n",
       " 'similarli': 607,\n",
       " 'deliveri': 187,\n",
       " 'apolog': 18,\n",
       " 'pack': 482,\n",
       " 'whole': 778,\n",
       " 'choos': 124,\n",
       " 'entre': 242,\n",
       " 'stranger': 684,\n",
       " 'strang': 683,\n",
       " 'boyfriend': 84,\n",
       " 'recent': 543,\n",
       " 'room': 565,\n",
       " 'mayb': 421,\n",
       " 'howev': 352,\n",
       " 'tapa': 708,\n",
       " 'vinegrett': 761,\n",
       " 'babi': 42,\n",
       " 'believ': 64,\n",
       " 'guest': 321,\n",
       " 'super': 698,\n",
       " 'fan': 265,\n",
       " 'dri': 224,\n",
       " 'cheap': 114,\n",
       " 'present': 519,\n",
       " 'color': 137,\n",
       " 'sit': 612,\n",
       " 'fairli': 262,\n",
       " 'soooooo': 640,\n",
       " 'paper': 485,\n",
       " 'ok': 465,\n",
       " 'sorri': 642,\n",
       " 'despit': 194,\n",
       " 'mean': 424,\n",
       " 'half': 325,\n",
       " 'somehow': 630,\n",
       " 'non': 459,\n",
       " 'focus': 288,\n",
       " 'vibe': 760,\n",
       " 'somewhat': 634,\n",
       " 'edibl': 231,\n",
       " 'promis': 524,\n",
       " 'fail': 261,\n",
       " 'deliv': 186,\n",
       " 'averag': 35,\n",
       " 'italian': 370,\n",
       " 'legit': 390,\n",
       " 'somethat': 633,\n",
       " 'song': 636,\n",
       " 'ring': 558,\n",
       " 'smashburg': 620,\n",
       " 'spend': 652,\n",
       " 'slaw': 615,\n",
       " 'drench': 222,\n",
       " 'soundtrack': 644,\n",
       " 'plate': 506,\n",
       " 'auju': 33,\n",
       " 'smoothi': 625,\n",
       " 'needless': 451,\n",
       " 'cibo': 128,\n",
       " 'longer': 402,\n",
       " 'read': 537,\n",
       " 'simpl': 608,\n",
       " 'classic': 131,\n",
       " 'fli': 286,\n",
       " 'bare': 50,\n",
       " 'chines': 121,\n",
       " 'speedi': 651,\n",
       " 'anyth': 15,\n",
       " 'complain': 143,\n",
       " 'stood': 680,\n",
       " 'smaller': 619,\n",
       " 'spice': 653,\n",
       " 'crowd': 162,\n",
       " 'mid': 431,\n",
       " 'defin': 177,\n",
       " 'low': 408,\n",
       " 'sour': 646,\n",
       " 'style': 691,\n",
       " 'bother': 80,\n",
       " 'flavorless': 285,\n",
       " 'describ': 189,\n",
       " 'nacho': 448,\n",
       " 'crazi': 158,\n",
       " 'tribut': 742,\n",
       " 'salsa': 576,\n",
       " 'surpris': 700,\n",
       " 'fell': 273,\n",
       " 'devin': 197,\n",
       " 'employe': 237,\n",
       " 'put': 528,\n",
       " 'delic': 181,\n",
       " 'aw': 38,\n",
       " 'articl': 23,\n",
       " 'head': 332,\n",
       " 'round': 567,\n",
       " 'disbelief': 210,\n",
       " 'heat': 336,\n",
       " 'start': 666,\n",
       " 'store': 682,\n",
       " 'three': 726,\n",
       " 'rotat': 566,\n",
       " 'strawberri': 685,\n",
       " 'correct': 151,\n",
       " 'steakhous': 671,\n",
       " 'batch': 53,\n",
       " 'christma': 127,\n",
       " 'degre': 179,\n",
       " 'lost': 404,\n",
       " 'denni': 188,\n",
       " 'ridicul': 556,\n",
       " 'stretch': 687,\n",
       " 'undercook': 748,\n",
       " 'chipolt': 123,\n",
       " 'dip': 203,\n",
       " 'spotti': 659,\n",
       " 'deuchebaggeri': 196,\n",
       " 'smoke': 623,\n",
       " 'solidifi': 629,\n",
       " 'del': 180,\n",
       " 'disapppoint': 208,\n",
       " 'circumst': 129}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:22:26.912319Z",
     "start_time": "2020-04-01T11:22:26.871322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'absolut': 0,\n",
       " 'acknowledg': 1,\n",
       " 'actual': 2,\n",
       " 'ad': 3,\n",
       " 'ago': 4,\n",
       " 'almost': 5,\n",
       " 'also': 6,\n",
       " 'although': 7,\n",
       " 'alway': 8,\n",
       " 'amaz': 9,\n",
       " 'ambianc': 10,\n",
       " 'ambienc': 11,\n",
       " 'amount': 12,\n",
       " 'anoth': 13,\n",
       " 'anyon': 14,\n",
       " 'anyth': 15,\n",
       " 'anytim': 16,\n",
       " 'anyway': 17,\n",
       " 'apolog': 18,\n",
       " 'appet': 19,\n",
       " 'area': 20,\n",
       " 'around': 21,\n",
       " 'arriv': 22,\n",
       " 'articl': 23,\n",
       " 'ask': 24,\n",
       " 'assur': 25,\n",
       " 'ate': 26,\n",
       " 'atmospher': 27,\n",
       " 'atroci': 28,\n",
       " 'attach': 29,\n",
       " 'attack': 30,\n",
       " 'attent': 31,\n",
       " 'attitud': 32,\n",
       " 'auju': 33,\n",
       " 'authent': 34,\n",
       " 'averag': 35,\n",
       " 'avocado': 36,\n",
       " 'avoid': 37,\n",
       " 'aw': 38,\n",
       " 'away': 39,\n",
       " 'awesom': 40,\n",
       " 'awkward': 41,\n",
       " 'babi': 42,\n",
       " 'bachi': 43,\n",
       " 'back': 44,\n",
       " 'bacon': 45,\n",
       " 'bad': 46,\n",
       " 'bagel': 47,\n",
       " 'bakeri': 48,\n",
       " 'bar': 49,\n",
       " 'bare': 50,\n",
       " 'bartend': 51,\n",
       " 'basic': 52,\n",
       " 'batch': 53,\n",
       " 'bathroom': 54,\n",
       " 'batter': 55,\n",
       " 'bay': 56,\n",
       " 'bean': 57,\n",
       " 'beat': 58,\n",
       " 'beauti': 59,\n",
       " 'becom': 60,\n",
       " 'beef': 61,\n",
       " 'beer': 62,\n",
       " 'behind': 63,\n",
       " 'believ': 64,\n",
       " 'belli': 65,\n",
       " 'best': 66,\n",
       " 'better': 67,\n",
       " 'beyond': 68,\n",
       " 'big': 69,\n",
       " 'bill': 70,\n",
       " 'biscuit': 71,\n",
       " 'bisqu': 72,\n",
       " 'bit': 73,\n",
       " 'bite': 74,\n",
       " 'black': 75,\n",
       " 'bland': 76,\n",
       " 'blow': 77,\n",
       " 'boba': 78,\n",
       " 'boot': 79,\n",
       " 'bother': 80,\n",
       " 'bowl': 81,\n",
       " 'box': 82,\n",
       " 'boy': 83,\n",
       " 'boyfriend': 84,\n",
       " 'bread': 85,\n",
       " 'break': 86,\n",
       " 'breakfast': 87,\n",
       " 'brick': 88,\n",
       " 'bring': 89,\n",
       " 'brought': 90,\n",
       " 'brunch': 91,\n",
       " 'buck': 92,\n",
       " 'buffet': 93,\n",
       " 'bug': 94,\n",
       " 'build': 95,\n",
       " 'bunch': 96,\n",
       " 'burger': 97,\n",
       " 'busi': 98,\n",
       " 'butter': 99,\n",
       " 'bye': 100,\n",
       " 'cafe': 101,\n",
       " 'cake': 102,\n",
       " 'call': 103,\n",
       " 'came': 104,\n",
       " 'camelback': 105,\n",
       " 'cannot': 106,\n",
       " 'cant': 107,\n",
       " 'car': 108,\n",
       " 'care': 109,\n",
       " 'cashier': 110,\n",
       " 'char': 111,\n",
       " 'charcoal': 112,\n",
       " 'charg': 113,\n",
       " 'cheap': 114,\n",
       " 'check': 115,\n",
       " 'chees': 116,\n",
       " 'cheeseburg': 117,\n",
       " 'chef': 118,\n",
       " 'chewi': 119,\n",
       " 'chicken': 120,\n",
       " 'chines': 121,\n",
       " 'chip': 122,\n",
       " 'chipolt': 123,\n",
       " 'choos': 124,\n",
       " 'choux': 125,\n",
       " 'chow': 126,\n",
       " 'christma': 127,\n",
       " 'cibo': 128,\n",
       " 'circumst': 129,\n",
       " 'claim': 130,\n",
       " 'classic': 131,\n",
       " 'clean': 132,\n",
       " 'close': 133,\n",
       " 'cocktail': 134,\n",
       " 'coffe': 135,\n",
       " 'cold': 136,\n",
       " 'color': 137,\n",
       " 'combin': 138,\n",
       " 'combo': 139,\n",
       " 'come': 140,\n",
       " 'comfort': 141,\n",
       " 'compani': 142,\n",
       " 'complain': 143,\n",
       " 'complaint': 144,\n",
       " 'complet': 145,\n",
       " 'consid': 146,\n",
       " 'contain': 147,\n",
       " 'conveni': 148,\n",
       " 'cook': 149,\n",
       " 'cool': 150,\n",
       " 'correct': 151,\n",
       " 'could': 152,\n",
       " 'coupl': 153,\n",
       " 'cours': 154,\n",
       " 'cover': 155,\n",
       " 'cow': 156,\n",
       " 'crab': 157,\n",
       " 'crazi': 158,\n",
       " 'cream': 159,\n",
       " 'creami': 160,\n",
       " 'crispi': 161,\n",
       " 'crowd': 162,\n",
       " 'crust': 163,\n",
       " 'curri': 164,\n",
       " 'custom': 165,\n",
       " 'cut': 166,\n",
       " 'cute': 167,\n",
       " 'damn': 168,\n",
       " 'dark': 169,\n",
       " 'date': 170,\n",
       " 'day': 171,\n",
       " 'deal': 172,\n",
       " 'decent': 173,\n",
       " 'decid': 174,\n",
       " 'decor': 175,\n",
       " 'def': 176,\n",
       " 'defin': 177,\n",
       " 'definit': 178,\n",
       " 'degre': 179,\n",
       " 'del': 180,\n",
       " 'delic': 181,\n",
       " 'delici': 182,\n",
       " 'delicioso': 183,\n",
       " 'delight': 184,\n",
       " 'delish': 185,\n",
       " 'deliv': 186,\n",
       " 'deliveri': 187,\n",
       " 'denni': 188,\n",
       " 'describ': 189,\n",
       " 'descript': 190,\n",
       " 'deserv': 191,\n",
       " 'desir': 192,\n",
       " 'despic': 193,\n",
       " 'despit': 194,\n",
       " 'dessert': 195,\n",
       " 'deuchebaggeri': 196,\n",
       " 'devin': 197,\n",
       " 'die': 198,\n",
       " 'differ': 199,\n",
       " 'dime': 200,\n",
       " 'dine': 201,\n",
       " 'dinner': 202,\n",
       " 'dip': 203,\n",
       " 'dirt': 204,\n",
       " 'dirti': 205,\n",
       " 'disagre': 206,\n",
       " 'disappoint': 207,\n",
       " 'disapppoint': 208,\n",
       " 'disast': 209,\n",
       " 'disbelief': 210,\n",
       " 'disgrac': 211,\n",
       " 'disgust': 212,\n",
       " 'dish': 213,\n",
       " 'disrespect': 214,\n",
       " 'dog': 215,\n",
       " 'done': 216,\n",
       " 'dont': 217,\n",
       " 'door': 218,\n",
       " 'doubl': 219,\n",
       " 'doubt': 220,\n",
       " 'downtown': 221,\n",
       " 'drench': 222,\n",
       " 'dress': 223,\n",
       " 'dri': 224,\n",
       " 'driest': 225,\n",
       " 'drink': 226,\n",
       " 'drive': 227,\n",
       " 'duck': 228,\n",
       " 'eat': 229,\n",
       " 'eaten': 230,\n",
       " 'edibl': 231,\n",
       " 'egg': 232,\n",
       " 'eggplant': 233,\n",
       " 'either': 234,\n",
       " 'els': 235,\n",
       " 'elsewher': 236,\n",
       " 'employe': 237,\n",
       " 'empti': 238,\n",
       " 'end': 239,\n",
       " 'enjoy': 240,\n",
       " 'enough': 241,\n",
       " 'entre': 242,\n",
       " 'equal': 243,\n",
       " 'especi': 244,\n",
       " 'establish': 245,\n",
       " 'even': 246,\n",
       " 'event': 247,\n",
       " 'ever': 248,\n",
       " 'everi': 249,\n",
       " 'everyon': 250,\n",
       " 'everyth': 251,\n",
       " 'excel': 252,\n",
       " 'excus': 253,\n",
       " 'expect': 254,\n",
       " 'experi': 255,\n",
       " 'experienc': 256,\n",
       " 'extra': 257,\n",
       " 'extrem': 258,\n",
       " 'eye': 259,\n",
       " 'fact': 260,\n",
       " 'fail': 261,\n",
       " 'fairli': 262,\n",
       " 'famili': 263,\n",
       " 'familiar': 264,\n",
       " 'fan': 265,\n",
       " 'fantast': 266,\n",
       " 'far': 267,\n",
       " 'fare': 268,\n",
       " 'fast': 269,\n",
       " 'favor': 270,\n",
       " 'favorit': 271,\n",
       " 'feel': 272,\n",
       " 'fell': 273,\n",
       " 'felt': 274,\n",
       " 'filet': 275,\n",
       " 'fill': 276,\n",
       " 'final': 277,\n",
       " 'find': 278,\n",
       " 'fine': 279,\n",
       " 'finish': 280,\n",
       " 'first': 281,\n",
       " 'fish': 282,\n",
       " 'flat': 283,\n",
       " 'flavor': 284,\n",
       " 'flavorless': 285,\n",
       " 'fli': 286,\n",
       " 'flower': 287,\n",
       " 'focus': 288,\n",
       " 'folk': 289,\n",
       " 'food': 290,\n",
       " 'found': 291,\n",
       " 'fresh': 292,\n",
       " 'fri': 293,\n",
       " 'friend': 294,\n",
       " 'friendli': 295,\n",
       " 'front': 296,\n",
       " 'frozen': 297,\n",
       " 'full': 298,\n",
       " 'fun': 299,\n",
       " 'garlic': 300,\n",
       " 'gave': 301,\n",
       " 'gener': 302,\n",
       " 'get': 303,\n",
       " 'give': 304,\n",
       " 'given': 305,\n",
       " 'glad': 306,\n",
       " 'go': 307,\n",
       " 'gold': 308,\n",
       " 'gone': 309,\n",
       " 'good': 310,\n",
       " 'got': 311,\n",
       " 'greas': 312,\n",
       " 'great': 313,\n",
       " 'greek': 314,\n",
       " 'green': 315,\n",
       " 'greet': 316,\n",
       " 'grill': 317,\n",
       " 'gross': 318,\n",
       " 'group': 319,\n",
       " 'guess': 320,\n",
       " 'guest': 321,\n",
       " 'guy': 322,\n",
       " 'gyro': 323,\n",
       " 'hair': 324,\n",
       " 'half': 325,\n",
       " 'hand': 326,\n",
       " 'handl': 327,\n",
       " 'happen': 328,\n",
       " 'happi': 329,\n",
       " 'hard': 330,\n",
       " 'hate': 331,\n",
       " 'head': 332,\n",
       " 'healthi': 333,\n",
       " 'heard': 334,\n",
       " 'heart': 335,\n",
       " 'heat': 336,\n",
       " 'help': 337,\n",
       " 'high': 338,\n",
       " 'highli': 339,\n",
       " 'highlight': 340,\n",
       " 'hip': 341,\n",
       " 'hit': 342,\n",
       " 'home': 343,\n",
       " 'homemad': 344,\n",
       " 'honest': 345,\n",
       " 'honestli': 346,\n",
       " 'hope': 347,\n",
       " 'horribl': 348,\n",
       " 'hot': 349,\n",
       " 'hour': 350,\n",
       " 'hous': 351,\n",
       " 'howev': 352,\n",
       " 'huge': 353,\n",
       " 'human': 354,\n",
       " 'hummu': 355,\n",
       " 'husband': 356,\n",
       " 'ice': 357,\n",
       " 'ignor': 358,\n",
       " 'imagin': 359,\n",
       " 'immedi': 360,\n",
       " 'impecc': 361,\n",
       " 'impress': 362,\n",
       " 'includ': 363,\n",
       " 'incred': 364,\n",
       " 'indian': 365,\n",
       " 'inexpens': 366,\n",
       " 'insid': 367,\n",
       " 'insult': 368,\n",
       " 'interest': 369,\n",
       " 'italian': 370,\n",
       " 'job': 371,\n",
       " 'joint': 372,\n",
       " 'joke': 373,\n",
       " 'judg': 374,\n",
       " 'kept': 375,\n",
       " 'kid': 376,\n",
       " 'kind': 377,\n",
       " 'know': 378,\n",
       " 'known': 379,\n",
       " 'lack': 380,\n",
       " 'ladi': 381,\n",
       " 'larg': 382,\n",
       " 'last': 383,\n",
       " 'late': 384,\n",
       " 'later': 385,\n",
       " 'least': 386,\n",
       " 'leav': 387,\n",
       " 'left': 388,\n",
       " 'leg': 389,\n",
       " 'legit': 390,\n",
       " 'let': 391,\n",
       " 'life': 392,\n",
       " 'light': 393,\n",
       " 'like': 394,\n",
       " 'list': 395,\n",
       " 'liter': 396,\n",
       " 'littl': 397,\n",
       " 'live': 398,\n",
       " 'lobster': 399,\n",
       " 'locat': 400,\n",
       " 'long': 401,\n",
       " 'longer': 402,\n",
       " 'look': 403,\n",
       " 'lost': 404,\n",
       " 'lot': 405,\n",
       " 'love': 406,\n",
       " 'lover': 407,\n",
       " 'low': 408,\n",
       " 'lukewarm': 409,\n",
       " 'lunch': 410,\n",
       " 'made': 411,\n",
       " 'main': 412,\n",
       " 'make': 413,\n",
       " 'mall': 414,\n",
       " 'manag': 415,\n",
       " 'mani': 416,\n",
       " 'margarita': 417,\n",
       " 'mari': 418,\n",
       " 'marrow': 419,\n",
       " 'may': 420,\n",
       " 'mayb': 421,\n",
       " 'mayo': 422,\n",
       " 'meal': 423,\n",
       " 'mean': 424,\n",
       " 'meat': 425,\n",
       " 'mediocr': 426,\n",
       " 'meh': 427,\n",
       " 'melt': 428,\n",
       " 'menu': 429,\n",
       " 'mexican': 430,\n",
       " 'mid': 431,\n",
       " 'min': 432,\n",
       " 'mind': 433,\n",
       " 'minut': 434,\n",
       " 'miss': 435,\n",
       " 'mistak': 436,\n",
       " 'mmmm': 437,\n",
       " 'moist': 438,\n",
       " 'mom': 439,\n",
       " 'money': 440,\n",
       " 'mood': 441,\n",
       " 'mouth': 442,\n",
       " 'much': 443,\n",
       " 'multipl': 444,\n",
       " 'mushroom': 445,\n",
       " 'music': 446,\n",
       " 'must': 447,\n",
       " 'nacho': 448,\n",
       " 'nasti': 449,\n",
       " 'need': 450,\n",
       " 'needless': 451,\n",
       " 'neighborhood': 452,\n",
       " 'never': 453,\n",
       " 'new': 454,\n",
       " 'next': 455,\n",
       " 'nice': 456,\n",
       " 'nicest': 457,\n",
       " 'night': 458,\n",
       " 'non': 459,\n",
       " 'none': 460,\n",
       " 'note': 461,\n",
       " 'noth': 462,\n",
       " 'offer': 463,\n",
       " 'oh': 464,\n",
       " 'ok': 465,\n",
       " 'old': 466,\n",
       " 'omg': 467,\n",
       " 'one': 468,\n",
       " 'opportun': 469,\n",
       " 'option': 470,\n",
       " 'order': 471,\n",
       " 'other': 472,\n",
       " 'outsid': 473,\n",
       " 'outstand': 474,\n",
       " 'oven': 475,\n",
       " 'overal': 476,\n",
       " 'overcook': 477,\n",
       " 'overpr': 478,\n",
       " 'overwhelm': 479,\n",
       " 'owner': 480,\n",
       " 'pace': 481,\n",
       " 'pack': 482,\n",
       " 'paid': 483,\n",
       " 'pancak': 484,\n",
       " 'paper': 485,\n",
       " 'par': 486,\n",
       " 'part': 487,\n",
       " 'parti': 488,\n",
       " 'pass': 489,\n",
       " 'pasta': 490,\n",
       " 'patio': 491,\n",
       " 'pay': 492,\n",
       " 'peanut': 493,\n",
       " 'peopl': 494,\n",
       " 'pepper': 495,\n",
       " 'perfect': 496,\n",
       " 'perfectli': 497,\n",
       " 'person': 498,\n",
       " 'pho': 499,\n",
       " 'phoenix': 500,\n",
       " 'pictur': 501,\n",
       " 'piec': 502,\n",
       " 'pita': 503,\n",
       " 'pizza': 504,\n",
       " 'place': 505,\n",
       " 'plate': 506,\n",
       " 'play': 507,\n",
       " 'pleas': 508,\n",
       " 'pleasant': 509,\n",
       " 'plu': 510,\n",
       " 'point': 511,\n",
       " 'poor': 512,\n",
       " 'pop': 513,\n",
       " 'pork': 514,\n",
       " 'portion': 515,\n",
       " 'possibl': 516,\n",
       " 'potato': 517,\n",
       " 'prepar': 518,\n",
       " 'present': 519,\n",
       " 'pretti': 520,\n",
       " 'price': 521,\n",
       " 'probabl': 522,\n",
       " 'profession': 523,\n",
       " 'promis': 524,\n",
       " 'provid': 525,\n",
       " 'pull': 526,\n",
       " 'pure': 527,\n",
       " 'put': 528,\n",
       " 'qualiti': 529,\n",
       " 'quick': 530,\n",
       " 'quickli': 531,\n",
       " 'quit': 532,\n",
       " 'rare': 533,\n",
       " 'rate': 534,\n",
       " 'rather': 535,\n",
       " 'rave': 536,\n",
       " 'read': 537,\n",
       " 'real': 538,\n",
       " 'realiz': 539,\n",
       " 'realli': 540,\n",
       " 'reason': 541,\n",
       " 'receiv': 542,\n",
       " 'recent': 543,\n",
       " 'recommend': 544,\n",
       " 'red': 545,\n",
       " 'refil': 546,\n",
       " 'regular': 547,\n",
       " 'relax': 548,\n",
       " 'remind': 549,\n",
       " 'restaur': 550,\n",
       " 'return': 551,\n",
       " 'review': 552,\n",
       " 'rice': 553,\n",
       " 'rich': 554,\n",
       " 'rick': 555,\n",
       " 'ridicul': 556,\n",
       " 'right': 557,\n",
       " 'ring': 558,\n",
       " 'rins': 559,\n",
       " 'rip': 560,\n",
       " 'risotto': 561,\n",
       " 'roast': 562,\n",
       " 'rock': 563,\n",
       " 'roll': 564,\n",
       " 'room': 565,\n",
       " 'rotat': 566,\n",
       " 'round': 567,\n",
       " 'rowdi': 568,\n",
       " 'rubber': 569,\n",
       " 'rude': 570,\n",
       " 'run': 571,\n",
       " 'sad': 572,\n",
       " 'said': 573,\n",
       " 'salad': 574,\n",
       " 'salmon': 575,\n",
       " 'salsa': 576,\n",
       " 'salt': 577,\n",
       " 'sandwich': 578,\n",
       " 'sashimi': 579,\n",
       " 'sat': 580,\n",
       " 'satisfi': 581,\n",
       " 'sauc': 582,\n",
       " 'say': 583,\n",
       " 'scallop': 584,\n",
       " 'seafood': 585,\n",
       " 'season': 586,\n",
       " 'seat': 587,\n",
       " 'second': 588,\n",
       " 'see': 589,\n",
       " 'seem': 590,\n",
       " 'seen': 591,\n",
       " 'select': 592,\n",
       " 'serious': 593,\n",
       " 'serv': 594,\n",
       " 'server': 595,\n",
       " 'servic': 596,\n",
       " 'set': 597,\n",
       " 'sever': 598,\n",
       " 'shop': 599,\n",
       " 'show': 600,\n",
       " 'shower': 601,\n",
       " 'shrimp': 602,\n",
       " 'sick': 603,\n",
       " 'side': 604,\n",
       " 'sign': 605,\n",
       " 'similar': 606,\n",
       " 'similarli': 607,\n",
       " 'simpl': 608,\n",
       " 'simpli': 609,\n",
       " 'sinc': 610,\n",
       " 'singl': 611,\n",
       " 'sit': 612,\n",
       " 'six': 613,\n",
       " 'skimp': 614,\n",
       " 'slaw': 615,\n",
       " 'slice': 616,\n",
       " 'slow': 617,\n",
       " 'small': 618,\n",
       " 'smaller': 619,\n",
       " 'smashburg': 620,\n",
       " 'smear': 621,\n",
       " 'smell': 622,\n",
       " 'smoke': 623,\n",
       " 'smooth': 624,\n",
       " 'smoothi': 625,\n",
       " 'soggi': 626,\n",
       " 'soi': 627,\n",
       " 'solid': 628,\n",
       " 'solidifi': 629,\n",
       " 'somehow': 630,\n",
       " 'someon': 631,\n",
       " 'someth': 632,\n",
       " 'somethat': 633,\n",
       " 'somewhat': 634,\n",
       " 'son': 635,\n",
       " 'song': 636,\n",
       " 'soon': 637,\n",
       " 'soooo': 638,\n",
       " 'sooooo': 639,\n",
       " 'soooooo': 640,\n",
       " 'sore': 641,\n",
       " 'sorri': 642,\n",
       " 'sound': 643,\n",
       " 'soundtrack': 644,\n",
       " 'soup': 645,\n",
       " 'sour': 646,\n",
       " 'southwest': 647,\n",
       " 'space': 648,\n",
       " 'spaghetti': 649,\n",
       " 'special': 650,\n",
       " 'speedi': 651,\n",
       " 'spend': 652,\n",
       " 'spice': 653,\n",
       " 'spici': 654,\n",
       " 'spicier': 655,\n",
       " 'spinach': 656,\n",
       " 'sport': 657,\n",
       " 'spot': 658,\n",
       " 'spotti': 659,\n",
       " 'spring': 660,\n",
       " 'sprout': 661,\n",
       " 'staff': 662,\n",
       " 'stale': 663,\n",
       " 'standard': 664,\n",
       " 'star': 665,\n",
       " 'start': 666,\n",
       " 'starv': 667,\n",
       " 'station': 668,\n",
       " 'stay': 669,\n",
       " 'steak': 670,\n",
       " 'steakhous': 671,\n",
       " 'steiner': 672,\n",
       " 'step': 673,\n",
       " 'steve': 674,\n",
       " 'stick': 675,\n",
       " 'still': 676,\n",
       " 'stink': 677,\n",
       " 'stir': 678,\n",
       " 'stomach': 679,\n",
       " 'stood': 680,\n",
       " 'stop': 681,\n",
       " 'store': 682,\n",
       " 'strang': 683,\n",
       " 'stranger': 684,\n",
       " 'strawberri': 685,\n",
       " 'street': 686,\n",
       " 'stretch': 687,\n",
       " 'strip': 688,\n",
       " 'stuf': 689,\n",
       " 'stuff': 690,\n",
       " 'style': 691,\n",
       " 'sub': 692,\n",
       " 'subway': 693,\n",
       " 'suck': 694,\n",
       " 'sugari': 695,\n",
       " 'suggest': 696,\n",
       " 'summer': 697,\n",
       " 'super': 698,\n",
       " 'sure': 699,\n",
       " 'surpris': 700,\n",
       " 'sushi': 701,\n",
       " 'sweet': 702,\n",
       " 'tabl': 703,\n",
       " 'taco': 704,\n",
       " 'take': 705,\n",
       " 'talk': 706,\n",
       " 'tap': 707,\n",
       " 'tapa': 708,\n",
       " 'tartar': 709,\n",
       " 'tast': 710,\n",
       " 'tasteless': 711,\n",
       " 'tasti': 712,\n",
       " 'tea': 713,\n",
       " 'tell': 714,\n",
       " 'ten': 715,\n",
       " 'tender': 716,\n",
       " 'terribl': 717,\n",
       " 'textur': 718,\n",
       " 'thai': 719,\n",
       " 'thin': 720,\n",
       " 'thing': 721,\n",
       " 'think': 722,\n",
       " 'third': 723,\n",
       " 'though': 724,\n",
       " 'thought': 725,\n",
       " 'three': 726,\n",
       " 'thumb': 727,\n",
       " 'time': 728,\n",
       " 'tip': 729,\n",
       " 'toast': 730,\n",
       " 'today': 731,\n",
       " 'told': 732,\n",
       " 'took': 733,\n",
       " 'top': 734,\n",
       " 'tot': 735,\n",
       " 'total': 736,\n",
       " 'touch': 737,\n",
       " 'toward': 738,\n",
       " 'town': 739,\n",
       " 'treat': 740,\n",
       " 'tri': 741,\n",
       " 'tribut': 742,\n",
       " 'trip': 743,\n",
       " 'tuna': 744,\n",
       " 'twice': 745,\n",
       " 'two': 746,\n",
       " 'unbeliev': 747,\n",
       " 'undercook': 748,\n",
       " 'underwhelm': 749,\n",
       " 'unfortun': 750,\n",
       " 'unless': 751,\n",
       " 'us': 752,\n",
       " 'use': 753,\n",
       " 'valley': 754,\n",
       " 'valu': 755,\n",
       " 'vega': 756,\n",
       " 'veget': 757,\n",
       " 'vegetarian': 758,\n",
       " 'ventur': 759,\n",
       " 'vibe': 760,\n",
       " 'vinegrett': 761,\n",
       " 'visit': 762,\n",
       " 'wait': 763,\n",
       " 'waiter': 764,\n",
       " 'waitress': 765,\n",
       " 'walk': 766,\n",
       " 'wall': 767,\n",
       " 'want': 768,\n",
       " 'warm': 769,\n",
       " 'wast': 770,\n",
       " 'watch': 771,\n",
       " 'water': 772,\n",
       " 'way': 773,\n",
       " 'week': 774,\n",
       " 'well': 775,\n",
       " 'went': 776,\n",
       " 'white': 777,\n",
       " 'whole': 778,\n",
       " 'wife': 779,\n",
       " 'wine': 780,\n",
       " 'wing': 781,\n",
       " 'without': 782,\n",
       " 'wonder': 783,\n",
       " 'word': 784,\n",
       " 'work': 785,\n",
       " 'worker': 786,\n",
       " 'world': 787,\n",
       " 'wors': 788,\n",
       " 'worst': 789,\n",
       " 'worth': 790,\n",
       " 'would': 791,\n",
       " 'wow': 792,\n",
       " 'wrap': 793,\n",
       " 'wrong': 794,\n",
       " 'year': 795,\n",
       " 'yet': 796,\n",
       " 'yum': 797,\n",
       " 'yummi': 798,\n",
       " 'zero': 799}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:22:27.107326Z",
     "start_time": "2020-04-01T11:22:27.072319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'absolut': 0,\n",
       " 'acknowledg': 1,\n",
       " 'actual': 2,\n",
       " 'ad': 3,\n",
       " 'ago': 4,\n",
       " 'almost': 5,\n",
       " 'also': 6,\n",
       " 'although': 7,\n",
       " 'alway': 8,\n",
       " 'amaz': 9,\n",
       " 'ambianc': 10,\n",
       " 'ambienc': 11,\n",
       " 'amount': 12,\n",
       " 'anoth': 13,\n",
       " 'anyon': 14,\n",
       " 'anyth': 15,\n",
       " 'anytim': 16,\n",
       " 'anyway': 17,\n",
       " 'apolog': 18,\n",
       " 'appet': 19,\n",
       " 'area': 20,\n",
       " 'around': 21,\n",
       " 'arriv': 22,\n",
       " 'articl': 23,\n",
       " 'ask': 24,\n",
       " 'assur': 25,\n",
       " 'ate': 26,\n",
       " 'atmospher': 27,\n",
       " 'atroci': 28,\n",
       " 'attach': 29,\n",
       " 'attack': 30,\n",
       " 'attent': 31,\n",
       " 'attitud': 32,\n",
       " 'auju': 33,\n",
       " 'authent': 34,\n",
       " 'averag': 35,\n",
       " 'avocado': 36,\n",
       " 'avoid': 37,\n",
       " 'aw': 38,\n",
       " 'away': 39,\n",
       " 'awesom': 40,\n",
       " 'awkward': 41,\n",
       " 'babi': 42,\n",
       " 'bachi': 43,\n",
       " 'back': 44,\n",
       " 'bacon': 45,\n",
       " 'bad': 46,\n",
       " 'bagel': 47,\n",
       " 'bakeri': 48,\n",
       " 'bar': 49,\n",
       " 'bare': 50,\n",
       " 'bartend': 51,\n",
       " 'basic': 52,\n",
       " 'batch': 53,\n",
       " 'bathroom': 54,\n",
       " 'batter': 55,\n",
       " 'bay': 56,\n",
       " 'bean': 57,\n",
       " 'beat': 58,\n",
       " 'beauti': 59,\n",
       " 'becom': 60,\n",
       " 'beef': 61,\n",
       " 'beer': 62,\n",
       " 'behind': 63,\n",
       " 'believ': 64,\n",
       " 'belli': 65,\n",
       " 'best': 66,\n",
       " 'better': 67,\n",
       " 'beyond': 68,\n",
       " 'big': 69,\n",
       " 'bill': 70,\n",
       " 'biscuit': 71,\n",
       " 'bisqu': 72,\n",
       " 'bit': 73,\n",
       " 'bite': 74,\n",
       " 'black': 75,\n",
       " 'bland': 76,\n",
       " 'blow': 77,\n",
       " 'boba': 78,\n",
       " 'boot': 79,\n",
       " 'bother': 80,\n",
       " 'bowl': 81,\n",
       " 'box': 82,\n",
       " 'boy': 83,\n",
       " 'boyfriend': 84,\n",
       " 'bread': 85,\n",
       " 'break': 86,\n",
       " 'breakfast': 87,\n",
       " 'brick': 88,\n",
       " 'bring': 89,\n",
       " 'brought': 90,\n",
       " 'brunch': 91,\n",
       " 'buck': 92,\n",
       " 'buffet': 93,\n",
       " 'bug': 94,\n",
       " 'build': 95,\n",
       " 'bunch': 96,\n",
       " 'burger': 97,\n",
       " 'busi': 98,\n",
       " 'butter': 99,\n",
       " 'bye': 100,\n",
       " 'cafe': 101,\n",
       " 'cake': 102,\n",
       " 'call': 103,\n",
       " 'came': 104,\n",
       " 'camelback': 105,\n",
       " 'cannot': 106,\n",
       " 'cant': 107,\n",
       " 'car': 108,\n",
       " 'care': 109,\n",
       " 'cashier': 110,\n",
       " 'char': 111,\n",
       " 'charcoal': 112,\n",
       " 'charg': 113,\n",
       " 'cheap': 114,\n",
       " 'check': 115,\n",
       " 'chees': 116,\n",
       " 'cheeseburg': 117,\n",
       " 'chef': 118,\n",
       " 'chewi': 119,\n",
       " 'chicken': 120,\n",
       " 'chines': 121,\n",
       " 'chip': 122,\n",
       " 'chipolt': 123,\n",
       " 'choos': 124,\n",
       " 'choux': 125,\n",
       " 'chow': 126,\n",
       " 'christma': 127,\n",
       " 'cibo': 128,\n",
       " 'circumst': 129,\n",
       " 'claim': 130,\n",
       " 'classic': 131,\n",
       " 'clean': 132,\n",
       " 'close': 133,\n",
       " 'cocktail': 134,\n",
       " 'coffe': 135,\n",
       " 'cold': 136,\n",
       " 'color': 137,\n",
       " 'combin': 138,\n",
       " 'combo': 139,\n",
       " 'come': 140,\n",
       " 'comfort': 141,\n",
       " 'compani': 142,\n",
       " 'complain': 143,\n",
       " 'complaint': 144,\n",
       " 'complet': 145,\n",
       " 'consid': 146,\n",
       " 'contain': 147,\n",
       " 'conveni': 148,\n",
       " 'cook': 149,\n",
       " 'cool': 150,\n",
       " 'correct': 151,\n",
       " 'could': 152,\n",
       " 'coupl': 153,\n",
       " 'cours': 154,\n",
       " 'cover': 155,\n",
       " 'cow': 156,\n",
       " 'crab': 157,\n",
       " 'crazi': 158,\n",
       " 'cream': 159,\n",
       " 'creami': 160,\n",
       " 'crispi': 161,\n",
       " 'crowd': 162,\n",
       " 'crust': 163,\n",
       " 'curri': 164,\n",
       " 'custom': 165,\n",
       " 'cut': 166,\n",
       " 'cute': 167,\n",
       " 'damn': 168,\n",
       " 'dark': 169,\n",
       " 'date': 170,\n",
       " 'day': 171,\n",
       " 'deal': 172,\n",
       " 'decent': 173,\n",
       " 'decid': 174,\n",
       " 'decor': 175,\n",
       " 'def': 176,\n",
       " 'defin': 177,\n",
       " 'definit': 178,\n",
       " 'degre': 179,\n",
       " 'del': 180,\n",
       " 'delic': 181,\n",
       " 'delici': 182,\n",
       " 'delicioso': 183,\n",
       " 'delight': 184,\n",
       " 'delish': 185,\n",
       " 'deliv': 186,\n",
       " 'deliveri': 187,\n",
       " 'denni': 188,\n",
       " 'describ': 189,\n",
       " 'descript': 190,\n",
       " 'deserv': 191,\n",
       " 'desir': 192,\n",
       " 'despic': 193,\n",
       " 'despit': 194,\n",
       " 'dessert': 195,\n",
       " 'deuchebaggeri': 196,\n",
       " 'devin': 197,\n",
       " 'die': 198,\n",
       " 'differ': 199,\n",
       " 'dime': 200,\n",
       " 'dine': 201,\n",
       " 'dinner': 202,\n",
       " 'dip': 203,\n",
       " 'dirt': 204,\n",
       " 'dirti': 205,\n",
       " 'disagre': 206,\n",
       " 'disappoint': 207,\n",
       " 'disapppoint': 208,\n",
       " 'disast': 209,\n",
       " 'disbelief': 210,\n",
       " 'disgrac': 211,\n",
       " 'disgust': 212,\n",
       " 'dish': 213,\n",
       " 'disrespect': 214,\n",
       " 'dog': 215,\n",
       " 'done': 216,\n",
       " 'dont': 217,\n",
       " 'door': 218,\n",
       " 'doubl': 219,\n",
       " 'doubt': 220,\n",
       " 'downtown': 221,\n",
       " 'drench': 222,\n",
       " 'dress': 223,\n",
       " 'dri': 224,\n",
       " 'driest': 225,\n",
       " 'drink': 226,\n",
       " 'drive': 227,\n",
       " 'duck': 228,\n",
       " 'eat': 229,\n",
       " 'eaten': 230,\n",
       " 'edibl': 231,\n",
       " 'egg': 232,\n",
       " 'eggplant': 233,\n",
       " 'either': 234,\n",
       " 'els': 235,\n",
       " 'elsewher': 236,\n",
       " 'employe': 237,\n",
       " 'empti': 238,\n",
       " 'end': 239,\n",
       " 'enjoy': 240,\n",
       " 'enough': 241,\n",
       " 'entre': 242,\n",
       " 'equal': 243,\n",
       " 'especi': 244,\n",
       " 'establish': 245,\n",
       " 'even': 246,\n",
       " 'event': 247,\n",
       " 'ever': 248,\n",
       " 'everi': 249,\n",
       " 'everyon': 250,\n",
       " 'everyth': 251,\n",
       " 'excel': 252,\n",
       " 'excus': 253,\n",
       " 'expect': 254,\n",
       " 'experi': 255,\n",
       " 'experienc': 256,\n",
       " 'extra': 257,\n",
       " 'extrem': 258,\n",
       " 'eye': 259,\n",
       " 'fact': 260,\n",
       " 'fail': 261,\n",
       " 'fairli': 262,\n",
       " 'famili': 263,\n",
       " 'familiar': 264,\n",
       " 'fan': 265,\n",
       " 'fantast': 266,\n",
       " 'far': 267,\n",
       " 'fare': 268,\n",
       " 'fast': 269,\n",
       " 'favor': 270,\n",
       " 'favorit': 271,\n",
       " 'feel': 272,\n",
       " 'fell': 273,\n",
       " 'felt': 274,\n",
       " 'filet': 275,\n",
       " 'fill': 276,\n",
       " 'final': 277,\n",
       " 'find': 278,\n",
       " 'fine': 279,\n",
       " 'finish': 280,\n",
       " 'first': 281,\n",
       " 'fish': 282,\n",
       " 'flat': 283,\n",
       " 'flavor': 284,\n",
       " 'flavorless': 285,\n",
       " 'fli': 286,\n",
       " 'flower': 287,\n",
       " 'focus': 288,\n",
       " 'folk': 289,\n",
       " 'food': 290,\n",
       " 'found': 291,\n",
       " 'fresh': 292,\n",
       " 'fri': 293,\n",
       " 'friend': 294,\n",
       " 'friendli': 295,\n",
       " 'front': 296,\n",
       " 'frozen': 297,\n",
       " 'full': 298,\n",
       " 'fun': 299,\n",
       " 'garlic': 300,\n",
       " 'gave': 301,\n",
       " 'gener': 302,\n",
       " 'get': 303,\n",
       " 'give': 304,\n",
       " 'given': 305,\n",
       " 'glad': 306,\n",
       " 'go': 307,\n",
       " 'gold': 308,\n",
       " 'gone': 309,\n",
       " 'good': 310,\n",
       " 'got': 311,\n",
       " 'greas': 312,\n",
       " 'great': 313,\n",
       " 'greek': 314,\n",
       " 'green': 315,\n",
       " 'greet': 316,\n",
       " 'grill': 317,\n",
       " 'gross': 318,\n",
       " 'group': 319,\n",
       " 'guess': 320,\n",
       " 'guest': 321,\n",
       " 'guy': 322,\n",
       " 'gyro': 323,\n",
       " 'hair': 324,\n",
       " 'half': 325,\n",
       " 'hand': 326,\n",
       " 'handl': 327,\n",
       " 'happen': 328,\n",
       " 'happi': 329,\n",
       " 'hard': 330,\n",
       " 'hate': 331,\n",
       " 'head': 332,\n",
       " 'healthi': 333,\n",
       " 'heard': 334,\n",
       " 'heart': 335,\n",
       " 'heat': 336,\n",
       " 'help': 337,\n",
       " 'high': 338,\n",
       " 'highli': 339,\n",
       " 'highlight': 340,\n",
       " 'hip': 341,\n",
       " 'hit': 342,\n",
       " 'home': 343,\n",
       " 'homemad': 344,\n",
       " 'honest': 345,\n",
       " 'honestli': 346,\n",
       " 'hope': 347,\n",
       " 'horribl': 348,\n",
       " 'hot': 349,\n",
       " 'hour': 350,\n",
       " 'hous': 351,\n",
       " 'howev': 352,\n",
       " 'huge': 353,\n",
       " 'human': 354,\n",
       " 'hummu': 355,\n",
       " 'husband': 356,\n",
       " 'ice': 357,\n",
       " 'ignor': 358,\n",
       " 'imagin': 359,\n",
       " 'immedi': 360,\n",
       " 'impecc': 361,\n",
       " 'impress': 362,\n",
       " 'includ': 363,\n",
       " 'incred': 364,\n",
       " 'indian': 365,\n",
       " 'inexpens': 366,\n",
       " 'insid': 367,\n",
       " 'insult': 368,\n",
       " 'interest': 369,\n",
       " 'italian': 370,\n",
       " 'job': 371,\n",
       " 'joint': 372,\n",
       " 'joke': 373,\n",
       " 'judg': 374,\n",
       " 'kept': 375,\n",
       " 'kid': 376,\n",
       " 'kind': 377,\n",
       " 'know': 378,\n",
       " 'known': 379,\n",
       " 'lack': 380,\n",
       " 'ladi': 381,\n",
       " 'larg': 382,\n",
       " 'last': 383,\n",
       " 'late': 384,\n",
       " 'later': 385,\n",
       " 'least': 386,\n",
       " 'leav': 387,\n",
       " 'left': 388,\n",
       " 'leg': 389,\n",
       " 'legit': 390,\n",
       " 'let': 391,\n",
       " 'life': 392,\n",
       " 'light': 393,\n",
       " 'like': 394,\n",
       " 'list': 395,\n",
       " 'liter': 396,\n",
       " 'littl': 397,\n",
       " 'live': 398,\n",
       " 'lobster': 399,\n",
       " 'locat': 400,\n",
       " 'long': 401,\n",
       " 'longer': 402,\n",
       " 'look': 403,\n",
       " 'lost': 404,\n",
       " 'lot': 405,\n",
       " 'love': 406,\n",
       " 'lover': 407,\n",
       " 'low': 408,\n",
       " 'lukewarm': 409,\n",
       " 'lunch': 410,\n",
       " 'made': 411,\n",
       " 'main': 412,\n",
       " 'make': 413,\n",
       " 'mall': 414,\n",
       " 'manag': 415,\n",
       " 'mani': 416,\n",
       " 'margarita': 417,\n",
       " 'mari': 418,\n",
       " 'marrow': 419,\n",
       " 'may': 420,\n",
       " 'mayb': 421,\n",
       " 'mayo': 422,\n",
       " 'meal': 423,\n",
       " 'mean': 424,\n",
       " 'meat': 425,\n",
       " 'mediocr': 426,\n",
       " 'meh': 427,\n",
       " 'melt': 428,\n",
       " 'menu': 429,\n",
       " 'mexican': 430,\n",
       " 'mid': 431,\n",
       " 'min': 432,\n",
       " 'mind': 433,\n",
       " 'minut': 434,\n",
       " 'miss': 435,\n",
       " 'mistak': 436,\n",
       " 'mmmm': 437,\n",
       " 'moist': 438,\n",
       " 'mom': 439,\n",
       " 'money': 440,\n",
       " 'mood': 441,\n",
       " 'mouth': 442,\n",
       " 'much': 443,\n",
       " 'multipl': 444,\n",
       " 'mushroom': 445,\n",
       " 'music': 446,\n",
       " 'must': 447,\n",
       " 'nacho': 448,\n",
       " 'nasti': 449,\n",
       " 'need': 450,\n",
       " 'needless': 451,\n",
       " 'neighborhood': 452,\n",
       " 'never': 453,\n",
       " 'new': 454,\n",
       " 'next': 455,\n",
       " 'nice': 456,\n",
       " 'nicest': 457,\n",
       " 'night': 458,\n",
       " 'non': 459,\n",
       " 'none': 460,\n",
       " 'note': 461,\n",
       " 'noth': 462,\n",
       " 'offer': 463,\n",
       " 'oh': 464,\n",
       " 'ok': 465,\n",
       " 'old': 466,\n",
       " 'omg': 467,\n",
       " 'one': 468,\n",
       " 'opportun': 469,\n",
       " 'option': 470,\n",
       " 'order': 471,\n",
       " 'other': 472,\n",
       " 'outsid': 473,\n",
       " 'outstand': 474,\n",
       " 'oven': 475,\n",
       " 'overal': 476,\n",
       " 'overcook': 477,\n",
       " 'overpr': 478,\n",
       " 'overwhelm': 479,\n",
       " 'owner': 480,\n",
       " 'pace': 481,\n",
       " 'pack': 482,\n",
       " 'paid': 483,\n",
       " 'pancak': 484,\n",
       " 'paper': 485,\n",
       " 'par': 486,\n",
       " 'part': 487,\n",
       " 'parti': 488,\n",
       " 'pass': 489,\n",
       " 'pasta': 490,\n",
       " 'patio': 491,\n",
       " 'pay': 492,\n",
       " 'peanut': 493,\n",
       " 'peopl': 494,\n",
       " 'pepper': 495,\n",
       " 'perfect': 496,\n",
       " 'perfectli': 497,\n",
       " 'person': 498,\n",
       " 'pho': 499,\n",
       " 'phoenix': 500,\n",
       " 'pictur': 501,\n",
       " 'piec': 502,\n",
       " 'pita': 503,\n",
       " 'pizza': 504,\n",
       " 'place': 505,\n",
       " 'plate': 506,\n",
       " 'play': 507,\n",
       " 'pleas': 508,\n",
       " 'pleasant': 509,\n",
       " 'plu': 510,\n",
       " 'point': 511,\n",
       " 'poor': 512,\n",
       " 'pop': 513,\n",
       " 'pork': 514,\n",
       " 'portion': 515,\n",
       " 'possibl': 516,\n",
       " 'potato': 517,\n",
       " 'prepar': 518,\n",
       " 'present': 519,\n",
       " 'pretti': 520,\n",
       " 'price': 521,\n",
       " 'probabl': 522,\n",
       " 'profession': 523,\n",
       " 'promis': 524,\n",
       " 'provid': 525,\n",
       " 'pull': 526,\n",
       " 'pure': 527,\n",
       " 'put': 528,\n",
       " 'qualiti': 529,\n",
       " 'quick': 530,\n",
       " 'quickli': 531,\n",
       " 'quit': 532,\n",
       " 'rare': 533,\n",
       " 'rate': 534,\n",
       " 'rather': 535,\n",
       " 'rave': 536,\n",
       " 'read': 537,\n",
       " 'real': 538,\n",
       " 'realiz': 539,\n",
       " 'realli': 540,\n",
       " 'reason': 541,\n",
       " 'receiv': 542,\n",
       " 'recent': 543,\n",
       " 'recommend': 544,\n",
       " 'red': 545,\n",
       " 'refil': 546,\n",
       " 'regular': 547,\n",
       " 'relax': 548,\n",
       " 'remind': 549,\n",
       " 'restaur': 550,\n",
       " 'return': 551,\n",
       " 'review': 552,\n",
       " 'rice': 553,\n",
       " 'rich': 554,\n",
       " 'rick': 555,\n",
       " 'ridicul': 556,\n",
       " 'right': 557,\n",
       " 'ring': 558,\n",
       " 'rins': 559,\n",
       " 'rip': 560,\n",
       " 'risotto': 561,\n",
       " 'roast': 562,\n",
       " 'rock': 563,\n",
       " 'roll': 564,\n",
       " 'room': 565,\n",
       " 'rotat': 566,\n",
       " 'round': 567,\n",
       " 'rowdi': 568,\n",
       " 'rubber': 569,\n",
       " 'rude': 570,\n",
       " 'run': 571,\n",
       " 'sad': 572,\n",
       " 'said': 573,\n",
       " 'salad': 574,\n",
       " 'salmon': 575,\n",
       " 'salsa': 576,\n",
       " 'salt': 577,\n",
       " 'sandwich': 578,\n",
       " 'sashimi': 579,\n",
       " 'sat': 580,\n",
       " 'satisfi': 581,\n",
       " 'sauc': 582,\n",
       " 'say': 583,\n",
       " 'scallop': 584,\n",
       " 'seafood': 585,\n",
       " 'season': 586,\n",
       " 'seat': 587,\n",
       " 'second': 588,\n",
       " 'see': 589,\n",
       " 'seem': 590,\n",
       " 'seen': 591,\n",
       " 'select': 592,\n",
       " 'serious': 593,\n",
       " 'serv': 594,\n",
       " 'server': 595,\n",
       " 'servic': 596,\n",
       " 'set': 597,\n",
       " 'sever': 598,\n",
       " 'shop': 599,\n",
       " 'show': 600,\n",
       " 'shower': 601,\n",
       " 'shrimp': 602,\n",
       " 'sick': 603,\n",
       " 'side': 604,\n",
       " 'sign': 605,\n",
       " 'similar': 606,\n",
       " 'similarli': 607,\n",
       " 'simpl': 608,\n",
       " 'simpli': 609,\n",
       " 'sinc': 610,\n",
       " 'singl': 611,\n",
       " 'sit': 612,\n",
       " 'six': 613,\n",
       " 'skimp': 614,\n",
       " 'slaw': 615,\n",
       " 'slice': 616,\n",
       " 'slow': 617,\n",
       " 'small': 618,\n",
       " 'smaller': 619,\n",
       " 'smashburg': 620,\n",
       " 'smear': 621,\n",
       " 'smell': 622,\n",
       " 'smoke': 623,\n",
       " 'smooth': 624,\n",
       " 'smoothi': 625,\n",
       " 'soggi': 626,\n",
       " 'soi': 627,\n",
       " 'solid': 628,\n",
       " 'solidifi': 629,\n",
       " 'somehow': 630,\n",
       " 'someon': 631,\n",
       " 'someth': 632,\n",
       " 'somethat': 633,\n",
       " 'somewhat': 634,\n",
       " 'son': 635,\n",
       " 'song': 636,\n",
       " 'soon': 637,\n",
       " 'soooo': 638,\n",
       " 'sooooo': 639,\n",
       " 'soooooo': 640,\n",
       " 'sore': 641,\n",
       " 'sorri': 642,\n",
       " 'sound': 643,\n",
       " 'soundtrack': 644,\n",
       " 'soup': 645,\n",
       " 'sour': 646,\n",
       " 'southwest': 647,\n",
       " 'space': 648,\n",
       " 'spaghetti': 649,\n",
       " 'special': 650,\n",
       " 'speedi': 651,\n",
       " 'spend': 652,\n",
       " 'spice': 653,\n",
       " 'spici': 654,\n",
       " 'spicier': 655,\n",
       " 'spinach': 656,\n",
       " 'sport': 657,\n",
       " 'spot': 658,\n",
       " 'spotti': 659,\n",
       " 'spring': 660,\n",
       " 'sprout': 661,\n",
       " 'staff': 662,\n",
       " 'stale': 663,\n",
       " 'standard': 664,\n",
       " 'star': 665,\n",
       " 'start': 666,\n",
       " 'starv': 667,\n",
       " 'station': 668,\n",
       " 'stay': 669,\n",
       " 'steak': 670,\n",
       " 'steakhous': 671,\n",
       " 'steiner': 672,\n",
       " 'step': 673,\n",
       " 'steve': 674,\n",
       " 'stick': 675,\n",
       " 'still': 676,\n",
       " 'stink': 677,\n",
       " 'stir': 678,\n",
       " 'stomach': 679,\n",
       " 'stood': 680,\n",
       " 'stop': 681,\n",
       " 'store': 682,\n",
       " 'strang': 683,\n",
       " 'stranger': 684,\n",
       " 'strawberri': 685,\n",
       " 'street': 686,\n",
       " 'stretch': 687,\n",
       " 'strip': 688,\n",
       " 'stuf': 689,\n",
       " 'stuff': 690,\n",
       " 'style': 691,\n",
       " 'sub': 692,\n",
       " 'subway': 693,\n",
       " 'suck': 694,\n",
       " 'sugari': 695,\n",
       " 'suggest': 696,\n",
       " 'summer': 697,\n",
       " 'super': 698,\n",
       " 'sure': 699,\n",
       " 'surpris': 700,\n",
       " 'sushi': 701,\n",
       " 'sweet': 702,\n",
       " 'tabl': 703,\n",
       " 'taco': 704,\n",
       " 'take': 705,\n",
       " 'talk': 706,\n",
       " 'tap': 707,\n",
       " 'tapa': 708,\n",
       " 'tartar': 709,\n",
       " 'tast': 710,\n",
       " 'tasteless': 711,\n",
       " 'tasti': 712,\n",
       " 'tea': 713,\n",
       " 'tell': 714,\n",
       " 'ten': 715,\n",
       " 'tender': 716,\n",
       " 'terribl': 717,\n",
       " 'textur': 718,\n",
       " 'thai': 719,\n",
       " 'thin': 720,\n",
       " 'thing': 721,\n",
       " 'think': 722,\n",
       " 'third': 723,\n",
       " 'though': 724,\n",
       " 'thought': 725,\n",
       " 'three': 726,\n",
       " 'thumb': 727,\n",
       " 'time': 728,\n",
       " 'tip': 729,\n",
       " 'toast': 730,\n",
       " 'today': 731,\n",
       " 'told': 732,\n",
       " 'took': 733,\n",
       " 'top': 734,\n",
       " 'tot': 735,\n",
       " 'total': 736,\n",
       " 'touch': 737,\n",
       " 'toward': 738,\n",
       " 'town': 739,\n",
       " 'treat': 740,\n",
       " 'tri': 741,\n",
       " 'tribut': 742,\n",
       " 'trip': 743,\n",
       " 'tuna': 744,\n",
       " 'twice': 745,\n",
       " 'two': 746,\n",
       " 'unbeliev': 747,\n",
       " 'undercook': 748,\n",
       " 'underwhelm': 749,\n",
       " 'unfortun': 750,\n",
       " 'unless': 751,\n",
       " 'us': 752,\n",
       " 'use': 753,\n",
       " 'valley': 754,\n",
       " 'valu': 755,\n",
       " 'vega': 756,\n",
       " 'veget': 757,\n",
       " 'vegetarian': 758,\n",
       " 'ventur': 759,\n",
       " 'vibe': 760,\n",
       " 'vinegrett': 761,\n",
       " 'visit': 762,\n",
       " 'wait': 763,\n",
       " 'waiter': 764,\n",
       " 'waitress': 765,\n",
       " 'walk': 766,\n",
       " 'wall': 767,\n",
       " 'want': 768,\n",
       " 'warm': 769,\n",
       " 'wast': 770,\n",
       " 'watch': 771,\n",
       " 'water': 772,\n",
       " 'way': 773,\n",
       " 'week': 774,\n",
       " 'well': 775,\n",
       " 'went': 776,\n",
       " 'white': 777,\n",
       " 'whole': 778,\n",
       " 'wife': 779,\n",
       " 'wine': 780,\n",
       " 'wing': 781,\n",
       " 'without': 782,\n",
       " 'wonder': 783,\n",
       " 'word': 784,\n",
       " 'work': 785,\n",
       " 'worker': 786,\n",
       " 'world': 787,\n",
       " 'wors': 788,\n",
       " 'worst': 789,\n",
       " 'worth': 790,\n",
       " 'would': 791,\n",
       " 'wow': 792,\n",
       " 'wrap': 793,\n",
       " 'wrong': 794,\n",
       " 'year': 795,\n",
       " 'yet': 796,\n",
       " 'yum': 797,\n",
       " 'yummi': 798,\n",
       " 'zero': 799}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_feat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:11.701687Z",
     "start_time": "2020-04-01T11:32:11.669686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.94470429e-02, -5.61094221e-02, -5.62442838e-02,  3.22852055e-01,\n",
       "       -2.61735079e-02, -2.51271324e-01,  4.54712392e-01, -2.92879665e-01,\n",
       "        4.33586657e-01,  1.76047785e+00,  5.32576886e-01,  4.95837017e-01,\n",
       "        1.93805910e-01, -3.81831961e-01, -6.74344696e-02,  5.49583345e-01,\n",
       "       -7.17251921e-01, -2.63598823e-01, -9.49874146e-02,  3.07743962e-01,\n",
       "        3.87902303e-02, -6.79677621e-02, -6.72201541e-01, -3.41166383e-02,\n",
       "       -2.04446018e-01,  5.17157985e-01, -2.71785282e-02,  4.51705987e-01,\n",
       "       -2.70901648e-02, -4.30586463e-01, -4.00190385e-01,  3.41270485e-01,\n",
       "       -3.06341504e-01,  0.00000000e+00,  3.31668124e-01, -7.54949995e-01,\n",
       "       -2.02449073e-01, -9.14649206e-01, -5.96800997e-01, -3.95487379e-01,\n",
       "        1.38342080e+00,  0.00000000e+00, -1.37700508e-02,  4.48970369e-01,\n",
       "       -2.86663537e-01,  9.76214414e-01, -1.41038345e+00,  2.53709855e-02,\n",
       "       -1.97457784e-01,  4.61244558e-01, -3.91924840e-01,  1.89494057e-01,\n",
       "       -3.14817562e-01,  0.00000000e+00, -1.66232208e-01, -2.29605291e-01,\n",
       "       -2.94567894e-01,  1.94925479e-01,  5.37800572e-02,  6.36705754e-01,\n",
       "       -2.90468089e-02, -4.31343603e-02,  5.51209938e-01, -1.72246594e-01,\n",
       "       -2.71494862e-01,  2.82509161e-01,  8.11387380e-01, -4.80727119e-01,\n",
       "        2.35158587e-01, -1.64179321e-01,  7.62259483e-02, -1.65051262e-01,\n",
       "       -1.08300664e-01, -7.89571790e-01,  8.66149847e-02,  3.20868116e-01,\n",
       "       -1.08450268e+00,  0.00000000e+00, -4.47808672e-01,  1.39747477e-02,\n",
       "       -3.18070175e-01,  2.10040942e-01,  4.77377553e-02,  2.06257968e-01,\n",
       "        2.61959599e-01,  5.90045094e-01, -2.89955066e-01,  7.15819252e-01,\n",
       "        2.41814723e-01,  4.45253406e-03, -2.11394995e-02,  1.15786544e-01,\n",
       "        1.50069572e-01,  7.60764306e-01, -1.18133642e-01, -1.06260991e-01,\n",
       "        1.98175735e-02, -5.22152876e-01, -9.16019259e-01,  2.47375731e-01,\n",
       "       -4.15280192e-01, -6.55950132e-03,  5.38354891e-02, -3.44414767e-01,\n",
       "       -1.48494634e-02, -3.15160854e-02, -2.04873074e-01,  2.02513371e-01,\n",
       "       -5.61094221e-02,  9.78738013e-02,  7.98377546e-02, -3.95774651e-01,\n",
       "       -9.65599510e-02, -2.16103893e-01, -2.04303776e-01,  8.39595877e-01,\n",
       "       -2.12563203e-03, -7.43750181e-02,  6.97094216e-01, -3.50834120e-01,\n",
       "        1.92282764e-01,  3.07727783e-01, -4.01289202e-01,  0.00000000e+00,\n",
       "        1.66894359e-01,  1.61983685e-01,  2.13524860e-01, -2.95430304e-01,\n",
       "       -2.57341308e-01, -7.66060049e-02,  3.51809816e-01, -8.36672914e-03,\n",
       "       -1.43577067e-01, -9.61559461e-02,  2.10560838e-01, -2.26168123e-01,\n",
       "       -5.16105112e-01, -6.83749222e-01,  1.53572338e-01, -1.65488523e-01,\n",
       "       -1.45433826e-01,  0.00000000e+00,  1.84785067e-01,  3.55686853e-01,\n",
       "        3.30629419e-01, -3.30196593e-01,  2.14879092e-01, -5.14073213e-01,\n",
       "        3.21692714e-01,  1.29433635e-01,  2.75408025e-01, -5.10371050e-02,\n",
       "        6.17272566e-01, -4.06633762e-02, -1.58050099e-01, -1.65626715e-01,\n",
       "        0.00000000e+00, -4.32980477e-01, -2.40015055e-01,  6.04498148e-02,\n",
       "        1.61983685e-01,  1.72602875e-01, -3.83570529e-01, -1.87656652e-01,\n",
       "        6.22159795e-02, -2.54307810e-01, -1.54913317e-01,  6.63055899e-01,\n",
       "        4.20747515e-02,  6.44080862e-01,  2.63330840e-01,  1.16138789e-01,\n",
       "       -3.67950307e-01, -2.21034454e-01,  2.37705098e-01,  4.84158453e-01,\n",
       "        0.00000000e+00,  7.86010384e-01,  5.54175575e-01,  0.00000000e+00,\n",
       "       -1.37994663e-01,  0.00000000e+00,  2.10735504e+00,  3.46880107e-01,\n",
       "        6.85751865e-01,  2.51517041e-01,  0.00000000e+00, -9.49874146e-02,\n",
       "       -4.33018553e-01,  0.00000000e+00,  0.00000000e+00, -3.42967190e-01,\n",
       "       -2.52308904e-01, -1.22959200e-01, -4.39446798e-02,  5.98135517e-01,\n",
       "       -9.25132366e-02,  4.78883379e-01,  7.37989293e-01,  1.99407450e-01,\n",
       "       -1.31302460e-01, -2.75215517e-01,  1.43918011e-02,  0.00000000e+00,\n",
       "       -4.26599532e-01, -8.76552352e-01, -2.23309709e-01, -8.87704346e-01,\n",
       "       -1.79371167e-01, -1.13201623e-01, -1.17455744e-01, -3.44433462e-01,\n",
       "       -6.11752242e-01,  2.01834141e-01, -2.32411058e-01, -2.69175679e-01,\n",
       "       -7.82131892e-01, -3.87228694e-01, -1.13667014e-01, -7.43750181e-02,\n",
       "       -2.86723485e-01, -2.19031391e-01,  1.41074090e-01, -2.57359212e-01,\n",
       "       -4.47871480e-01, -2.52964812e-01,  1.35464189e-02,  4.08415598e-01,\n",
       "        1.69692747e-01, -8.28524739e-01,  2.25926715e-01, -2.87238278e-01,\n",
       "       -2.29889087e-02,  3.42797617e-01, -7.46794312e-01, -3.49513249e-01,\n",
       "       -5.14952750e-01, -3.88030190e-01, -4.54356841e-01, -4.68092415e-02,\n",
       "        9.64409284e-01,  2.02114995e-01,  1.44114402e-01,  9.88428082e-02,\n",
       "       -2.83859306e-02, -7.75450826e-01,  2.11800168e-02,  5.99574007e-02,\n",
       "       -2.54023153e-01,  6.75227537e-01,  6.75166316e-02,  4.44938749e-01,\n",
       "        9.22238134e-01, -4.70302715e-01, -1.14312240e-01, -1.03756875e-01,\n",
       "        6.53728334e-02,  2.83076511e-01, -6.98313682e-01, -6.67360472e-02,\n",
       "        2.37443129e-01, -5.22818078e-01, -2.76415075e-01,  1.66440549e-01,\n",
       "        5.55811885e-02,  1.42088448e-01,  1.53923268e+00,  2.11689507e-01,\n",
       "       -3.65000099e-01,  2.56860639e-01, -5.63700605e-01,  7.41616480e-01,\n",
       "       -1.61190329e-01, -7.47604229e-05, -8.52409261e-02, -1.08300664e-01,\n",
       "        6.14392317e-02, -1.66489238e-01, -3.55006512e-01,  1.73853636e-01,\n",
       "        1.14818103e-02,  8.49170302e-01, -2.68269104e-02, -3.69988570e-01,\n",
       "        3.70075820e-02, -7.34756444e-01, -4.57745004e-01, -3.15160854e-02,\n",
       "       -2.16014425e-01,  4.85525977e-01, -2.04973694e-01, -2.53126963e-01,\n",
       "        3.50995105e-01, -2.84002565e-01,  9.24323976e-02,  1.57268611e+00,\n",
       "        0.00000000e+00, -2.05058021e-01,  5.50177434e-01,  7.41105478e-01,\n",
       "        1.13517169e-02,  7.72125408e-02,  2.45073138e-01, -5.20653504e-02,\n",
       "       -2.66329282e-01, -1.94989063e-01,  4.23784422e-01, -8.53556561e-02,\n",
       "        1.34588580e-01,  4.08275916e-01,  1.38945123e+00, -7.47339689e-01,\n",
       "       -2.78585863e-01,  2.81200774e+00,  3.25037776e-01, -2.22876954e-01,\n",
       "        0.00000000e+00,  4.97332682e-02, -8.68918858e-01,  3.51809816e-01,\n",
       "       -3.22378171e-01,  1.66682058e-01, -6.41874681e-02, -1.63594356e-01,\n",
       "       -1.54280397e-01, -6.40919700e-02,  7.06067973e-01,  4.57365831e-01,\n",
       "        0.00000000e+00,  1.36536241e+00, -9.01078632e-01, -5.36256763e-01,\n",
       "       -5.58272268e-01,  4.44366963e-01,  2.38085356e-02, -1.23547853e-01,\n",
       "        1.84013547e-01, -1.63091094e-01,  5.37354902e-02,  3.07942820e-01,\n",
       "        2.14398826e-01, -2.38504871e-01,  1.00082108e-01, -1.66388730e-02,\n",
       "        3.61256011e-01, -1.12424903e-01,  3.14561022e-01,  2.04289056e-02,\n",
       "       -5.08179645e-01,  2.07274897e-02, -5.64219890e-01,  5.74942689e-03,\n",
       "       -4.77627222e-01,  4.59960168e-02, -1.16703291e-01,  3.00882867e-01,\n",
       "        2.64407627e-01,  8.43120840e-01, -2.69947973e-01, -6.80183376e-01,\n",
       "        2.33943282e-01,  2.37502810e-01, -1.31190685e+00,  4.70017654e-01,\n",
       "        4.41173734e-01, -7.39023981e-02,  5.29137897e-01,  8.71226862e-01,\n",
       "       -6.11728680e-01,  4.22662975e-01,  2.75065255e-01, -7.67888803e-02,\n",
       "        3.03652622e-01,  0.00000000e+00, -2.27232870e-01, -4.23995149e-01,\n",
       "       -3.52129474e-01,  6.08689930e-02, -4.76782338e-01, -1.27388034e-01,\n",
       "       -6.51199665e-01, -2.30381358e-02, -1.90710624e-01, -1.18207092e-01,\n",
       "       -3.61412982e-02,  2.34473475e-01,  6.37332579e-02, -3.78669739e-01,\n",
       "        4.42493925e-02, -4.32980477e-01,  3.64984551e-01, -9.01583019e-01,\n",
       "        3.57012442e-01,  3.74167818e-01, -1.81969796e-01,  3.33856825e-02,\n",
       "       -2.88026554e-01,  2.20239874e-02, -6.97143983e-01,  1.34115581e-01,\n",
       "       -7.63212237e-01, -6.35793238e-01, -3.28590533e-01, -1.05056337e+00,\n",
       "       -6.25967516e-01, -1.38057710e-01,  1.64966439e+00, -5.79713680e-03,\n",
       "        1.10569026e-01, -1.28107928e-01, -1.63837802e-01, -2.62007971e-01,\n",
       "       -3.90822325e-01,  1.61602984e-01, -1.00603786e-01, -6.23824615e-01,\n",
       "        3.25162796e-03,  3.86230537e-02, -3.28071479e-01,  1.62544273e-01,\n",
       "        2.09329023e-01, -3.65000099e-01,  1.41074090e-01, -1.27245251e-01,\n",
       "       -2.06535670e-01, -6.04679272e-01, -8.58298183e-01, -4.73253086e-01,\n",
       "        2.48049820e-01,  5.62967179e-01,  1.98175735e-02, -4.46183915e-01,\n",
       "       -4.47182834e-01, -1.31027925e-01, -1.29348661e+00,  5.09223211e-01,\n",
       "       -3.44433462e-01,  4.23898807e-01,  3.50341201e-01, -8.86138198e-02,\n",
       "       -9.53540498e-01,  2.96259846e-01,  5.13144646e-01, -1.14377625e+00,\n",
       "        6.75186153e-02, -8.53900804e-02,  3.37209777e-01, -4.22887053e-01,\n",
       "       -5.88377469e-02, -6.58250134e-01, -3.61770403e-01, -1.74780597e-01,\n",
       "        1.26360664e-01, -8.05104631e-01,  2.14264530e-01,  2.64029112e-02,\n",
       "        1.33525733e+00,  6.73526403e-01,  1.44019101e-01,  5.54878070e-02,\n",
       "       -2.09264047e-01, -2.08181642e-01, -1.11624438e+00, -1.29858010e-01,\n",
       "       -1.38501675e-01, -5.23314202e-01, -7.19054551e-01,  7.89969287e-01,\n",
       "       -5.00838240e-01,  2.67526497e-03,  3.98626418e-01,  6.23736350e-01,\n",
       "       -3.29738472e-01, -5.73544906e-01,  8.10114110e-01,  2.41814723e-01,\n",
       "        1.72356584e-01, -2.06128473e-01, -9.03314115e-01,  9.25841552e-02,\n",
       "        4.46244429e-01,  2.15966923e-01, -6.41805058e-02, -5.07811669e-01,\n",
       "       -2.03616213e-01, -9.11495473e-02, -5.93219214e-01, -1.15288789e-01,\n",
       "        4.55967581e-01, -1.61306539e-01,  2.34972102e-01,  5.60763225e-01,\n",
       "       -3.56275062e-01, -2.17898704e-01, -4.61747851e-01, -1.08300664e-01,\n",
       "        1.37391088e+00,  4.89682965e-01,  4.69829768e-02, -1.23296011e-01,\n",
       "        5.97121823e-01,  1.88804888e-01, -6.53537191e-02,  1.80771489e-01,\n",
       "        2.64960222e-02,  4.35739766e-01, -7.09328227e-02, -3.11685149e-03,\n",
       "        3.52090148e-03, -1.01456295e-01,  5.92655336e-01,  3.32875803e-01,\n",
       "       -1.12003842e+00, -1.12738883e-01,  1.59438087e-01,  1.07437849e-02,\n",
       "       -1.46598342e-01,  7.33496765e-02, -7.24957206e-03,  1.62677145e-02,\n",
       "        6.41451290e-02,  2.05912156e-01, -1.06834130e+00,  1.79618102e-01,\n",
       "        6.34681116e-01,  2.46793977e-01, -1.80707578e-01,  3.66531672e-01,\n",
       "        1.13154229e-01,  2.93638157e-01,  2.98368408e-01, -2.80918317e-01,\n",
       "        2.81669361e-01, -3.19148161e-01, -6.51442386e-01, -1.17323078e-01,\n",
       "       -1.65849180e-01, -3.41166383e-02, -3.84080240e-01,  3.29481109e-01,\n",
       "        3.58907409e-01,  6.09143795e-01,  8.57981543e-02, -1.19163802e-01,\n",
       "        6.63001206e-01, -8.46423056e-02, -7.05393117e-02,  3.85920495e-01,\n",
       "        1.23879600e-02,  1.00338228e-01,  1.28959825e-01, -5.18728878e-01,\n",
       "       -1.52341404e-01, -4.66261544e-01,  3.78041146e-01,  5.88461164e-02,\n",
       "       -3.56431808e-01,  2.24090377e-01,  3.77671777e-02, -1.31027925e-01,\n",
       "       -3.19575993e-01, -1.08300664e-01, -1.64479488e-01, -2.39869364e-02,\n",
       "        1.64374591e-01,  3.21366460e-01,  1.58019064e-01,  1.47250516e-01,\n",
       "        1.05556014e-01, -2.78264921e-01, -1.17300565e+00,  8.31925108e-02,\n",
       "       -7.18412079e-01, -2.30366245e-01,  2.93905102e-01,  1.68218085e-01,\n",
       "       -1.56998457e-01, -1.65332834e-01,  3.27116768e-01, -3.72628030e-01,\n",
       "       -1.27740126e-01,  6.62049141e-01,  4.11353753e-01, -1.28634882e-01,\n",
       "       -2.11799778e-01, -9.81394040e-03,  4.33085756e-01,  1.88621382e-01,\n",
       "        4.89740488e-01, -2.72097015e-01, -2.99590996e-01, -5.03418890e-01,\n",
       "        7.28347460e-01,  6.36314434e-01, -1.46274145e-01, -1.63432930e-01,\n",
       "       -1.79617422e-02, -3.52028939e-01, -9.51643069e-02,  6.28702654e-02,\n",
       "       -5.90668208e-02, -2.62055850e-01, -1.33579767e-01, -8.34253330e-01,\n",
       "        3.95274249e-01, -6.99799624e-01,  1.96359961e-01, -9.49874146e-02,\n",
       "        3.53932378e-01, -3.87928948e-01,  3.57074252e-01, -2.30932783e-01,\n",
       "        9.47641477e-02, -2.70200080e-01,  4.12273957e-01,  1.41074090e-01,\n",
       "        1.06388197e-01, -1.14632668e+00, -4.14539241e-01, -1.99588304e-01,\n",
       "       -1.12738883e-01, -1.49819313e-01, -2.47018373e-01, -9.25132366e-02,\n",
       "        1.61983685e-01,  7.77301412e-02, -7.45102698e-01,  3.13895397e-01,\n",
       "        2.18949806e-01, -9.25132366e-02, -6.54788568e-02, -3.43696910e-01,\n",
       "       -1.04778609e-01,  2.64646323e-01,  0.00000000e+00,  6.39480204e-02,\n",
       "        9.61443956e-03,  1.39805129e-01,  2.00360240e-01,  2.13524860e-01,\n",
       "       -2.04303776e-01, -4.38439626e-01, -2.07377253e-01, -1.22996040e-02,\n",
       "        1.31512785e-01,  1.27024821e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -2.35903098e-01,  4.76081886e-01,  1.34123449e-01,\n",
       "       -4.79902758e-01,  9.29081823e-02,  6.33135063e-01,  2.56793174e-01,\n",
       "       -2.02449073e-01,  5.99574007e-02,  6.97608215e-01, -3.41118114e-01,\n",
       "        1.08789876e-01, -1.08300664e-01,  2.77443634e-01, -7.62198706e-01,\n",
       "       -5.52926048e-02,  3.73007673e-01, -3.42440868e-01,  0.00000000e+00,\n",
       "        7.99135524e-02,  3.17731083e-02,  2.15021732e-01, -3.44414767e-01,\n",
       "        3.47821016e-01, -6.16539778e-01,  5.88461164e-02,  5.62448571e-01,\n",
       "       -1.97230976e-01, -3.41118114e-01,  5.91060200e-02, -4.99401096e-01,\n",
       "       -1.45195797e-01,  4.67905810e-01,  0.00000000e+00, -3.09858970e-01,\n",
       "       -1.54280397e-01,  2.45123516e-01,  0.00000000e+00, -4.82019593e-01,\n",
       "        3.92061012e-01,  3.51156189e-01,  3.23183175e-01, -5.05924672e-01,\n",
       "       -2.16103893e-01, -1.86139605e-01, -8.44902694e-01, -2.74148539e-01,\n",
       "       -2.20127451e-02,  3.43736124e-01, -1.24929195e-02, -5.33677920e-01,\n",
       "        1.33167076e-01, -4.41878280e-01,  7.21440153e-01, -5.07090506e-01,\n",
       "        2.60605471e-01, -2.79605512e-01, -5.04886615e-01,  1.58019064e-01,\n",
       "        1.17972908e-01,  4.35551344e-01, -1.68037639e-02, -7.21487271e-01,\n",
       "        3.75431649e-01, -2.10883375e-01, -5.74523895e-02, -9.59499265e-02,\n",
       "        7.55704435e-01, -1.17969165e+00, -2.56798738e-01,  5.87433728e-01,\n",
       "        1.58822369e-01, -4.31442344e-01, -8.19563780e-01, -2.56908993e-01,\n",
       "        1.85567773e-01,  2.89648940e-01, -1.41865645e-01,  1.43038936e-01,\n",
       "       -1.40808366e-01, -5.77166426e-02,  1.13975922e-01,  3.84451854e-01,\n",
       "        1.99277492e-01, -3.89953439e-01,  5.44340483e-02,  3.77671777e-02,\n",
       "       -5.17043388e-01,  4.57061266e-01, -2.55530635e-01,  7.36206961e-01,\n",
       "        4.48568025e-01,  4.08534310e-01,  0.00000000e+00, -2.47864806e-01,\n",
       "       -1.33941196e-01,  2.72658477e-01, -4.91549655e-01,  4.59903046e-01,\n",
       "       -1.99071593e-01, -2.94081527e-01, -9.82764246e-01, -3.37123687e-01,\n",
       "        1.48866838e-01, -6.01014970e-01, -7.43137987e-01, -4.35614237e-01,\n",
       "        4.91613222e-01, -2.81158582e-01, -1.43324622e-01,  3.09787718e-01,\n",
       "        0.00000000e+00,  1.84548212e-01,  3.75061496e-01, -5.55341530e-01,\n",
       "       -3.69872304e-01,  1.40854692e-01, -2.81399669e-01,  8.90579905e-04,\n",
       "       -1.06391270e-01,  4.28801918e-01, -7.77514197e-01, -1.24465185e-01,\n",
       "       -7.05393117e-02, -8.94634886e-01,  3.58527377e-01,  2.13448175e-01,\n",
       "        4.08336448e-01,  6.87681214e-01, -7.68221240e-02, -2.31674918e-01,\n",
       "       -4.20735193e-01,  2.31723742e-01, -1.54325229e-01,  8.20606867e-01,\n",
       "       -7.40559755e-03, -9.45127504e-02, -4.65217868e-01,  1.20111378e-01,\n",
       "       -2.33366838e-01, -1.32629051e+00, -8.55777760e-02, -1.03629647e+00,\n",
       "        5.48481263e-01, -7.79463994e-02, -3.41823506e-03, -1.39375131e-01,\n",
       "       -9.22923540e-03,  0.00000000e+00,  3.32590026e-01, -7.58889882e-01])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_coeffs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:16.524518Z",
     "start_time": "2020-04-01T11:32:16.514516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Contextual Sentiments for Count Vectors\n",
    "sentiList_cv = []\n",
    "threshold = 0.5\n",
    "for word, index in vocab_cv.items():\n",
    "    weight = cv_coeffs[0][cv_feat_dict[word]]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        sentiList_cv.append((word, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:19.873366Z",
     "start_time": "2020-04-01T11:32:19.865371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiList_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:20.752586Z",
     "start_time": "2020-04-01T11:32:20.728600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wow', 0.548481263346505),\n",
       " ('love', 1.649664391934063),\n",
       " ('good', 1.389451232572293),\n",
       " ('nasti', -0.6582501337984253),\n",
       " ('recommend', 0.6630012059207112),\n",
       " ('select', 0.7283474600937245),\n",
       " ('menu', 0.5629671793238409),\n",
       " ('great', 2.8120077400425836),\n",
       " ('could', 0.6172725656004414),\n",
       " ('would', -1.0362964684053497),\n",
       " ('overpr', -0.9033141145344573),\n",
       " ('disgust', -0.611752241961287),\n",
       " ('sure', -0.5336779203406519),\n",
       " ('sign', -0.6997996242775838),\n",
       " ('slow', -1.146326677015529),\n",
       " ('let', -0.9015830186791481),\n",
       " ('amaz', 1.7604778455393633),\n",
       " ('cute', 0.6630558992412364),\n",
       " ('beauti', 0.6367057539222921),\n",
       " ('never', -0.8051046309918073),\n",
       " ('friendli', 1.5726861147950468),\n",
       " ('hour', -0.5642198904695445),\n",
       " ('tabl', -0.507090505638842),\n",
       " ('total', -0.5170433880784036),\n",
       " ('worst', -1.3262905138653105),\n",
       " ('burger', -0.5221528762545875),\n",
       " ('beer', 0.5512099375985268),\n",
       " ('favor', -0.563700604722426),\n",
       " ('look', -1.0505633664267473),\n",
       " ('elsewher', -0.51495275046101),\n",
       " ('inexpens', 0.5291378965008193),\n",
       " ('poor', -1.1200384216214683),\n",
       " ('everi', 0.6752275369118111),\n",
       " ('first', 0.8491703015655228),\n",
       " ('delight', 0.6857518645164072),\n",
       " ('suck', -0.8449026943861071),\n",
       " ('tender', 0.7557044351357599),\n",
       " ('establish', -0.7754508256279885),\n",
       " ('hard', -0.9010786316663092),\n",
       " ('gross', -0.8689188583037026),\n",
       " ('eat', -0.8285247388275679),\n",
       " ('sick', -0.8342533302058389),\n",
       " ('dessert', 0.5981355168508629),\n",
       " ('bad', -1.4103834478000976),\n",
       " ('order', 0.6237363500834028),\n",
       " ('insid', 0.8712268623969667),\n",
       " ('nice', 1.3352573276235609),\n",
       " ('outsid', -0.5735449057529012),\n",
       " ('horribl', -0.5081796445165457),\n",
       " ('talk', -0.5048866150292745),\n",
       " ('one', -0.5008382397869113),\n",
       " ('enjoy', 0.9644092838841738),\n",
       " ('wonder', 0.8206068667261117),\n",
       " ('imagin', -0.6801833761371541),\n",
       " ('much', -1.1437762462939598),\n",
       " ('tasteless', -0.7214872711816072),\n",
       " ('think', -0.8195637798642625),\n",
       " ('minut', -1.2934866069250799),\n",
       " ('delici', 2.1073550414438036),\n",
       " ('definit', 0.5541755752218165),\n",
       " ('got', -0.747339688558777),\n",
       " ('way', -0.8946348860216906),\n",
       " ('noth', -1.1162443824790924),\n",
       " ('use', -0.6010149704329206),\n",
       " ('sweet', 0.7214401525254043),\n",
       " ('buffet', 0.7607643055396133),\n",
       " ('wast', -0.777514196704573),\n",
       " ('wait', -0.5553415295589715),\n",
       " ('old', -0.7190545508812399),\n",
       " ('bland', -1.084502679607853),\n",
       " ('meat', -0.6046792723809549),\n",
       " ('die', 0.7379892926394366),\n",
       " ('disappoint', -0.8877043464692417),\n",
       " ('mouth', 0.5131446456101993),\n",
       " ('step', -0.6165397780159664),\n",
       " ('best', 0.8113873797052392),\n",
       " ('breakfast', 0.7158192524600736),\n",
       " ('arriv', -0.672201541297628),\n",
       " ('fantast', 1.5392326840699282),\n",
       " ('town', 0.7362069605212852),\n",
       " ('busi', -0.9160192590561677),\n",
       " ('thai', 0.5874337281962217),\n",
       " ('spici', 0.6331350627957113),\n",
       " ('check', 0.8395958765530168),\n",
       " ('bit', -0.7895717898804002),\n",
       " ('manag', -0.6238246148319384),\n",
       " ('spot', 0.6976082146312985),\n",
       " ('lack', -0.6511996645349756),\n",
       " ('reason', 0.6091437954653951),\n",
       " ('ambianc', 0.5325768858179709),\n",
       " ('return', -0.5187288783874507),\n",
       " ('mediocr', -0.8582981828404044),\n",
       " ('excel', 0.9222381335023652),\n",
       " ('rude', -1.173005652387936),\n",
       " ('serious', 0.6363144335118451),\n",
       " ('extrem', -0.6983136818855228),\n",
       " ('done', -0.7821318915848962),\n",
       " ('stale', -0.7621987057302408),\n",
       " ('unfortun', -0.9827642464181044),\n",
       " ('impress', -1.311906852150685),\n",
       " ('avoid', -0.91464920609221),\n",
       " ('full', 0.5501774338096521),\n",
       " ('hand', 0.706067973276935),\n",
       " ('phoenix', 0.5971218227212667),\n",
       " ('bacon', 0.9762144135203871),\n",
       " ('sad', -0.7184120792034291),\n",
       " ('zero', -0.7588898819797181),\n",
       " ('miss', 0.5092232111641718),\n",
       " ('probabl', -1.0683413011858678),\n",
       " ('ice', 0.843120839889292),\n",
       " ('bread', 0.5900450939891896),\n",
       " ('satisfi', 0.6620491407657383),\n",
       " ('valley', -0.7431379868399768),\n",
       " ('live', -0.6971439830886496),\n",
       " ('insult', -0.6117286795611075),\n",
       " ('white', 0.6876812136818989),\n",
       " ('soggi', -0.7451026975328987),\n",
       " ('either', -0.7467943118366571),\n",
       " ('cold', -0.5161051121259072),\n",
       " ('fun', 0.7411054777638385),\n",
       " ('chef', 0.6970942158717122),\n",
       " ('money', -0.9535404976316986),\n",
       " ('dark', 0.6440808624786233),\n",
       " ('par', -0.5932192136584543),\n",
       " ('stick', 0.5624485710664926),\n",
       " ('plu', 0.592655335701269),\n",
       " ('patio', 0.5607632246856153),\n",
       " ('outstand', 0.8101141101080757),\n",
       " ('hate', -0.5362567625567202),\n",
       " ('locat', -0.7632122373265139),\n",
       " ('rate', -0.651442386181577),\n",
       " ('happi', 1.365362414782125),\n",
       " ('favorit', 0.7416164804739621),\n",
       " ('seen', -0.503418890386014),\n",
       " ('dirti', -0.8765523522186827),\n",
       " ('omg', 0.7899692872219441),\n",
       " ('terribl', -1.1796916528431836),\n",
       " ('perfect', 1.373910884948678),\n",
       " ('assur', 0.5171579850994029),\n",
       " ('nicest', 0.6735264025324367),\n",
       " ('anytim', -0.717251920966943),\n",
       " ('contain', -0.5140732134738143),\n",
       " ('paid', -0.5078116689478508),\n",
       " ('long', -0.6357932379913037),\n",
       " ('awesom', 1.383420801398193),\n",
       " ('color', -0.6837492216818952),\n",
       " ('ok', -0.5233142021659314),\n",
       " ('promis', 0.6346811160701102),\n",
       " ('fail', -0.5228180781161551),\n",
       " ('averag', -0.7549499945529459),\n",
       " ('anyth', 0.5495833446590033),\n",
       " ('defin', 0.7860103839849966),\n",
       " ('style', -0.5059246720178183),\n",
       " ('flavorless', -0.7347564437536963),\n",
       " ('aw', -0.5968009967140865),\n",
       " ('head', -0.5582722679049327),\n",
       " ('lost', -0.6259675163994584)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiList_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:25.814996Z",
     "start_time": "2020-04-01T11:32:25.807995Z"
    }
   },
   "outputs": [],
   "source": [
    "# Contextual Sentiments for TfIdf Vectors\n",
    "sentiList_tf = []\n",
    "threshold = 0.5\n",
    "for word, index in vocab_tf.items():\n",
    "    weight = tf_coeffs[0][tf_feat_dict[word]]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        sentiList_tf.append((word, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:26.261999Z",
     "start_time": "2020-04-01T11:32:26.254020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiList_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:27.227702Z",
     "start_time": "2020-04-01T11:32:27.208707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 2.054451159709322),\n",
       " ('place', 0.7864700638202643),\n",
       " ('good', 1.8965369518517652),\n",
       " ('nasti', -0.5579547487663885),\n",
       " ('stop', 0.5286926756209503),\n",
       " ('select', 0.9315573667778193),\n",
       " ('menu', 0.7889585563194407),\n",
       " ('great', 3.436207553670508),\n",
       " ('like', -0.6035021577277421),\n",
       " ('would', -1.2835494793459397),\n",
       " ('back', -0.5881386703554209),\n",
       " ('overpr', -0.9257982403194037),\n",
       " ('tri', 0.5561829338287412),\n",
       " ('disgust', -0.6214273246093673),\n",
       " ('sign', -0.5519318142953481),\n",
       " ('slow', -1.005776474348022),\n",
       " ('let', -0.603473232805371),\n",
       " ('amaz', 1.927315738369261),\n",
       " ('also', 0.6014541240826266),\n",
       " ('cute', 0.5322550978097917),\n",
       " ('beauti', 0.5774274599582191),\n",
       " ('never', -1.0024956570759322),\n",
       " ('friendli', 1.6404496421140058),\n",
       " ('total', -0.5751667841425459),\n",
       " ('worst', -1.3421737499597404),\n",
       " ('beer', 0.684181788916334),\n",
       " ('look', -0.8372815786738633),\n",
       " ('poor', -1.0426791169241307),\n",
       " ('everi', 0.629601565875173),\n",
       " ('first', 0.9134931566392146),\n",
       " ('delight', 0.6605677904947117),\n",
       " ('suck', -0.9379012126272156),\n",
       " ('tender', 0.6421922637953134),\n",
       " ('establish', -0.6408826675427457),\n",
       " ('hard', -0.8400840831581625),\n",
       " ('gross', -0.8207674584302749),\n",
       " ('eat', -0.9905846223388641),\n",
       " ('sick', -0.7261610002103644),\n",
       " ('dessert', 0.5043641896206881),\n",
       " ('bad', -1.446202895720262),\n",
       " ('insid', 0.7630318554423737),\n",
       " ('nice', 1.397048777499805),\n",
       " ('enjoy', 0.817673661394851),\n",
       " ('wonder', 0.6999667412159595),\n",
       " ('imagin', -0.52802364444823),\n",
       " ('much', -1.0090836993646168),\n",
       " ('tasteless', -0.6704596185275594),\n",
       " ('think', -0.9349283888104122),\n",
       " ('minut', -1.2632772133567496),\n",
       " ('delici', 2.146279036094962),\n",
       " ('definit', 0.7709120695188194),\n",
       " ('alway', 0.5628709784274571),\n",
       " ('got', -0.854636319257141),\n",
       " ('way', -0.9065326328791171),\n",
       " ('realli', 0.5381918469003925),\n",
       " ('meh', -0.5211619209241968),\n",
       " ('noth', -0.9668571028752972),\n",
       " ('sweet', 0.5983839810307267),\n",
       " ('buffet', 0.6043857098102593),\n",
       " ('wast', -0.8281155260249832),\n",
       " ('wait', -0.8708406124465834),\n",
       " ('old', -0.5167807791168795),\n",
       " ('bland', -0.9745688306129486),\n",
       " ('meat', -0.583158724929808),\n",
       " ('die', 0.6143468036754863),\n",
       " ('everyth', 0.5669058874543909),\n",
       " ('disappoint', -1.0045407293525441),\n",
       " ('best', 0.9343079214061372),\n",
       " ('breakfast', 0.8066993618272551),\n",
       " ('arriv', -0.5053924439263807),\n",
       " ('fantast', 1.451434448775091),\n",
       " ('town', 0.8085192901278874),\n",
       " ('ambienc', 0.5141428150158305),\n",
       " ('busi', -0.8677591566237625),\n",
       " ('spici', 0.6517017779866825),\n",
       " ('check', 0.9124659921067275),\n",
       " ('bit', -0.7667701560103739),\n",
       " ('know', -0.5217573551713119),\n",
       " ('manag', -0.5755184541805458),\n",
       " ('spot', 0.7634673106206804),\n",
       " ('lack', -0.5954243262957133),\n",
       " ('ambianc', 0.6340309730180597),\n",
       " ('return', -0.6927523813332679),\n",
       " ('mediocr', -0.8848365572283696),\n",
       " ('excel', 0.910084533052653),\n",
       " ('rude', -0.97092708619224),\n",
       " ('serious', 0.6706713623351672),\n",
       " ('extrem', -0.5444897869975193),\n",
       " ('done', -0.6597708216492646),\n",
       " ('stale', -0.6231840061943685),\n",
       " ('unfortun', -0.8202354515537391),\n",
       " ('impress', -1.159286786895802),\n",
       " ('avoid', -0.7783448474080719),\n",
       " ('hand', 0.5509882741072133),\n",
       " ('bacon', 0.9235648870522722),\n",
       " ('sad', -0.6552877634653975),\n",
       " ('zero', -0.609713899729942),\n",
       " ('probabl', -1.111135414658114),\n",
       " ('ice', 0.7476624912463894),\n",
       " ('bread', 0.5405774938572934),\n",
       " ('satisfi', 0.5728805817859914),\n",
       " ('insult', -0.633182565677122),\n",
       " ('white', 0.5751309085602271),\n",
       " ('soggi', -0.6910230874712414),\n",
       " ('either', -0.7150917970092049),\n",
       " ('cold', -0.5109426852169475),\n",
       " ('fun', 0.680777524884634),\n",
       " ('chef', 0.6268183392964655),\n",
       " ('money', -0.9184589620446072),\n",
       " ('plu', 0.5009087829895359),\n",
       " ('outstand', 0.6835032856640358),\n",
       " ('locat', -0.5563393149123789),\n",
       " ('rate', -0.6961914802792666),\n",
       " ('happi', 1.275171984089799),\n",
       " ('favorit', 0.5885742928506177),\n",
       " ('dirti', -0.520601617495641),\n",
       " ('omg', 0.5318852572810603),\n",
       " ('terribl', -1.277748308891894),\n",
       " ('perfect', 1.3778847443327014),\n",
       " ('nicest', 0.5140178380349968),\n",
       " ('anytim', -0.5843765870755476),\n",
       " ('long', -0.6423256855354322),\n",
       " ('awesom', 1.391656676094768),\n",
       " ('averag', -0.5386281600282833),\n",
       " ('spend', -0.5081633077186781),\n",
       " ('defin', 0.5576471833941095),\n",
       " ('aw', -0.5724346985787369)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiList_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:33.937194Z",
     "start_time": "2020-04-01T11:32:33.924173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'place',\n",
       " 'good',\n",
       " 'nasti',\n",
       " 'stop',\n",
       " 'select',\n",
       " 'menu',\n",
       " 'great',\n",
       " 'like',\n",
       " 'would',\n",
       " 'back',\n",
       " 'overpr',\n",
       " 'tri',\n",
       " 'disgust',\n",
       " 'sign',\n",
       " 'slow',\n",
       " 'let',\n",
       " 'amaz',\n",
       " 'also',\n",
       " 'cute',\n",
       " 'beauti',\n",
       " 'never',\n",
       " 'friendli',\n",
       " 'total',\n",
       " 'worst',\n",
       " 'beer',\n",
       " 'look',\n",
       " 'poor',\n",
       " 'everi',\n",
       " 'first',\n",
       " 'delight',\n",
       " 'suck',\n",
       " 'tender',\n",
       " 'establish',\n",
       " 'hard',\n",
       " 'gross',\n",
       " 'eat',\n",
       " 'sick',\n",
       " 'dessert',\n",
       " 'bad',\n",
       " 'insid',\n",
       " 'nice',\n",
       " 'enjoy',\n",
       " 'wonder',\n",
       " 'imagin',\n",
       " 'much',\n",
       " 'tasteless',\n",
       " 'think',\n",
       " 'minut',\n",
       " 'delici',\n",
       " 'definit',\n",
       " 'alway',\n",
       " 'got',\n",
       " 'way',\n",
       " 'realli',\n",
       " 'meh',\n",
       " 'noth',\n",
       " 'sweet',\n",
       " 'buffet',\n",
       " 'wast',\n",
       " 'wait',\n",
       " 'old',\n",
       " 'bland',\n",
       " 'meat',\n",
       " 'die',\n",
       " 'everyth',\n",
       " 'disappoint',\n",
       " 'best',\n",
       " 'breakfast',\n",
       " 'arriv',\n",
       " 'fantast',\n",
       " 'town',\n",
       " 'ambienc',\n",
       " 'busi',\n",
       " 'spici',\n",
       " 'check',\n",
       " 'bit',\n",
       " 'know',\n",
       " 'manag',\n",
       " 'spot',\n",
       " 'lack',\n",
       " 'ambianc',\n",
       " 'return',\n",
       " 'mediocr',\n",
       " 'excel',\n",
       " 'rude',\n",
       " 'serious',\n",
       " 'extrem',\n",
       " 'done',\n",
       " 'stale',\n",
       " 'unfortun',\n",
       " 'impress',\n",
       " 'avoid',\n",
       " 'hand',\n",
       " 'bacon',\n",
       " 'sad',\n",
       " 'zero',\n",
       " 'probabl',\n",
       " 'ice',\n",
       " 'bread',\n",
       " 'satisfi',\n",
       " 'insult',\n",
       " 'white',\n",
       " 'soggi',\n",
       " 'either',\n",
       " 'cold',\n",
       " 'fun',\n",
       " 'chef',\n",
       " 'money',\n",
       " 'plu',\n",
       " 'outstand',\n",
       " 'locat',\n",
       " 'rate',\n",
       " 'happi',\n",
       " 'favorit',\n",
       " 'dirti',\n",
       " 'omg',\n",
       " 'terribl',\n",
       " 'perfect',\n",
       " 'nicest',\n",
       " 'anytim',\n",
       " 'long',\n",
       " 'awesom',\n",
       " 'averag',\n",
       " 'spend',\n",
       " 'defin',\n",
       " 'aw']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word[0] for word in sentiList_tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:32:38.784403Z",
     "start_time": "2020-04-01T11:32:38.773399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([word[0] for word in sentiList_tf]).intersection(set([word[0] for word in sentiList_cv])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:33:18.052196Z",
     "start_time": "2020-04-01T11:33:18.043194Z"
    }
   },
   "outputs": [],
   "source": [
    "polarized_dict = get_feat_dict(list(set([word[0] for word in sentiList_tf]).intersection(set([word[0] for word in sentiList_cv]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:33:27.967853Z",
     "start_time": "2020-04-01T11:33:27.960849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(polarized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:33:40.524369Z",
     "start_time": "2020-04-01T11:33:40.514367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meat': 0,\n",
       " 'happi': 1,\n",
       " 'anytim': 2,\n",
       " 'suck': 3,\n",
       " 'avoid': 4,\n",
       " 'locat': 5,\n",
       " 'busi': 6,\n",
       " 'breakfast': 7,\n",
       " 'nicest': 8,\n",
       " 'disgust': 9,\n",
       " 'amaz': 10,\n",
       " 'great': 11,\n",
       " 'ice': 12,\n",
       " 'either': 13,\n",
       " 'sick': 14,\n",
       " 'zero': 15,\n",
       " 'sign': 16,\n",
       " 'stale': 17,\n",
       " 'look': 18,\n",
       " 'buffet': 19,\n",
       " 'eat': 20,\n",
       " 'defin': 21,\n",
       " 'enjoy': 22,\n",
       " 'love': 23,\n",
       " 'select': 24,\n",
       " 'poor': 25,\n",
       " 'rude': 26,\n",
       " 'aw': 27,\n",
       " 'cute': 28,\n",
       " 'bad': 29,\n",
       " 'town': 30,\n",
       " 'ambianc': 31,\n",
       " 'good': 32,\n",
       " 'minut': 33,\n",
       " 'check': 34,\n",
       " 'omg': 35,\n",
       " 'wonder': 36,\n",
       " 'disappoint': 37,\n",
       " 'bit': 38,\n",
       " 'delight': 39,\n",
       " 'menu': 40,\n",
       " 'nasti': 41,\n",
       " 'much': 42,\n",
       " 'money': 43,\n",
       " 'noth': 44,\n",
       " 'averag': 45,\n",
       " 'extrem': 46,\n",
       " 'long': 47,\n",
       " 'got': 48,\n",
       " 'lack': 49,\n",
       " 'way': 50,\n",
       " 'fun': 51,\n",
       " 'best': 52,\n",
       " 'beauti': 53,\n",
       " 'tasteless': 54,\n",
       " 'unfortun': 55,\n",
       " 'think': 56,\n",
       " 'excel': 57,\n",
       " 'return': 58,\n",
       " 'rate': 59,\n",
       " 'hard': 60,\n",
       " 'dirti': 61,\n",
       " 'tender': 62,\n",
       " 'spot': 63,\n",
       " 'plu': 64,\n",
       " 'dessert': 65,\n",
       " 'done': 66,\n",
       " 'slow': 67,\n",
       " 'beer': 68,\n",
       " 'gross': 69,\n",
       " 'imagin': 70,\n",
       " 'insult': 71,\n",
       " 'fantast': 72,\n",
       " 'total': 73,\n",
       " 'nice': 74,\n",
       " 'cold': 75,\n",
       " 'definit': 76,\n",
       " 'friendli': 77,\n",
       " 'probabl': 78,\n",
       " 'let': 79,\n",
       " 'sweet': 80,\n",
       " 'would': 81,\n",
       " 'bland': 82,\n",
       " 'awesom': 83,\n",
       " 'spici': 84,\n",
       " 'favorit': 85,\n",
       " 'manag': 86,\n",
       " 'insid': 87,\n",
       " 'overpr': 88,\n",
       " 'bacon': 89,\n",
       " 'bread': 90,\n",
       " 'die': 91,\n",
       " 'white': 92,\n",
       " 'worst': 93,\n",
       " 'arriv': 94,\n",
       " 'satisfi': 95,\n",
       " 'serious': 96,\n",
       " 'hand': 97,\n",
       " 'wait': 98,\n",
       " 'sad': 99,\n",
       " 'never': 100,\n",
       " 'chef': 101,\n",
       " 'wast': 102,\n",
       " 'impress': 103,\n",
       " 'terribl': 104,\n",
       " 'everi': 105,\n",
       " 'mediocr': 106,\n",
       " 'soggi': 107,\n",
       " 'establish': 108,\n",
       " 'first': 109,\n",
       " 'delici': 110,\n",
       " 'old': 111,\n",
       " 'outstand': 112,\n",
       " 'perfect': 113}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarized_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:40:10.077830Z",
     "start_time": "2020-04-01T11:40:10.071841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model with Count Vectors on polarized words\n",
    "cv_polarized = CountVectorizer(vocabulary = polarized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:40:10.488828Z",
     "start_time": "2020-04-01T11:40:10.457828Z"
    }
   },
   "outputs": [],
   "source": [
    "X = cv_polarized.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:40:10.814827Z",
     "start_time": "2020-04-01T11:40:10.799829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:40:11.234827Z",
     "start_time": "2020-04-01T11:40:11.101829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_cv_polarized = LogisticRegression()\n",
    "lr_cv_polarized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:40:11.468827Z",
     "start_time": "2020-04-01T11:40:11.446831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred_cv_polarized = lr_cv_polarized.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:40:12.193442Z",
     "start_time": "2020-04-01T11:40:12.170457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.90      0.76        97\n",
      "           1       0.85      0.56      0.68       103\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.76      0.73      0.72       200\n",
      "weighted avg       0.76      0.72      0.72       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve\n",
    "cm2 = confusion_matrix(y_test, y_pred_cv_polarized)\n",
    "ac2 = accuracy_score(y_test, y_pred_cv_polarized)\n",
    "roc2 = roc_auc_score(y_test, y_pred_cv_polarized)\n",
    "print(classification_report(y_test, y_pred_cv_polarized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:40:12.697444Z",
     "start_time": "2020-04-01T11:40:12.691453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T11:40:13.355446Z",
     "start_time": "2020-04-01T11:40:13.346443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[87, 10],\n",
       "       [45, 58]], dtype=int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embedding is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together. In the deep learning frameworks such as TensorFlow, Keras, this part is usually handled by an embedding layer which stores a lookup table to map the words represented by numeric indexes to their dense vector representations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:13:22.137787Z",
     "start_time": "2020-04-02T15:06:54.112664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/34/d5/ce8c17971067c0184c9045112b755be5461d5ce5253ef65a367e1298d7c5/tensorflow-2.1.0-cp37-cp37m-win_amd64.whl\n",
      "Collecting gast==0.2.2 (from tensorflow)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.4)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/a7/6d/99aba8db04bf58193ed157dfe7e848494b93dd8aa3f6a4d1edfef318779c/grpcio-1.27.2-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (41.0.1)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.15.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.12.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/ab/c4/ba46d44855e6eb1770a12edace5a165a0c6de13349f592b9036257f3c3d3/Markdown-3.2.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.22.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\n",
      "Installing collected packages: gast, absl-py, grpcio, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.9.0 gast-0.2.2 google-auth-oauthlib-0.4.1 grpcio-1.27.2 markdown-3.2.1 oauthlib-3.1.0 requests-oauthlib-1.3.0 tensorboard-2.1.1 tensorflow-2.1.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:47:02.911592Z",
     "start_time": "2020-04-30T11:46:21.570028Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Others\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:47:08.605674Z",
     "start_time": "2020-04-30T11:47:03.011164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-ba1988a0e00b>\", line 2, in <module>\n",
      "    from keras.preprocessing.text import Tokenizer\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-ba1988a0e00b>\", line 2, in <module>\n",
      "    from keras.preprocessing.text import Tokenizer\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-ba1988a0e00b>\", line 2, in <module>\n",
      "    from keras.preprocessing.text import Tokenizer\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3248, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3342, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2042, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1385, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-ba1988a0e00b>\", line 2, in <module>\n",
      "    from keras.preprocessing.text import Tokenizer\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3248, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3342, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2042, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1385, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2038\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2039\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2040\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImportError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[0;32m   3340\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3341\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3342\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3343\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3344\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2040\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2041\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2042\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1385\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1286\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1288\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m             )\n\u001b[0;32m   1290\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m             \u001b[0mformatted_exceptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m             \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Flatten, Dropout, Activation, Input, Dense, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:48:05.475396Z",
     "start_time": "2020-04-30T11:48:04.813112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bbc-text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:48:10.960229Z",
     "start_time": "2020-04-30T11:48:10.889212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:48:14.812278Z",
     "start_time": "2020-04-30T11:48:14.745282Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['category'] = le.fit_transform(df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:48:19.417505Z",
     "start_time": "2020-04-30T11:48:19.402521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text\n",
       "0         4  tv future in the hands of viewers with home th...\n",
       "1         0  worldcom boss  left books alone  former worldc...\n",
       "2         3  tigers wary of farrell  gamble  leicester say ...\n",
       "3         3  yeading face newcastle in fa cup premiership s...\n",
       "4         1  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:17:05.408397Z",
     "start_time": "2020-04-02T15:17:05.400415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'entertainment', 'politics', 'sport', 'tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:48:26.605273Z",
     "start_time": "2020-04-30T11:48:26.367135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y = pd.get_dummies(df['category']).values\n",
    "dummy_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:48:29.189936Z",
     "start_time": "2020-04-30T11:48:29.179937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:48:32.693626Z",
     "start_time": "2020-04-30T11:48:32.666624Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:48:54.564769Z",
     "start_time": "2020-04-30T11:48:33.167050Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply the above function to df['text']\n",
    "df['text'] = df['text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:49:15.647934Z",
     "start_time": "2020-04-30T11:49:15.633934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>futur hand viewer home theatr system plasma hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>worldcom boss left book alon former worldcom b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>tiger wari farrel gambl leicest say rush make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>yead face newcastl cup premiership side newcas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ocean twelv raid box offic ocean twelv crime c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text\n",
       "0         4  futur hand viewer home theatr system plasma hi...\n",
       "1         0  worldcom boss left book alon former worldcom b...\n",
       "2         3  tiger wari farrel gambl leicest say rush make ...\n",
       "3         3  yead face newcastl cup premiership side newcas...\n",
       "4         1  ocean twelv raid box offic ocean twelv crime c..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:49:19.466130Z",
     "start_time": "2020-04-30T11:49:19.458145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'futur hand viewer home theatr system plasma high - definit tvs digit video record move live room way peopl watch radic differ five year time accord expert panel gather annual consum electron show las vega discuss new technolog impact one favourit pastim lead trend programm content deliv viewer via home network cabl satellit telecom compani broadband servic provid front room portabl devic one talk - about technolog ces digit person video record dvr pvr set - top box like tivo sky + system allow peopl record store play paus forward wind programm want essenti technolog allow much personalis tv also built - in high - definit set big busi japan slower take europ lack high - definit program peopl forward wind advert also forget abid network channel schedul put togeth a - la - cart entertain network cabl satellit compani worri mean term advertis revenu well brand ident viewer loyalti channel although lead technolog moment also concern rais europ particular grow uptak servic like sky + happen today see nine month year time adam hume bbc broadcast futurologist told bbc news websit like bbc issu lost advertis revenu yet press issu moment commerci broadcast brand loyalti import everyon talk content brand rather network brand said tim hanlon brand communic firm starcom mediavest realiti broadband connect anybodi produc content ad : challeng hard promot programm much choic mean said stacey jolna senior vice presid guid group way peopl find content want watch simplifi viewer mean network term channel could take leaf googl book search engin futur instead schedul help peopl find want watch kind channel model might work younger ipod generat use take control gadget play them might suit everyon panel recognis older generat comfort familiar schedul channel brand know get perhap want much choic put hand hanlon suggest end kid diaper push button alreadi everyth possibl avail said hanlon ultim consum tell market want 000 new gadget technolog showcas ces mani enhanc tv - watch experi high - definit set everywher mani new model lcd liquid crystal display tvs launch dvr capabl built instead extern box one exampl launch show humax 26 - inch lcd 80 - hour tivo dvr dvd record one biggest satellit compani directtv even launch brand dvr show 100 - hour record capabl instant replay search function set paus rewind hour microsoft chief bill gate announc pre - show keynot speech partnership tivo call tivotogo mean peopl play record programm window pcs mobil devic reflect increas trend free multimedia peopl watch want want'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:49:37.774031Z",
     "start_time": "2020-04-30T11:49:37.630010Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'].values, dummy_y, test_size = 0.25, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:49:38.482418Z",
     "start_time": "2020-04-30T11:49:38.308419Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-7ab7cb886988>\", line 1, in <module>\n",
      "    tokenizer = Tokenizer()\n",
      "NameError: name 'Tokenizer' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-7ab7cb886988>\", line 1, in <module>\n",
      "    tokenizer = Tokenizer()\n",
      "NameError: name 'Tokenizer' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T11:49:43.078025Z",
     "start_time": "2020-04-30T11:49:42.981474Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-f9979903851c>\", line 1, in <module>\n",
      "    tokenizer.word_index\n",
      "NameError: name 'tokenizer' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-f9979903851c>\", line 1, in <module>\n",
      "    tokenizer.word_index\n",
      "NameError: name 'tokenizer' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:17:41.016954Z",
     "start_time": "2020-04-02T15:17:23.618Z"
    }
   },
   "outputs": [],
   "source": [
    "trainsequences = tokenizer.texts_to_sequences(X_train)\n",
    "print(trainsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:17:41.027953Z",
     "start_time": "2020-04-02T15:17:24.250Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:17:41.042956Z",
     "start_time": "2020-04-02T15:17:24.771Z"
    }
   },
   "outputs": [],
   "source": [
    "len(trainsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:17:41.052952Z",
     "start_time": "2020-04-02T15:17:25.164Z"
    }
   },
   "outputs": [],
   "source": [
    "trainsequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:17:41.063954Z",
     "start_time": "2020-04-02T15:17:25.937Z"
    }
   },
   "outputs": [],
   "source": [
    "MAXLEN = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:17:41.074953Z",
     "start_time": "2020-04-02T15:17:26.420Z"
    }
   },
   "outputs": [],
   "source": [
    "trainseqs = pad_sequences(trainsequences, maxlen=MAXLEN, padding='post')\n",
    "print(trainseqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1668, 250)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainseqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsequences = tokenizer.texts_to_sequences(X_test)\n",
    "testseqs = pad_sequences(testsequences, maxlen=MAXLEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 529  252  275 ...    0    0    0]\n",
      " [ 141 2463  167 ...  108  525    1]\n",
      " [ 957 1324 4982 ...    0    0    0]\n",
      " ...\n",
      " [  14  273  861 ... 1358 1046  476]\n",
      " [ 874  536 1339 ...    0    0    0]\n",
      " [5604  323  277 ...  116 2714  488]]\n"
     ]
    }
   ],
   "source": [
    "print(testseqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1668, 250)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainseqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557, 5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_units = df['category'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=MAXLEN))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(op_units, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 32)           581056    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 40005     \n",
      "=================================================================\n",
      "Total params: 621,061\n",
      "Trainable params: 621,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='Embedding.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1668 samples, validate on 557 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 1.4989 - acc: 0.3753 - val_loss: 1.3889 - val_acc: 0.4865\n",
      "Epoch 2/5\n",
      " - 1s - loss: 0.9327 - acc: 0.8369 - val_loss: 0.7150 - val_acc: 0.8761\n",
      "Epoch 3/5\n",
      " - 1s - loss: 0.2707 - acc: 0.9832 - val_loss: 0.2899 - val_acc: 0.9677\n",
      "Epoch 4/5\n",
      " - 1s - loss: 0.0800 - acc: 0.9988 - val_loss: 0.1891 - val_acc: 0.9713\n",
      "Epoch 5/5\n",
      " - 1s - loss: 0.0362 - acc: 1.0000 - val_loss: 0.1518 - val_acc: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10911bff28>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(trainseqs, \n",
    "          y_train, \n",
    "          epochs=5,\n",
    "          validation_data=(testseqs,y_test),\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.151848\n",
      "Accuracy: 97.666068\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(testseqs, y_test, verbose=2)\n",
    "print('Loss: %f' % (loss))\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18158, 32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from the Embedding Layers\n",
    "embeddings = model.layers[0].get_weights()[0]\n",
    "\n",
    "# `embeddings` has a shape of (num_vocab, embedding_dim) \n",
    "\n",
    "# `word_to_index` is a mapping (i.e. dict) from words to \n",
    "# their index\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05851227,  0.00641839, -0.02863956, -0.01390276, -0.01745366,\n",
       "       -0.02456674, -0.0247659 , -0.04967704,  0.01430894,  0.00141364,\n",
       "       -0.03670051,  0.0137229 , -0.0260178 , -0.05774996, -0.03162028,\n",
       "       -0.01499818,  0.05789851,  0.04830869, -0.02433261,  0.0626341 ,\n",
       "        0.05774857,  0.02380782,  0.01222595,  0.04541155,  0.00917102,\n",
       "        0.03889833, -0.00480174,  0.05583175,  0.00934568,  0.03900613,\n",
       "       -0.03565377,  0.0149274 ], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_embeddings['tiger']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus = df['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus = []\n",
    "for i in range(len(df['text'])):\n",
    "    news = df['text'][i].lower()\n",
    "    news = news.split()\n",
    "    news_corpus.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['futur',\n",
       " 'hand',\n",
       " 'viewer',\n",
       " 'home',\n",
       " 'theatr',\n",
       " 'system',\n",
       " 'plasma',\n",
       " 'high',\n",
       " '-',\n",
       " 'definit',\n",
       " 'tvs',\n",
       " 'digit',\n",
       " 'video',\n",
       " 'record',\n",
       " 'move',\n",
       " 'live',\n",
       " 'room',\n",
       " 'way',\n",
       " 'peopl',\n",
       " 'watch',\n",
       " 'radic',\n",
       " 'differ',\n",
       " 'five',\n",
       " 'year',\n",
       " 'time',\n",
       " 'accord',\n",
       " 'expert',\n",
       " 'panel',\n",
       " 'gather',\n",
       " 'annual',\n",
       " 'consum',\n",
       " 'electron',\n",
       " 'show',\n",
       " 'las',\n",
       " 'vega',\n",
       " 'discuss',\n",
       " 'new',\n",
       " 'technolog',\n",
       " 'impact',\n",
       " 'one',\n",
       " 'favourit',\n",
       " 'pastim',\n",
       " 'lead',\n",
       " 'trend',\n",
       " 'programm',\n",
       " 'content',\n",
       " 'deliv',\n",
       " 'viewer',\n",
       " 'via',\n",
       " 'home',\n",
       " 'network',\n",
       " 'cabl',\n",
       " 'satellit',\n",
       " 'telecom',\n",
       " 'compani',\n",
       " 'broadband',\n",
       " 'servic',\n",
       " 'provid',\n",
       " 'front',\n",
       " 'room',\n",
       " 'portabl',\n",
       " 'devic',\n",
       " 'one',\n",
       " 'talk',\n",
       " '-',\n",
       " 'about',\n",
       " 'technolog',\n",
       " 'ces',\n",
       " 'digit',\n",
       " 'person',\n",
       " 'video',\n",
       " 'record',\n",
       " 'dvr',\n",
       " 'pvr',\n",
       " 'set',\n",
       " '-',\n",
       " 'top',\n",
       " 'box',\n",
       " 'like',\n",
       " 'tivo',\n",
       " 'sky',\n",
       " '+',\n",
       " 'system',\n",
       " 'allow',\n",
       " 'peopl',\n",
       " 'record',\n",
       " 'store',\n",
       " 'play',\n",
       " 'paus',\n",
       " 'forward',\n",
       " 'wind',\n",
       " 'programm',\n",
       " 'want',\n",
       " 'essenti',\n",
       " 'technolog',\n",
       " 'allow',\n",
       " 'much',\n",
       " 'personalis',\n",
       " 'tv',\n",
       " 'also',\n",
       " 'built',\n",
       " '-',\n",
       " 'in',\n",
       " 'high',\n",
       " '-',\n",
       " 'definit',\n",
       " 'set',\n",
       " 'big',\n",
       " 'busi',\n",
       " 'japan',\n",
       " 'slower',\n",
       " 'take',\n",
       " 'europ',\n",
       " 'lack',\n",
       " 'high',\n",
       " '-',\n",
       " 'definit',\n",
       " 'program',\n",
       " 'peopl',\n",
       " 'forward',\n",
       " 'wind',\n",
       " 'advert',\n",
       " 'also',\n",
       " 'forget',\n",
       " 'abid',\n",
       " 'network',\n",
       " 'channel',\n",
       " 'schedul',\n",
       " 'put',\n",
       " 'togeth',\n",
       " 'a',\n",
       " '-',\n",
       " 'la',\n",
       " '-',\n",
       " 'cart',\n",
       " 'entertain',\n",
       " 'network',\n",
       " 'cabl',\n",
       " 'satellit',\n",
       " 'compani',\n",
       " 'worri',\n",
       " 'mean',\n",
       " 'term',\n",
       " 'advertis',\n",
       " 'revenu',\n",
       " 'well',\n",
       " 'brand',\n",
       " 'ident',\n",
       " 'viewer',\n",
       " 'loyalti',\n",
       " 'channel',\n",
       " 'although',\n",
       " 'lead',\n",
       " 'technolog',\n",
       " 'moment',\n",
       " 'also',\n",
       " 'concern',\n",
       " 'rais',\n",
       " 'europ',\n",
       " 'particular',\n",
       " 'grow',\n",
       " 'uptak',\n",
       " 'servic',\n",
       " 'like',\n",
       " 'sky',\n",
       " '+',\n",
       " 'happen',\n",
       " 'today',\n",
       " 'see',\n",
       " 'nine',\n",
       " 'month',\n",
       " 'year',\n",
       " 'time',\n",
       " 'adam',\n",
       " 'hume',\n",
       " 'bbc',\n",
       " 'broadcast',\n",
       " 'futurologist',\n",
       " 'told',\n",
       " 'bbc',\n",
       " 'news',\n",
       " 'websit',\n",
       " 'like',\n",
       " 'bbc',\n",
       " 'issu',\n",
       " 'lost',\n",
       " 'advertis',\n",
       " 'revenu',\n",
       " 'yet',\n",
       " 'press',\n",
       " 'issu',\n",
       " 'moment',\n",
       " 'commerci',\n",
       " 'broadcast',\n",
       " 'brand',\n",
       " 'loyalti',\n",
       " 'import',\n",
       " 'everyon',\n",
       " 'talk',\n",
       " 'content',\n",
       " 'brand',\n",
       " 'rather',\n",
       " 'network',\n",
       " 'brand',\n",
       " 'said',\n",
       " 'tim',\n",
       " 'hanlon',\n",
       " 'brand',\n",
       " 'communic',\n",
       " 'firm',\n",
       " 'starcom',\n",
       " 'mediavest',\n",
       " 'realiti',\n",
       " 'broadband',\n",
       " 'connect',\n",
       " 'anybodi',\n",
       " 'produc',\n",
       " 'content',\n",
       " 'ad',\n",
       " ':',\n",
       " 'challeng',\n",
       " 'hard',\n",
       " 'promot',\n",
       " 'programm',\n",
       " 'much',\n",
       " 'choic',\n",
       " 'mean',\n",
       " 'said',\n",
       " 'stacey',\n",
       " 'jolna',\n",
       " 'senior',\n",
       " 'vice',\n",
       " 'presid',\n",
       " 'guid',\n",
       " 'group',\n",
       " 'way',\n",
       " 'peopl',\n",
       " 'find',\n",
       " 'content',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'simplifi',\n",
       " 'viewer',\n",
       " 'mean',\n",
       " 'network',\n",
       " 'term',\n",
       " 'channel',\n",
       " 'could',\n",
       " 'take',\n",
       " 'leaf',\n",
       " 'googl',\n",
       " 'book',\n",
       " 'search',\n",
       " 'engin',\n",
       " 'futur',\n",
       " 'instead',\n",
       " 'schedul',\n",
       " 'help',\n",
       " 'peopl',\n",
       " 'find',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'kind',\n",
       " 'channel',\n",
       " 'model',\n",
       " 'might',\n",
       " 'work',\n",
       " 'younger',\n",
       " 'ipod',\n",
       " 'generat',\n",
       " 'use',\n",
       " 'take',\n",
       " 'control',\n",
       " 'gadget',\n",
       " 'play',\n",
       " 'them',\n",
       " 'might',\n",
       " 'suit',\n",
       " 'everyon',\n",
       " 'panel',\n",
       " 'recognis',\n",
       " 'older',\n",
       " 'generat',\n",
       " 'comfort',\n",
       " 'familiar',\n",
       " 'schedul',\n",
       " 'channel',\n",
       " 'brand',\n",
       " 'know',\n",
       " 'get',\n",
       " 'perhap',\n",
       " 'want',\n",
       " 'much',\n",
       " 'choic',\n",
       " 'put',\n",
       " 'hand',\n",
       " 'hanlon',\n",
       " 'suggest',\n",
       " 'end',\n",
       " 'kid',\n",
       " 'diaper',\n",
       " 'push',\n",
       " 'button',\n",
       " 'alreadi',\n",
       " 'everyth',\n",
       " 'possibl',\n",
       " 'avail',\n",
       " 'said',\n",
       " 'hanlon',\n",
       " 'ultim',\n",
       " 'consum',\n",
       " 'tell',\n",
       " 'market',\n",
       " 'want',\n",
       " '000',\n",
       " 'new',\n",
       " 'gadget',\n",
       " 'technolog',\n",
       " 'showcas',\n",
       " 'ces',\n",
       " 'mani',\n",
       " 'enhanc',\n",
       " 'tv',\n",
       " '-',\n",
       " 'watch',\n",
       " 'experi',\n",
       " 'high',\n",
       " '-',\n",
       " 'definit',\n",
       " 'set',\n",
       " 'everywher',\n",
       " 'mani',\n",
       " 'new',\n",
       " 'model',\n",
       " 'lcd',\n",
       " 'liquid',\n",
       " 'crystal',\n",
       " 'display',\n",
       " 'tvs',\n",
       " 'launch',\n",
       " 'dvr',\n",
       " 'capabl',\n",
       " 'built',\n",
       " 'instead',\n",
       " 'extern',\n",
       " 'box',\n",
       " 'one',\n",
       " 'exampl',\n",
       " 'launch',\n",
       " 'show',\n",
       " 'humax',\n",
       " '26',\n",
       " '-',\n",
       " 'inch',\n",
       " 'lcd',\n",
       " '80',\n",
       " '-',\n",
       " 'hour',\n",
       " 'tivo',\n",
       " 'dvr',\n",
       " 'dvd',\n",
       " 'record',\n",
       " 'one',\n",
       " 'biggest',\n",
       " 'satellit',\n",
       " 'compani',\n",
       " 'directtv',\n",
       " 'even',\n",
       " 'launch',\n",
       " 'brand',\n",
       " 'dvr',\n",
       " 'show',\n",
       " '100',\n",
       " '-',\n",
       " 'hour',\n",
       " 'record',\n",
       " 'capabl',\n",
       " 'instant',\n",
       " 'replay',\n",
       " 'search',\n",
       " 'function',\n",
       " 'set',\n",
       " 'paus',\n",
       " 'rewind',\n",
       " 'hour',\n",
       " 'microsoft',\n",
       " 'chief',\n",
       " 'bill',\n",
       " 'gate',\n",
       " 'announc',\n",
       " 'pre',\n",
       " '-',\n",
       " 'show',\n",
       " 'keynot',\n",
       " 'speech',\n",
       " 'partnership',\n",
       " 'tivo',\n",
       " 'call',\n",
       " 'tivotogo',\n",
       " 'mean',\n",
       " 'peopl',\n",
       " 'play',\n",
       " 'record',\n",
       " 'programm',\n",
       " 'window',\n",
       " 'pcs',\n",
       " 'mobil',\n",
       " 'devic',\n",
       " 'reflect',\n",
       " 'increas',\n",
       " 'trend',\n",
       " 'free',\n",
       " 'multimedia',\n",
       " 'peopl',\n",
       " 'watch',\n",
       " 'want',\n",
       " 'want']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:13:22.217789Z",
     "start_time": "2020-04-02T15:13:22.172792Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b37a5f7c8eda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Word2Vec Data processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m model_w2v = Word2Vec(sentences=news_corpus, size=embedding_size, \n\u001b[0m\u001b[0;32m      4\u001b[0m                 window=5, workers=4, min_count=50)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'news_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# Word2Vec Data processing\n",
    "from gensim.models import Word2Vec\n",
    "model_w2v = Word2Vec(sentences=news_corpus, size=embedding_size, \n",
    "                window=5, workers=4, min_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('problem', 0.8933914303779602),\n",
       " ('worri', 0.8912731409072876),\n",
       " ('possibl', 0.8881564140319824),\n",
       " ('particular', 0.8845880031585693),\n",
       " ('awar', 0.8747023940086365),\n",
       " ('potenti', 0.8711709976196289),\n",
       " ('integr', 0.8678703308105469),\n",
       " ('consid', 0.8651309609413147),\n",
       " ('longer', 0.8611962795257568),\n",
       " ('focus', 0.8564385175704956)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar('futur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1853\n"
     ]
    }
   ],
   "source": [
    "words = list(model_w2v.wv.vocab)\n",
    "print(\"Vocabulary size: %d\"% len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'news_w2v.txt'\n",
    "model_w2v.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pre-trained w2v as Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index_w2v = {}\n",
    "f = open(os.path.join('','news_w2v.txt'), encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coeffs = np.asarray(values[1:])\n",
    "    embeddings_index_w2v[word] = coeffs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index_w2v['futur'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(news_corpus)\n",
    "seqs = tokenizer.texts_to_sequences(news_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20660 unique tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2225, 50)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad sequences\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %d unique tokens\"% len(word_index))\n",
    "\n",
    "news_pad = pad_sequences(seqs, maxlen = 50)\n",
    "news_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20661\n"
     ]
    }
   ],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix_w2v = np.zeros((num_words, embedding_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index_w2v.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embeddings_index_w2v will be all zeros\n",
    "        embedding_matrix_w2v[i] = embedding_vector\n",
    "print(num_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model_pt_w2v = Sequential()\n",
    "embedding_layer_pt_w2v = Embedding(num_words, \n",
    "                            embedding_size,\n",
    "                            embeddings_initializer = Constant(embedding_matrix_w2v),\n",
    "                            input_length=MAXLEN,\n",
    "                            trainable=False)\n",
    "model_pt_w2v.add(embedding_layer_pt_w2v)\n",
    "model_pt_w2v.add(Flatten())\n",
    "model_pt_w2v.add(Dense(op_units, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 32)           581056    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 40005     \n",
      "=================================================================\n",
      "Total params: 621,061\n",
      "Trainable params: 621,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1668 samples, validate on 557 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 0.0192 - acc: 1.0000 - val_loss: 0.1153 - val_acc: 0.9785\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0993 - val_acc: 0.9803\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0911 - val_acc: 0.9749\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0878 - val_acc: 0.9767\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 0.9785\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0842 - val_acc: 0.9785\n",
      "Epoch 7/50\n",
      " - 1s - loss: 8.4065e-04 - acc: 1.0000 - val_loss: 0.0833 - val_acc: 0.9785\n",
      "Epoch 8/50\n",
      " - 1s - loss: 6.9079e-04 - acc: 1.0000 - val_loss: 0.0827 - val_acc: 0.9785\n",
      "Epoch 9/50\n",
      " - 0s - loss: 5.8509e-04 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 0.9785\n",
      "Epoch 10/50\n",
      " - 1s - loss: 5.0056e-04 - acc: 1.0000 - val_loss: 0.0818 - val_acc: 0.9785\n",
      "Epoch 11/50\n",
      " - 0s - loss: 4.3485e-04 - acc: 1.0000 - val_loss: 0.0812 - val_acc: 0.9785\n",
      "Epoch 12/50\n",
      " - 1s - loss: 3.8180e-04 - acc: 1.0000 - val_loss: 0.0805 - val_acc: 0.9785\n",
      "Epoch 13/50\n",
      " - 0s - loss: 3.3773e-04 - acc: 1.0000 - val_loss: 0.0802 - val_acc: 0.9785\n",
      "Epoch 14/50\n",
      " - 1s - loss: 3.0148e-04 - acc: 1.0000 - val_loss: 0.0802 - val_acc: 0.9785\n",
      "Epoch 15/50\n",
      " - 0s - loss: 2.7032e-04 - acc: 1.0000 - val_loss: 0.0800 - val_acc: 0.9785\n",
      "Epoch 16/50\n",
      " - 0s - loss: 2.4375e-04 - acc: 1.0000 - val_loss: 0.0797 - val_acc: 0.9785\n",
      "Epoch 17/50\n",
      " - 0s - loss: 2.2121e-04 - acc: 1.0000 - val_loss: 0.0799 - val_acc: 0.9785\n",
      "Epoch 18/50\n",
      " - 0s - loss: 2.0177e-04 - acc: 1.0000 - val_loss: 0.0797 - val_acc: 0.9785\n",
      "Epoch 19/50\n",
      " - 1s - loss: 1.8507e-04 - acc: 1.0000 - val_loss: 0.0795 - val_acc: 0.9785\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1.6946e-04 - acc: 1.0000 - val_loss: 0.0793 - val_acc: 0.9803\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1.5689e-04 - acc: 1.0000 - val_loss: 0.0792 - val_acc: 0.9803\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1.4529e-04 - acc: 1.0000 - val_loss: 0.0792 - val_acc: 0.9803\n",
      "Epoch 23/50\n",
      " - 1s - loss: 1.3485e-04 - acc: 1.0000 - val_loss: 0.0791 - val_acc: 0.9785\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1.2560e-04 - acc: 1.0000 - val_loss: 0.0790 - val_acc: 0.9785\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1.1740e-04 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9785\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1.0998e-04 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9785\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1.0348e-04 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9785\n",
      "Epoch 28/50\n",
      " - 1s - loss: 9.7500e-05 - acc: 1.0000 - val_loss: 0.0788 - val_acc: 0.9785\n",
      "Epoch 29/50\n",
      " - 0s - loss: 9.1648e-05 - acc: 1.0000 - val_loss: 0.0791 - val_acc: 0.9785\n",
      "Epoch 30/50\n",
      " - 0s - loss: 8.6443e-05 - acc: 1.0000 - val_loss: 0.0790 - val_acc: 0.9785\n",
      "Epoch 31/50\n",
      " - 0s - loss: 8.1727e-05 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9785\n",
      "Epoch 32/50\n",
      " - 1s - loss: 7.7326e-05 - acc: 1.0000 - val_loss: 0.0788 - val_acc: 0.9785\n",
      "Epoch 33/50\n",
      " - 1s - loss: 7.3230e-05 - acc: 1.0000 - val_loss: 0.0786 - val_acc: 0.9785\n",
      "Epoch 34/50\n",
      " - 0s - loss: 6.9522e-05 - acc: 1.0000 - val_loss: 0.0777 - val_acc: 0.9785\n",
      "Epoch 35/50\n",
      " - 1s - loss: 6.6108e-05 - acc: 1.0000 - val_loss: 0.0776 - val_acc: 0.9785\n",
      "Epoch 36/50\n",
      " - 1s - loss: 6.3063e-05 - acc: 1.0000 - val_loss: 0.0776 - val_acc: 0.9785\n",
      "Epoch 37/50\n",
      " - 1s - loss: 6.0184e-05 - acc: 1.0000 - val_loss: 0.0775 - val_acc: 0.9785\n",
      "Epoch 38/50\n",
      " - 1s - loss: 5.7443e-05 - acc: 1.0000 - val_loss: 0.0776 - val_acc: 0.9785\n",
      "Epoch 39/50\n",
      " - 0s - loss: 5.4848e-05 - acc: 1.0000 - val_loss: 0.0777 - val_acc: 0.9785\n",
      "Epoch 40/50\n",
      " - 0s - loss: 5.2463e-05 - acc: 1.0000 - val_loss: 0.0777 - val_acc: 0.9785\n",
      "Epoch 41/50\n",
      " - 1s - loss: 5.0235e-05 - acc: 1.0000 - val_loss: 0.0777 - val_acc: 0.9785\n",
      "Epoch 42/50\n",
      " - 1s - loss: 4.8159e-05 - acc: 1.0000 - val_loss: 0.0780 - val_acc: 0.9785\n",
      "Epoch 43/50\n",
      " - 0s - loss: 4.6271e-05 - acc: 1.0000 - val_loss: 0.0782 - val_acc: 0.9785\n",
      "Epoch 44/50\n",
      " - 1s - loss: 4.4478e-05 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9767\n",
      "Epoch 45/50\n",
      " - 1s - loss: 4.2831e-05 - acc: 1.0000 - val_loss: 0.0782 - val_acc: 0.9767\n",
      "Epoch 46/50\n",
      " - 0s - loss: 4.1222e-05 - acc: 1.0000 - val_loss: 0.0782 - val_acc: 0.9785\n",
      "Epoch 47/50\n",
      " - 0s - loss: 3.9612e-05 - acc: 1.0000 - val_loss: 0.0782 - val_acc: 0.9785\n",
      "Epoch 48/50\n",
      " - 0s - loss: 3.8112e-05 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9785\n",
      "Epoch 49/50\n",
      " - 1s - loss: 3.6670e-05 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9785\n",
      "Epoch 50/50\n",
      " - 0s - loss: 3.5308e-05 - acc: 1.0000 - val_loss: 0.0784 - val_acc: 0.9785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1049183908>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(trainseqs, \n",
    "          y_train, \n",
    "          batch_size=128, \n",
    "          epochs=50, \n",
    "          validation_data=(testseqs, y_test), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Hyper Tunnning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T11:35:51.982361Z",
     "start_time": "2020-05-05T11:35:48.940960Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-007d0a4f042c>\", line 1, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-007d0a4f042c>\", line 1, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-007d0a4f042c>\", line 1, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3248, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3342, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2042, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1385, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-007d0a4f042c>\", line 1, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3248, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3342, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2042, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1385, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2038\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2039\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2040\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImportError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[0;32m   3340\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3341\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3342\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3343\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3344\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2040\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2041\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2042\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1385\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1286\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1288\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m             )\n\u001b[0;32m   1290\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m             \u001b[0mformatted_exceptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m             \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "print(tf.__version__)\n",
    "print(kt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.991px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
