{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T16:35:29.400659Z",
     "start_time": "2020-10-06T16:35:29.385667Z"
    }
   },
   "source": [
    "- Stemming is rule based and faster but where as Lemmatization convert the word in meaningfull words\n",
    "- Stemming Application like sentiment analysis, spam classifier as root will work in that case\n",
    "- Lemmatization application like chat boat,Question answer application as reponse we get is meaning full.\n",
    "\n",
    "<img src=\"Stem.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Countvectorizer (Bag of words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types...\n",
    "\n",
    "1. Binary Bag of word(Replace by 0 or 1)\n",
    "\n",
    "<img src='BBOW.PNG'>\n",
    "\n",
    "2. Normal Bag of Word(Replace by frequency)\n",
    "\n",
    "<img src='BOW.PNG'>\n",
    "\n",
    "<b>Disadvantage</b>\n",
    "\n",
    "- Not able to capture the rare words\n",
    "- Give more weightage to common words but in case of Binary BOW, it will give equal weightage to all words\n",
    "- It create a sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T17:11:55.575165Z",
     "start_time": "2020-08-27T17:11:55.562173Z"
    }
   },
   "source": [
    "<img src=\"TFIDF.png\">\n",
    "\n",
    "<img src=\"TF.PNG\">\n",
    "\n",
    "- Handle rare words\n",
    "- Give less weightage to commom words/most occuring words\n",
    "- Not able to understand the context or topic only focused on words frequency\n",
    "- Both BOW and TF-IDF approach semantic information is not stored(sequence of word). TF-IDF gives importance to uncommon words.\n",
    "- There is definitely chance of over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T17:19:47.139806Z",
     "start_time": "2020-08-27T17:19:47.127805Z"
    }
   },
   "source": [
    "# LSA(Latent Symmentic Analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T17:24:59.179475Z",
     "start_time": "2020-08-27T17:24:59.165473Z"
    }
   },
   "source": [
    "- It is based on topic modeling and used SVD.\n",
    "<img src=\"LSA1.png\">\n",
    "\n",
    "- Use only Sigma and V Transpose \n",
    "<img src=\"LSA2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec(Google 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T17:28:02.962647Z",
     "start_time": "2020-08-27T17:28:02.949652Z"
    }
   },
   "source": [
    "- Each word is basically represent as a vector of 32 or more dimension instead of a single number\n",
    "- Here semantics information and relation between different words is also preserved\n",
    "- Understand the context based on near by words(window_size)\n",
    "- Having OOV(Out of bag) issue\n",
    "- Not able to Handle <font color=blue>Polysemi</font> words\n",
    "\n",
    "<img src='Word2Vec.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE(Stanford 2014) Global vector of word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "\n",
    "Glove is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently we see this word in some “context” (the columns) in a large corpus. The number of “contexts” is of course large, since it is essentially combinatorial in size. So then we factorize this matrix to yield a lower-dimensional (word x features) matrix, where each row now yields a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTTEXT(Facebook 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has one advantage over other two, it <font color=\"purple\"><b>handles out of bag words</b></font>, which was problem with Word2Vec and GloVe.\n",
    "\n",
    "FastText, builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. FastText vectors have been shown to be more accurate than Word2Vec vectors by a number of different measures.\n",
    "\n",
    "<font color=\"blue\">Is Fasttext better than GloVe and Word2Vec?</font>(Yes it Is)\n",
    "\n",
    "Generate better word embeddings for rare words ( even if words are rare their character n grams are still shared with other words — hence the embeddings can still be good).\n",
    "Out of vocabulary words — they can construct the vector for a word from its character n-grams even if word doesn’t appear in training corpus, both Word2vec and Glove can’t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMO(AllenNLP 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Embedding based on the full Paragraph and able to understand context\n",
    "- No word <font color=\"blue\"><b>Polysemi</b></font> issue.(One word with different context has different embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo word vectors are computed on top of a two-layer bidirectional language model (biLM). This biLM model has two layers stacked together. Each layer has 2 passes — forward pass and backward pass:\n",
    "\n",
    "\n",
    "- The architecture above uses a character-level convolutional neural network (CNN) to represent words of a text string into raw word vectors.\n",
    "\n",
    "- These raw word vectors act as inputs to the first layer of biLM.\n",
    "- The forward pass contains information about a certain word and the context (other words) before that word.\n",
    "- The backward pass contains information about the word and the context after it.\n",
    "- This pair of information, from the forward and backward pass, forms the intermediate word vectors.\n",
    "- These intermediate word vectors are fed into the next layer of biLM.\n",
    "\n",
    "<img src=\"ELMO.png\">\n",
    "\n",
    "The final representation (ELMo) is the weighted sum of the raw word vectors and the 2 intermediate word vectors.\n",
    "As the input to the biLM is computed from characters rather than words, it captures the inner structure of the word. For example, the biLM will be able to figure out that terms like beauty and beautiful are related at some level without even looking at the context they often appear in. Sounds incredible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easiest way we can add up the word vectors in a sentence to form a sentence vector or we can average them.\n",
    "\n",
    "<b>Skip-thoughts vectors(2015)</b> : It is similar to Skip gram model, here we have to predict surrounding sentences give current sentence. It uses RNN base encoder decoder for training.\n",
    "\n",
    "<b>InferSent(2017)</b> : It is also a classification based approach. Target sentence and input sentence both are encoded using same encoder. It uses Bi-Directional LSTM.\n",
    "\n",
    "<b>Quick-thoughts vectors(2018)</b> : It uses the classification approach to predict the next sentence. Decoder replaced by classifier. It is faster than Skip through vectors.\n",
    "\n",
    "<b>Multi-task learning(2018)</b>: It is also a classification based approach. Here Encoder uses Transformer- Network.\n",
    "\n",
    "[Reference Type of Embedding Article](https://medium.com/@mpspatel555/b3ab4465697d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN(Bastiaan Quast,2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It used weight of previous stage as Input\n",
    "2. It will take long time with large sequence as it has <font color='Blue'>gradient vanishing and exploding</font> issue as long partial derivative chain rule calculation.\n",
    "\n",
    "<img src=\"RNN_ISSUE.PNG\">\n",
    "\n",
    "There are couple of remedies there to avoid this problem.\n",
    "We can use ReLu unit as an activation function, RMS Prop as an optimization algorithm and LSTM’s or GRU’s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM(1997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T14:30:54.331370Z",
     "start_time": "2020-09-30T14:30:54.317370Z"
    }
   },
   "source": [
    "1. It has solved the long sequence issue.\n",
    "2. It use the weights from previous stage along with memory cell which help to forgot th old un-needed sequence.\n",
    "\n",
    "- Forget Gate “f” ( a neural network with sigmoid)\n",
    "- Candidate layer “C`\"(a NN with Tanh)\n",
    "- Input Gate “I” ( a NN with sigmoid )\n",
    "- Output Gate “O”( a NN with sigmoid)\n",
    "- Hidden state “H” ( a vector )\n",
    "- Memory state “C” ( a vector)\n",
    "\n",
    "<img src=\"LSTM.PNG\">\n",
    "\n",
    "[LSTM Reference Article](https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While greedy decoding can give us quite reasonable translation quality, a beam search decoder can further boost performance. The idea of beam search is to better explore the search space of all possible translations by keeping around a small set of top candidates as we translate. The size of the beam is called beam width; a minimal beam width of, say size 10, is generally sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BELU vs ROUGE(Recall-Oriented Understudy for Gisting Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Bleu measures precision</b>: how much the words (and/or n-grams) in the machine generated summaries appeared in the human reference summaries.\n",
    "\n",
    "<b>Rouge measures recall</b>: how much the words (and/or n-grams) in the human reference summaries appeared in the machine generated summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- <b>ROUGE-N</b>: N-gram based co-occurrence statistics.It measures unigram, bigram, trigram and higher order n-gram overlap\n",
    "\n",
    "\n",
    "- <b>ROUGE-L</b>: Longest Common Subsequence (LCS) based statistics.It measures longest matching sequence of words using LCS. An <b>advantage</b> of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order. Since it automatically includes longest in-sequence common n-grams, you don't need a predefined n-gram length. <b>Disadvantage</b>:- LCS suffers one disadvantage that it only counts the main in-sequence words; therefore,\n",
    "other alternative LCSes and shorter sequences are not reflected in the final score.\n",
    "\n",
    "Example:- one list of list as X and another list of list as Y, where we will check the matching of longest element in both the list into consideration using LCS longest common sub sequence between two elements. \n",
    "\n",
    "- <b>ROUGE-W</b>: Weighted LCS-based statistics that favors consecutive \n",
    "\n",
    "\n",
    "\n",
    "- <b>ROUGE-S</b>: Skip-bigram based co-occurrence statistics. Skip-bigram is any pair of words in their sentence order. \n",
    "\n",
    "\n",
    "- <b>ROUGE-SU</b>: Skip-bigram plus unigram-based co-occurrence statistics.\n",
    "\n",
    " \n",
    " [Example How Rouge Score works](http://text-analytics101.rxnlp.com/2017/01/how-rouge-works-for-evaluation-of.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally - these results are complementing, as is often the case in precision vs recall. If you have many words from the system results appearing in the human references you will have high Bleu, and if you have many words from the human references appearing in the system results you will have high Rouge.\n",
    "\n",
    "\n",
    "In your case it would appear that sys1 has a higher Rouge than sys2 since the results in sys1 consistently had more words from the human references appear in them than the results from sys2. However, since your Bleu score showed that sys1 has lower recall than sys2, this would suggest that not so many words from your sys1 results appeared in the human references, in respect to sys2.\n",
    "\n",
    "This could happen for example if your sys1 is outputting results which contain words from the references (upping the Rouge), but also many words which the references didn't include (lowering the Bleu). sys2, as it seems, is giving results for which most words outputted do appear in the human references (upping the Blue), but also missing many words from its results which do appear in the human references.\n",
    "\n",
    "BTW, there's something called <font color=blue>brevity penalty</font>, which is quite important and has already been added to standard Bleu implementations. It penalizes system results which are shorter than the general length of a reference (read more about it here). This complements the n-gram metric behavior which in effect penalizes longer than reference results, since the denominator grows the longer the system result is.\n",
    "\n",
    "You could also implement something similar for Rouge, but this time penalizing system results which are longer than the general reference length, which would otherwise enable them to obtain artificially higher Rouge scores (since the longer the result, the higher the chance you would hit some word appearing in the references). In Rouge we divide by the length of the human references, so we would need an additional penalty for longer system results which could artificially raise their Rouge score.\n",
    "\n",
    "Finally, you could use the F1 measure to make the metrics work together: \n",
    "\n",
    "\n",
    " $ F1 = 2 * (Bleu * Rouge) / (Bleu + Rouge) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RougeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T11:18:58.611105Z",
     "start_time": "2020-10-08T11:18:58.606097Z"
    }
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[rouge_score package reference](https://pypi.org/project/rouge-score/)\n",
    "\n",
    "- sentence-level: Compute longest common subsequence (LCS) between two pieces of text. Newlines are ignored. This is called rougeL in this package.\n",
    "- summary-level: Newlines in the text are interpreted as sentence boundaries, and the LCS is computed between each pair of reference and candidate sentences, and something called union-LCS is computed. This is called rougeLsum in this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T11:19:30.745566Z",
     "start_time": "2020-10-08T11:19:30.739573Z"
    }
   },
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL','rougeLsum'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T11:19:31.292827Z",
     "start_time": "2020-10-08T11:19:31.286828Z"
    }
   },
   "outputs": [],
   "source": [
    "machine=\"the cat was found under the bed\"\n",
    "human=\"the cat was under the bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T11:19:32.140301Z",
     "start_time": "2020-10-08T11:19:32.127284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=1.0, recall=0.8571428571428571, fmeasure=0.923076923076923),\n",
       " 'rouge2': Score(precision=0.8, recall=0.6666666666666666, fmeasure=0.7272727272727272),\n",
       " 'rougeL': Score(precision=1.0, recall=0.8571428571428571, fmeasure=0.923076923076923),\n",
       " 'rougeLsum': Score(precision=1.0, recall=0.8571428571428571, fmeasure=0.923076923076923)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scorer.score(machine,human)\n",
    "scores\n",
    "\n",
    "#rouge1 1 gram comparision, rouge2 2 gram comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T11:13:37.033763Z",
     "start_time": "2020-10-08T11:13:37.026766Z"
    }
   },
   "outputs": [],
   "source": [
    "S1=\"police killed the gunman\"\n",
    "S2=\"police kill the gunman\"\n",
    "S3=\"the gunman kill police\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T11:14:26.906001Z",
     "start_time": "2020-10-08T11:14:26.895999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0),\n",
       " 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(S1,S2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T11:14:27.318119Z",
     "start_time": "2020-10-08T11:14:27.307115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0),\n",
       " 'rougeL': Score(precision=0.5, recall=0.5, fmeasure=0.5)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scorer.score(S1,S3)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T17:27:16.309224Z",
     "start_time": "2020-09-04T17:27:16.299223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=scores['rougeL'] \n",
    "finalScore = round(res.fmeasure,3)\n",
    "finalScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sofcosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [machine, human]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "    \n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sentences into bag-of-words vectors.\n",
    "sent_1 = dictionary.doc2bow(sent_1)\n",
    "sent_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_2 = dictionary.doc2bow(sent_2)\n",
    "sent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute soft cosine similarity\n",
    "simc=softcossim(sent_1, sent_2, similarity_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
